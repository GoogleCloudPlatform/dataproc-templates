{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1db5371a-8f16-47b7-bcc7-5af386e9b6d8",
   "metadata": {},
   "source": [
    "# <center>Oracle to Postgres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98acd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd944742",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- [DataprocPySparkBatchOp reference](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html)\n",
    "- [Kubeflow SDK Overview](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)\n",
    "- [Dataproc Serverless in Vertex AI Pipelines tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)\n",
    "- [Build a Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)\n",
    "\n",
    "#### Overview - Oracle to Postgres Migration\n",
    "\n",
    "This notebook helps with the step by step process of migrating Oracle database tables to PostgreSQL using Dataproc template.\n",
    "\n",
    "#### Permissions\n",
    "\n",
    "This notebook is built to run a Vertex AI User-Managed Notebook using the default Compute Engine Service Account.  \n",
    "Check the Dataproc Serverless in Vertex AI Pipelines tutorial linked above to learn how to setup a different Service Account. If using custom service account, service account attached to Vertex AI notebook should have Service Account User role to use custom role in job.\n",
    "\n",
    "Make sure that the service account used to run the notebook has the following roles:\n",
    "\n",
    "- roles/aiplatform.serviceAgent\n",
    "- roles/aiplatform.customCodeServiceAgent\n",
    "- roles/storage.objectCreator\n",
    "- roles/storage.objectViewer\n",
    "- roles/dataproc.editor\n",
    "- roles/dataproc.worker\n",
    "- roles/bigquery.dataEditor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "740926fd-d602-4420-a362-d0646b5e11ce",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "Share you feedback, ideas, thoughts [feedback-form](https://forms.gle/XXCJeWeCJJ9fNLQS6)  \n",
    "Questions, issues, and comments should be directed to dataproc-templates-support-external@googlegroups.com\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d89b301-5249-462a-97d8-986488b303fd",
   "metadata": {},
   "source": [
    "## Step 1: Install Libraries\n",
    "\n",
    "#### Run Step 1 one time for each new notebook instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef65ec2-ad6b-407f-a993-7cdf871bba11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SQLAlchemy in /opt/conda/lib/python3.7/site-packages (1.4.45)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from SQLAlchemy) (5.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from SQLAlchemy) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->SQLAlchemy) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->SQLAlchemy) (4.4.0)\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libpq-dev is already the newest version (11.18-0+deb10u1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n",
      "Requirement already satisfied: psycopg2 in /opt/conda/lib/python3.7/site-packages (2.9.5)\n",
      "Requirement already satisfied: cx-Oracle in /opt/conda/lib/python3.7/site-packages (8.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install SQLAlchemy\n",
    "!pip3 install --upgrade google-cloud-pipeline-components kfp --user -q\n",
    "!sudo apt-get install libpq-dev --yes\n",
    "!pip3 install psycopg2\n",
    "!pip3 install cx-Oracle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fe7de03-566e-4902-97e9-eb9c9c4a8d8f",
   "metadata": {},
   "source": [
    "#### Oracle Client Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc9e6c-408e-4e87-9e44-82dff4f97969",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo mkdir -p /opt/oracle\n",
    "sudo rm -fr /opt/oracle/instantclient*\n",
    "cd /opt/oracle\n",
    "sudo wget --no-verbose https://download.oracle.com/otn_software/linux/instantclient/instantclient-basic-linuxx64.zip\n",
    "sudo unzip instantclient-basic-linuxx64.zip\n",
    "INSTANT_CLIENT_DIR=$(find /opt/oracle -maxdepth 1 -type d -name \"instantclient_[0-9]*_[0-9]*\" | sort | tail -1)\n",
    "test -n \"${INSTANT_CLIENT_DIR}\" || echo \"ERROR: Could not find instant client\"\n",
    "test -n \"${INSTANT_CLIENT_DIR}\" || exit 1\n",
    "sudo apt-get install libaio1\n",
    "sudo sh -c \"echo ${INSTANT_CLIENT_DIR} > /etc/ld.so.conf.d/oracle-instantclient.conf\"\n",
    "sudo ldconfig\n",
    "export LD_LIBRARY_PATH=${INSTANT_CLIENT_DIR}:$LD_LIBRARY_PATH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31456ac6-d058-4d90-8b18-c10f92922f81",
   "metadata": {},
   "source": [
    "#### Once you've installed the additional packages, you may need to restart the notebook kernel so it can find the packages.\n",
    "\n",
    "Uncomment & Run this cell if you have installed anything from above commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef1307-902e-4713-8948-b86084e19312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import IPython\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d01e33-9099-4d2e-b57e-575c3a998d84",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703b502-1b41-44f1-bf21-41069255bc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from google_cloud_pipeline_components.experimental.dataproc import (\n",
    "    DataprocPySparkBatchOp,\n",
    ")\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09c4a209-db59-42f6-bba7-30cd46b16bad",
   "metadata": {},
   "source": [
    "## Step 3: Assign Parameters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92d3fbd8-013f-45e6-b7e9-8f31a4580e91",
   "metadata": {},
   "source": [
    "### Step 3.1 Common Parameters\n",
    "\n",
    "- PROJECT : GCP project-id\n",
    "- REGION : GCP region\n",
    "- GCS_STAGING_LOCATION : GCS staging location to be used for this notebook to store artifacts\n",
    "- SUBNET : VPC subnet\n",
    "- JARS : list of jars. For this notebook postgres connector jar and oracle connector jar is required in addition with the dataproc template jars\n",
    "- MAX_PARALLELISM : Parameter for number of jobs to run in parallel default value is 5\n",
    "- SERVICE_ACCOUNT : Custom service account email to use for vertex ai pipeline and dataproc job with above mentioned permissions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b0650",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "IS_PARAMETERIZED = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9f42b4-4fc7-48de-8689-bc0f09d155f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GCP Project\n",
    "if not IS_PARAMETERIZED:\n",
    "    PROJECT = \"<project-id>\"\n",
    "    REGION = \"<region>\"\n",
    "    GCS_STAGING_LOCATION = \"<gs://bucket/[folder]>\"\n",
    "    SUBNET = \"<projects/{project}/regions/{region}/subnetworks/{subnet}>\"\n",
    "    MAX_PARALLELISM = 5  # max number of tables which will migrated parallelly\n",
    "    SERVICE_ACCOUNT = \"\"\n",
    "\n",
    "# Do not change this parameter unless you want to refer below JARS from new location\n",
    "JARS = [\n",
    "    GCS_STAGING_LOCATION + \"/jars/ojdbc8-21.7.0.0.jar\",\n",
    "    GCS_STAGING_LOCATION + \"/jars/postgresql-42.2.6.jar\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87c4e3-a4bd-44bd-8120-99097383bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If SERVICE_ACCOUNT is not specified it will take the one attached to Notebook\n",
    "if SERVICE_ACCOUNT == '':\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "    print(\"Service Account: \",SERVICE_ACCOUNT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "051df2af-bd8b-47c7-8cb2-05404ca0d859",
   "metadata": {},
   "source": [
    "### Step 3.2 ORACLE to Postgres Parameters\n",
    "\n",
    "- ORACLE_HOST : Oracle instance ip address\n",
    "- ORACLE_PORT : Oracle instance port\n",
    "- ORACLE_USERNAME : Oracle username\n",
    "- ORACLE_PASSWORD : Oracle password\n",
    "- ORACLE_DATABASE : Name of database/service for Oracle connection\n",
    "- ORACLETABLE_LIST : list of tables you want to migrate eg: ['table1','table2'] else provide an empty list for migration whole database eg : []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c61a12-7f39-41df-ace8-5d5c573680f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IS_PARAMETERIZED:\n",
    "    ORACLE_HOST = \"<host>\"\n",
    "    ORACLE_PORT = \"<port\"\n",
    "    ORACLE_USERNAME = \"<username>\"\n",
    "    ORACLE_PASSWORD = \"<password>\"\n",
    "    ORACLE_DATABASE = \"<database>\"\n",
    "    ORACLETABLE_LIST = (\n",
    "        []\n",
    "    )  # leave list empty for migrating complete database else provide tables as ['table1','table2']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17131137-9cd1-41a0-a942-52c3fa9f34e6",
   "metadata": {},
   "source": [
    "### Step 3.3 POSTGRES Parameters\n",
    "\n",
    "- POSTGRES_HOST : POSTGRES instance ip address\n",
    "- POSTGRES_PORT : POSTGRES instance port\n",
    "- POSTGRES_USERNAME : POSTGRES username\n",
    "- POSTGRES_PASSWORD : POSTGRES password\n",
    "- POSTGRES_DATABASE : name of database that you want to migrate\n",
    "- POSTGRES_SCHEMA : name of schema that you want to migrate\n",
    "- OUTPUT_MODE : Output write mode (one of: append,overwrite,ignore,errorifexists)(Defaults to overwrite)\n",
    "- BATCH_SIZE : JDBC output batch size. Default set to 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd8c35-8c55-40e2-ae5b-9a7e6a13bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_PARAMETERIZED:\n",
    "    POSTGRES_HOST = \"<host>\"\n",
    "    POSTGRES_PORT = \"<port>\"\n",
    "    POSTGRES_USERNAME = \"<username>\"\n",
    "    POSTGRES_PASSWORD = \"<password>\"\n",
    "    POSTGRES_DATABASE = \"<database>\"\n",
    "    POSTGRES_SCHEMA = \"<schema>\"\n",
    "    JDBCTOJDBC_OUTPUT_MODE = (\n",
    "        \"<modeoverwrite>\"  # one of append/overwrite/ignore/errorifexists\n",
    "    )\n",
    "    JDBCTOJDBC_OUTPUT_BATCH_SIZE = \"1000\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "166b1536-d58e-423b-b3c2-cc0c171d275e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3.4 Notebook Configuration Parameters\n",
    "\n",
    "Below variables should not be changed unless required\n",
    "\n",
    "- ORACLE_URL : Oracle Python URL\n",
    "- JDBC_DRIVER : JDBC driver class\n",
    "- JDBC_URL : Oracle JDBC URL\n",
    "- JDBC_FETCH_SIZE : Determines how many rows to fetch per round trip\n",
    "- MAIN_CLASS : Dataproc Template Main Class\n",
    "- WORKING_DIRECTORY : Python working directory\n",
    "- PACKAGE_EGG_FILE : Dataproc Template distributio file\n",
    "- PIPELINE_ROOT : Path to Vertex AI pipeline artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a2cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = Path(os.getcwd())\n",
    "if IS_PARAMETERIZED:\n",
    "    WORKING_DIRECTORY = os.path.join(cur_path.parent, \"python\")\n",
    "else:\n",
    "    WORKING_DIRECTORY = os.path.join(cur_path.parent.parent, \"python\")\n",
    "\n",
    "# If the above code doesn't fetches the correct path please\n",
    "# provide complete path to python folder in your dataproc\n",
    "# template repo which you cloned\n",
    "\n",
    "# WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/python/\"\n",
    "print(WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0f037-e888-4479-a143-f06a39bd5cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ORACLE_URL = \"oracle://{}:{}@{}:{}?service_name={}\".format(\n",
    "    ORACLE_USERNAME, ORACLE_PASSWORD, ORACLE_HOST, ORACLE_PORT, ORACLE_DATABASE\n",
    ")\n",
    "JDBC_INPUT_DRIVER = \"oracle.jdbc.OracleDriver\"\n",
    "JDBC_INPUT_URL = \"jdbc:oracle:thin:{}/{}@{}:{}/{}\".format(\n",
    "    ORACLE_USERNAME, ORACLE_PASSWORD, ORACLE_HOST, ORACLE_PORT, ORACLE_DATABASE\n",
    ")\n",
    "MAIN_CLASS = \"com.google.cloud.dataproc.templates.main.DataProcTemplate\"\n",
    "WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/python/\"\n",
    "JDBC_OUTPUT_DRIVER = \"org.postgresql.Driver\"\n",
    "JDBC_OUTPUT_URL = \"jdbc:postgresql://{0}:{1}/{2}?user={3}&password={4}&reWriteBatchedInserts=true\".format(\n",
    "    POSTGRES_HOST,\n",
    "    POSTGRES_PORT,\n",
    "    POSTGRES_DATABASE,\n",
    "    POSTGRES_USERNAME,\n",
    "    POSTGRES_PASSWORD,\n",
    ")\n",
    "PACKAGE_EGG_FILE = \"dataproc_templates_distribution.egg\"\n",
    "\n",
    "JDBC_FETCH_SIZE = 200\n",
    "\n",
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\"\n",
    "MAIN_PYTHON_FILE = GCS_STAGING_LOCATION + \"/main.py\"\n",
    "PYTHON_FILE_URIS = [GCS_STAGING_LOCATION + \"/dataproc_templates_distribution.egg\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "115c062b-5a91-4372-b440-5c37a12fbf87",
   "metadata": {},
   "source": [
    "## Step 4: Generate ORACLE Table List\n",
    "\n",
    "This step creates list of tables for migration. If ORACLETABLE_LIST is kept empty all the tables in the ORACLE_DATABASE are listed for migration otherwise the provided list is used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e362ac-30cd-4857-9e2a-0e9eb926e627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(ORACLETABLE_LIST) == 0:\n",
    "    DB = sqlalchemy.create_engine(ORACLE_URL)\n",
    "    with DB.connect() as conn:\n",
    "        print(\"connected to database\")\n",
    "        results = conn.execute(text(\"SELECT table_name FROM user_tables\")).fetchall()\n",
    "        print(\"Total Tables = \", len(results))\n",
    "        for row in results:\n",
    "            ORACLETABLE_LIST.append(row[0])\n",
    "\n",
    "print(\"list of tables for migration :\")\n",
    "print(ORACLETABLE_LIST)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d9a62e8-7499-41c6-b32b-73b539b0c7c4",
   "metadata": {},
   "source": [
    "## Step 5: Get Primary Keys for tables from ORACLE source\n",
    "\n",
    "This step fetches primary key of tables listed in ORACLETABLE_LIST from ORACLE_DATABASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93608d0f-ef97-4700-b360-cfb3444e88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_TABLE_PRIMARY_KEYS = {}  # dict for storing primary keys for each table\n",
    "\n",
    "DB = sqlalchemy.create_engine(ORACLE_URL)\n",
    "with DB.connect() as conn:\n",
    "    for table in ORACLETABLE_LIST:\n",
    "        primary_keys = []\n",
    "        results = conn.execute(\n",
    "            text(\n",
    "                \"SELECT cols.column_name FROM sys.all_constraints cons, sys.all_cons_columns cols WHERE cols.table_name = '{}' AND cons.constraint_type = 'P' AND cons.status = 'ENABLED' AND cols.position = 1 AND cons.constraint_name = cols.constraint_name AND cons.owner = cols.owner\".format(\n",
    "                    table\n",
    "                )\n",
    "            )\n",
    "        ).fetchall()\n",
    "        for row in results:\n",
    "            primary_keys.append(row[0])\n",
    "        if primary_keys:\n",
    "            SQL_TABLE_PRIMARY_KEYS[table] = \",\".join(primary_keys)\n",
    "        else:\n",
    "            SQL_TABLE_PRIMARY_KEYS[table] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f69245-f3f4-464f-a4c1-85ffd3e805e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkDF = pd.DataFrame(\n",
    "    {\"table\": ORACLETABLE_LIST, \"primary_keys\": list(SQL_TABLE_PRIMARY_KEYS.values())}\n",
    ")\n",
    "print(\"Below are identified primary keys for migrating ORACLE table to Postgres:\")\n",
    "pkDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8ac9a9f-1332-4c75-b6ca-bfb3b776671f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 6: Get Row Count of Tables and identify Partition Columns\n",
    "\n",
    "This step uses PARTITION_THRESHOLD(default value is 1 million) parameter and any table having rows greater than PARTITION_THRESHOLD will be used for partitioned read based on Primary Keys\n",
    "\n",
    "- CHECK_PARTITION_COLUMN_LIST : List will have table and its partitioned column if exceeds threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe54e83c-7845-4043-baf8-c0ee6f32787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION_THRESHOLD = 1000000\n",
    "CHECK_PARTITION_COLUMN_LIST = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692a051-7555-42fc-af27-b05e461576f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DB.connect() as conn:\n",
    "    for table in ORACLETABLE_LIST:\n",
    "        results = conn.execute(text(\"SELECT count(1) FROM {}\".format(table))).fetchall()\n",
    "        if (\n",
    "            results[0][0] > int(PARTITION_THRESHOLD)\n",
    "            and len(SQL_TABLE_PRIMARY_KEYS.get(table).split(\",\")[0]) > 0\n",
    "        ):\n",
    "            column_list = SQL_TABLE_PRIMARY_KEYS.get(table).split(\",\")\n",
    "            column = column_list[0]\n",
    "            results_datatype = conn.execute(\n",
    "                text(\n",
    "                    \"select DATA_TYPE from sys.all_tab_columns where TABLE_NAME = '{0}' and COLUMN_NAME = '{1}'\".format(\n",
    "                        table, column\n",
    "                    )\n",
    "                )\n",
    "            ).fetchall()\n",
    "            if (\n",
    "                results_datatype[0][0] == \"NUMBER\"\n",
    "                or results_datatype[0][0] == \"INTEGER\"\n",
    "            ):\n",
    "                lowerbound = conn.execute(\n",
    "                    text(\"SELECT min({0}) from {1}\".format(column, table))\n",
    "                ).fetchall()\n",
    "                upperbound = conn.execute(\n",
    "                    text(\"SELECT max({0}) from {1}\".format(column, table))\n",
    "                ).fetchall()\n",
    "                numberPartitions = math.ceil(\n",
    "                    (upperbound[0][0] - lowerbound[0][0]) / PARTITION_THRESHOLD\n",
    "                )\n",
    "                CHECK_PARTITION_COLUMN_LIST[table] = [\n",
    "                    column,\n",
    "                    lowerbound[0][0],\n",
    "                    upperbound[0][0],\n",
    "                    numberPartitions,\n",
    "                ]\n",
    "\n",
    "print(CHECK_PARTITION_COLUMN_LIST)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fa5f841-a687-4723-a8e6-6e7e752ba36e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 7: Download JAR files and Upload to GCS (only rquired to run one-time)\n",
    "\n",
    "#### Run Step 7 one time for each new notebook instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22220ae3-9fb4-471c-b5aa-f606deeca15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd $WORKING_DIRECTORY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdee7afc-699b-4c1a-aeec-df0f99764ae0",
   "metadata": {},
   "source": [
    "#### Downloading JDBC Oracle Driver and Postgres Jar files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40f634-1983-4267-a4c1-b072bf6d81ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget --no-verbose https://repo1.maven.org/maven2/com/oracle/database/jdbc/ojdbc8/21.7.0.0/ojdbc8-21.7.0.0.jar\n",
    "wget --no-verbose https://jdbc.postgresql.org/download/postgresql-42.2.6.jar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e6d6813-36f0-45bc-a784-217842635138",
   "metadata": {},
   "source": [
    "#### Build Dataproc Templates python package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06735281-1c22-48d3-9b71-20f9ca99d04a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python ./setup.py bdist_egg --output=$PACKAGE_EGG_FILE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e1a779f-2c39-42ec-98be-0f5e9d715447",
   "metadata": {},
   "source": [
    "#### Copying JARS files to GCS_STAGING_LOCATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cdcd5-0f3e-4f51-aa78-93d1976cb0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil cp main.py $GCS_STAGING_LOCATION/\n",
    "! gsutil cp -r $PACKAGE_EGG_FILE $GCS_STAGING_LOCATION/\n",
    "! gsutil cp ojdbc8-21.7.0.0.jar $GCS_STAGING_LOCATION/jars/ojdbc8-21.7.0.0.jar\n",
    "! gsutil cp postgresql-42.2.6.jar $GCS_STAGING_LOCATION/jars/postgresql-42.2.6.jar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d9bb170-09c4-40d1-baaf-9e907f215889",
   "metadata": {},
   "source": [
    "## Step 8: Calculate Parallel Jobs for ORACLE to Postgres\n",
    "\n",
    "This step uses MAX_PARALLELISM parameter to calculate number of parallel jobs to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c501db0-c1fb-4a05-88b8-a7e546e2b1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate parallel jobs:\n",
    "COMPLETE_LIST = copy.deepcopy(ORACLETABLE_LIST)\n",
    "PARALLEL_JOBS = len(ORACLETABLE_LIST) // MAX_PARALLELISM\n",
    "JOB_LIST = []\n",
    "while len(COMPLETE_LIST) > 0:\n",
    "    SUB_LIST = []\n",
    "    for i in range(MAX_PARALLELISM):\n",
    "        if len(COMPLETE_LIST) > 0:\n",
    "            SUB_LIST.append(COMPLETE_LIST[0])\n",
    "            COMPLETE_LIST.pop(0)\n",
    "        else:\n",
    "            break\n",
    "    JOB_LIST.append(SUB_LIST)\n",
    "print(\"list of tables for execution : \")\n",
    "print(JOB_LIST)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e31a5e7-f8d4-4474-816f-13217dfa5fad",
   "metadata": {},
   "source": [
    "## Step 9:Create Source Schemas in POSTGRES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac53ca9-75a1-48d7-82a3-9674a1fbdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "postgresDB = psycopg2.connect(\n",
    "    user=POSTGRES_USERNAME,\n",
    "    password=POSTGRES_PASSWORD,\n",
    "    dbname=POSTGRES_DATABASE,\n",
    "    host=POSTGRES_HOST,\n",
    "    port=POSTGRES_PORT,\n",
    ")\n",
    "postgresDB.autocommit = True\n",
    "conn = postgresDB.cursor()\n",
    "conn.execute(\"\"\"CREATE SCHEMA IF NOT EXISTS {};\"\"\".format(POSTGRES_SCHEMA))\n",
    "conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63075f47-4d9f-4237-8833-aa29eebb09f4",
   "metadata": {},
   "source": [
    "## Step 10: Execute Pipeline to Migrate tables from ORACLE to POSTGRES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7d00c-bd2a-4aaa-aee4-e41a9793dfb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oracle_to_postgres_jobs = []\n",
    "\n",
    "\n",
    "def migrate_oracle_to_postgres(EXECUTION_LIST):\n",
    "    EXECUTION_LIST = EXECUTION_LIST\n",
    "    aiplatform.init(project=PROJECT, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name=\"python-oracle-to-postgres-pyspark\",\n",
    "        description=\"Pipeline to get data from Oracle to postgres\",\n",
    "    )\n",
    "    def pipeline(\n",
    "        PROJECT_ID: str = PROJECT,\n",
    "        LOCATION: str = REGION,\n",
    "        MAIN_PYTHON_CLASS: str = MAIN_PYTHON_FILE,\n",
    "        PYTHON_FILE_URIS: list = PYTHON_FILE_URIS,\n",
    "        JAR_FILE_URIS: list = JARS,\n",
    "        SUBNETWORK_URI: str = SUBNET,\n",
    "    ):\n",
    "        for table in EXECUTION_LIST:\n",
    "            BATCH_ID = (\n",
    "                \"oracle2postgres-{}\".format(datetime.now().strftime(\"%s\"))\n",
    "                .replace(\".\", \"-\")\n",
    "                .replace(\"_\", \"-\")\n",
    "                .lower()\n",
    "            )\n",
    "            oracle_to_postgres_jobs.append(BATCH_ID)\n",
    "\n",
    "            if table in CHECK_PARTITION_COLUMN_LIST.keys():\n",
    "                TEMPLATE_SPARK_ARGS = [\n",
    "                    \"--template=JDBCTOJDBC\",\n",
    "                    \"--jdbctojdbc.input.url={}\".format(JDBC_INPUT_URL),\n",
    "                    \"--jdbctojdbc.input.driver={}\".format(JDBC_INPUT_DRIVER),\n",
    "                    \"--jdbctojdbc.input.table={}\".format(table),\n",
    "                    \"--jdbctojdbc.output.url={}\".format(JDBC_OUTPUT_URL),\n",
    "                    \"--jdbctojdbc.output.driver={}\".format(JDBC_OUTPUT_DRIVER),\n",
    "                    \"--jdbctojdbc.output.table={}\".format(\n",
    "                        POSTGRES_SCHEMA + \".\" + table\n",
    "                    ),\n",
    "                    \"--jdbctojdbc.input.partitioncolumn={}\".format(\n",
    "                        CHECK_PARTITION_COLUMN_LIST[table][0]\n",
    "                    ),\n",
    "                    \"--jdbctojdbc.input.lowerbound={}\".format(\n",
    "                        CHECK_PARTITION_COLUMN_LIST[table][1]\n",
    "                    ),\n",
    "                    \"--jdbctojdbc.input.upperbound={}\".format(\n",
    "                        CHECK_PARTITION_COLUMN_LIST[table][2]\n",
    "                    ),\n",
    "                    \"--jdbctojdbc.numpartitions={}\".format(\n",
    "                        CHECK_PARTITION_COLUMN_LIST[table][3]\n",
    "                    ),\n",
    "                    \"--jdbctojdbc.output.mode={}\".format(JDBCTOJDBC_OUTPUT_MODE),\n",
    "                    \"--jdbctojdbc.output.batch.size={}\".format(\n",
    "                        JDBCTOJDBC_OUTPUT_BATCH_SIZE\n",
    "                    ),\n",
    "                ]\n",
    "            else:\n",
    "                TEMPLATE_SPARK_ARGS = [\n",
    "                    \"--template=JDBCTOJDBC\",\n",
    "                    \"--jdbctojdbc.input.url={}\".format(JDBC_INPUT_URL),\n",
    "                    \"--jdbctojdbc.input.driver={}\".format(JDBC_INPUT_DRIVER),\n",
    "                    \"--jdbctojdbc.input.table={}\".format(table),\n",
    "                    \"--jdbctojdbc.output.url={}\".format(JDBC_OUTPUT_URL),\n",
    "                    \"--jdbctojdbc.output.driver={}\".format(JDBC_OUTPUT_DRIVER),\n",
    "                    \"--jdbctojdbc.output.table={}\".format(\n",
    "                        POSTGRES_SCHEMA + \".\" + table\n",
    "                    ),\n",
    "                    \"--jdbctojdbc.output.mode={}\".format(JDBCTOJDBC_OUTPUT_MODE),\n",
    "                    \"--jdbctojdbc.output.batch.size={}\".format(\n",
    "                        JDBCTOJDBC_OUTPUT_BATCH_SIZE\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "            _ = DataprocPySparkBatchOp(\n",
    "                project=PROJECT_ID,\n",
    "                location=LOCATION,\n",
    "                batch_id=BATCH_ID,\n",
    "                main_python_file_uri=MAIN_PYTHON_CLASS,\n",
    "                jar_file_uris=JAR_FILE_URIS,\n",
    "                python_file_uris=PYTHON_FILE_URIS,\n",
    "                subnetwork_uri=SUBNETWORK_URI,\n",
    "                service_account=SERVICE_ACCOUNT,\n",
    "                args=TEMPLATE_SPARK_ARGS,\n",
    "            )\n",
    "            time.sleep(3)\n",
    "\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "    pipeline = aiplatform.PipelineJob(\n",
    "        display_name=\"pipeline\",\n",
    "        template_path=\"pipeline.json\",\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        enable_caching=False,\n",
    "    )\n",
    "    pipeline.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44205b54-1ac7-42f3-85ad-5b20f531056b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for execution_list in JOB_LIST:\n",
    "    print(execution_list)\n",
    "    migrate_oracle_to_postgres(execution_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ce7f828-dacc-404b-8927-dc3813e7216a",
   "metadata": {},
   "source": [
    "## Step 10: Get status for tables migrated from ORACLE to POSTGRES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611510f-271c-447a-899d-42fbb983268d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bearer_token():\n",
    "    try:\n",
    "        # Defining Scope\n",
    "        CREDENTIAL_SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "\n",
    "        # Assining credentials and project value\n",
    "        credentials, project_id = google.auth.default(scopes=CREDENTIAL_SCOPES)\n",
    "\n",
    "        # Refreshing credentials data\n",
    "        credentials.refresh(requests.Request())\n",
    "\n",
    "        # Get refreshed token\n",
    "        token = credentials.token\n",
    "        if token:\n",
    "            return (token, 200)\n",
    "        else:\n",
    "            return \"Bearer token not generated\"\n",
    "    except Exception as error:\n",
    "        return (\"Bearer token not generated. Error : {}\".format(error), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcbc63-19db-42a8-a2ed-d9855da00c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.auth.transport import requests\n",
    "import google\n",
    "\n",
    "token = get_bearer_token()\n",
    "if token[1] == 200:\n",
    "    print(\"Bearer token generated\")\n",
    "else:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3cf87-6d28-4b23-8466-87d3399f7a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "oracle_to_pg_status = []\n",
    "job_status_url = (\n",
    "    \"https://dataproc.googleapis.com/v1/projects/{}/locations/{}/batches/{}\"\n",
    ")\n",
    "for job in oracle_to_postgres_jobs:\n",
    "    auth = \"Bearer \" + token[0]\n",
    "    url = job_status_url.format(PROJECT, REGION, job)\n",
    "    headers = {\"Content-Type\": \"application/json; charset=UTF-8\", \"Authorization\": auth}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    oracle_to_pg_status.append(response.json()[\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097575d-07c2-4659-a75f-d7e898e3f077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statusDF = pd.DataFrame(\n",
    "    {\n",
    "        \"table\": ORACLETABLE_LIST,\n",
    "        \"oracle_to_postgres_job\": oracle_to_postgres_jobs,\n",
    "        \"oracle_to_postgres_status\": oracle_to_pg_status,\n",
    "    }\n",
    ")\n",
    "statusDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0961f164-c7e4-4bb5-80f0-25fd1051147b",
   "metadata": {},
   "source": [
    "## Step 11: Validate row counts of migrated tables from ORACLE to Postgres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a28fb-3a39-4a10-b92d-0685b351a1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oracle_row_count = []\n",
    "postgres_row_count = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25299344-c167-4764-a5d1-56c1b384d104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Oracle table counts\n",
    "DB = sqlalchemy.create_engine(ORACLE_URL)\n",
    "with DB.connect() as conn:\n",
    "    for table in ORACLETABLE_LIST:\n",
    "        results = conn.execute(text(\"select count(*) from {}\".format(table))).fetchall()\n",
    "        for row in results:\n",
    "            oracle_row_count.append(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e539d-5180-4f5b-915e-35f7ea45e0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "postgresDB = psycopg2.connect(\n",
    "    user=POSTGRES_USERNAME,\n",
    "    password=POSTGRES_PASSWORD,\n",
    "    dbname=POSTGRES_DATABASE,\n",
    "    host=POSTGRES_HOST,\n",
    "    port=POSTGRES_PORT,\n",
    ")\n",
    "\n",
    "conn = postgresDB.cursor()\n",
    "for table in ORACLETABLE_LIST:\n",
    "    conn.execute(\"\"\"select count(*) from {}.{}\"\"\".format(POSTGRES_SCHEMA, table))\n",
    "    results = conn.fetchall()\n",
    "    for row in results:\n",
    "        postgres_row_count.append(row[0])\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1afe12-3eb9-4133-8377-66dc63ac649c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statusDF[\"oracle_row_count\"] = oracle_row_count\n",
    "statusDF[\"postgres_row_count\"] = postgres_row_count\n",
    "statusDF"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
