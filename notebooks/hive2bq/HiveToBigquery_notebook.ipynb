{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f55fbb-2197-4038-88c5-6c896a9f071a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9eff8d-6b63-4e8f-b369-68cbb4ef04ee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "- [DataprocPySparkBatchOp reference](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html)\n",
    "- [Kubeflow SDK Overview](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)\n",
    "- [Dataproc Serverless in Vertex AI Pipelines tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)\n",
    "- [Build a Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)\n",
    "\n",
    "This notebook is built to run a Vertex AI User-Managed Notebook using the default Compute Engine Service Account.  \n",
    "Check the Dataproc Serverless in Vertex AI Pipelines tutorial linked above to learn how to setup a different Service Account.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852a764-dad3-4f3b-b0b5-a71825469fd3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Permissions\n",
    "\n",
    "Make sure that the service account used to run the notebook has the following roles:\n",
    "\n",
    "- roles/aiplatform.serviceAgent\n",
    "- roles/aiplatform.customCodeServiceAgent\n",
    "- roles/storage.objectCreator\n",
    "- roles/storage.objectViewer\n",
    "- roles/dataproc.editor\n",
    "- roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640fa29-1a04-4301-974d-9fcec95b7e7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Step 1:\n",
    "#### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3742b9a-143d-49fc-b43c-e56179c7f0f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=7eecaf1e78c50a25e4e7f8cfa1820f0b2f2bf08cb9d869cd0c6c052347d3beb7\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/06/51/98/f7a41aad64c08302d6c26c90650e713c3dfeb5cdec4946db00\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "\u001b[33m  WARNING: The script strip-hints is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (23.0.1)\n",
      "Collecting install\n",
      "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
      "Collecting google-auth==2.13.0\n",
      "  Downloading google_auth-2.13.0-py2.py3-none-any.whl (174 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==2.13.0) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth==2.13.0) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth==2.13.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==2.13.0) (5.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth==2.13.0) (0.4.8)\n",
      "Installing collected packages: install, google-auth\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.17.1\n",
      "    Uninstalling google-auth-2.17.1:\n",
      "      Successfully uninstalled google-auth-2.17.1\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '__init__.cpython-37.pyc'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "Get:2 http://packages.cloud.google.com/apt gcsfuse-buster InRelease [5004 B]   \n",
      "Hit:3 http://packages.cloud.google.com/apt google-compute-engine-buster-stable InRelease\n",
      "Get:4 http://packages.cloud.google.com/apt cloud-sdk-buster InRelease [6396 B] \n",
      "Get:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [8993 B]\n",
      "Hit:5 http://deb.debian.org/debian buster InRelease                            \n",
      "Get:6 https://download.docker.com/linux/debian buster InRelease [54.0 kB]\n",
      "Hit:7 http://security.debian.org/debian-security buster/updates InRelease      \n",
      "Get:8 http://deb.debian.org/debian buster-updates InRelease [56.6 kB]          \n",
      "Get:9 http://deb.debian.org/debian buster-backports InRelease [51.4 kB]\n",
      "Fetched 182 kB in 2s (115 kB/s)     \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  ca-certificates-java default-jdk-headless default-jre default-jre-headless\n",
      "  fonts-dejavu-extra java-common libatk-wrapper-java libatk-wrapper-java-jni\n",
      "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
      "  libice-dev libnspr4 libnss3 libpcsclite1 libpthread-stubs0-dev libsm-dev\n",
      "  libx11-dev libxau-dev libxcb1-dev libxdmcp-dev libxt-dev openjdk-11-jdk\n",
      "  openjdk-11-jdk-headless openjdk-11-jre openjdk-11-jre-headless\n",
      "  x11proto-core-dev x11proto-dev xorg-sgml-doctools xtrans-dev\n",
      "Suggested packages:\n",
      "  gvfs libice-doc pcscd libsm-doc libx11-doc libxcb-doc libxt-doc\n",
      "  openjdk-11-demo openjdk-11-source visualvm libnss-mdns fonts-ipafont-gothic\n",
      "  fonts-ipafont-mincho fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic\n",
      "The following NEW packages will be installed:\n",
      "  ca-certificates-java default-jdk default-jdk-headless default-jre\n",
      "  default-jre-headless fonts-dejavu-extra java-common libatk-wrapper-java\n",
      "  libatk-wrapper-java-jni libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin\n",
      "  libgtk2.0-common libice-dev libnspr4 libnss3 libpcsclite1\n",
      "  libpthread-stubs0-dev libsm-dev libx11-dev libxau-dev libxcb1-dev\n",
      "  libxdmcp-dev libxt-dev openjdk-11-jdk openjdk-11-jdk-headless openjdk-11-jre\n",
      "  openjdk-11-jre-headless x11proto-core-dev x11proto-dev xorg-sgml-doctools\n",
      "  xtrans-dev\n",
      "0 upgraded, 33 newly installed, 0 to remove and 10 not upgraded.\n",
      "Need to get 276 MB of archives.\n",
      "After this operation, 457 MB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian buster/main amd64 java-common all 0.71 [14.4 kB]\n",
      "Get:2 http://security.debian.org/debian-security buster/updates/main amd64 libnss3 amd64 2:3.42.1-1+deb10u6 [1245 kB]\n",
      "Get:3 http://deb.debian.org/debian buster/main amd64 libnspr4 amd64 2:4.20-1 [112 kB]\n",
      "Get:4 http://deb.debian.org/debian buster/main amd64 libpcsclite1 amd64 1.8.24-1 [58.5 kB]\n",
      "Get:5 http://deb.debian.org/debian buster/main amd64 default-jre-headless amd64 2:1.11-71 [10.9 kB]\n",
      "Get:6 http://deb.debian.org/debian buster/main amd64 ca-certificates-java all 20190405 [15.7 kB]\n",
      "Get:7 http://deb.debian.org/debian buster/main amd64 default-jre amd64 2:1.11-71 [1044 B]\n",
      "Get:8 http://deb.debian.org/debian buster/main amd64 default-jdk-headless amd64 2:1.11-71 [1104 B]\n",
      "Get:9 http://deb.debian.org/debian buster/main amd64 default-jdk amd64 2:1.11-71 [1056 B]\n",
      "Get:10 http://deb.debian.org/debian buster/main amd64 fonts-dejavu-extra all 2.37-1 [1982 kB]\n",
      "Get:11 http://deb.debian.org/debian buster/main amd64 libatk-wrapper-java all 0.33.3-22+deb10u1 [45.5 kB]\n",
      "Get:12 http://deb.debian.org/debian buster/main amd64 libgtk2.0-common all 2.24.32-3 [2698 kB]\n",
      "Get:13 http://security.debian.org/debian-security buster/updates/main amd64 openjdk-11-jre-headless amd64 11.0.18+10-1~deb10u1 [37.3 MB]\n",
      "Get:14 http://deb.debian.org/debian buster/main amd64 libgtk2.0-0 amd64 2.24.32-3 [1809 kB]\n",
      "Get:15 http://deb.debian.org/debian buster/main amd64 libatk-wrapper-java-jni amd64 0.33.3-22+deb10u1 [39.5 kB]\n",
      "Get:16 http://deb.debian.org/debian buster/main amd64 libgail18 amd64 2.24.32-3 [55.5 kB]\n",
      "Get:17 http://deb.debian.org/debian buster/main amd64 libgail-common amd64 2.24.32-3 [156 kB]\n",
      "Get:18 http://deb.debian.org/debian buster/main amd64 libgtk2.0-bin amd64 2.24.32-3 [48.5 kB]\n",
      "Get:19 http://deb.debian.org/debian buster/main amd64 xorg-sgml-doctools all 1:1.11-1 [21.9 kB]\n",
      "Get:20 http://deb.debian.org/debian buster/main amd64 x11proto-dev all 2018.4-4 [251 kB]\n",
      "Get:21 http://deb.debian.org/debian buster/main amd64 x11proto-core-dev all 2018.4-4 [3128 B]\n",
      "Get:22 http://deb.debian.org/debian buster/main amd64 libice-dev amd64 2:1.0.9-2 [66.8 kB]\n",
      "Get:23 http://deb.debian.org/debian buster/main amd64 libpthread-stubs0-dev amd64 0.4-1 [5344 B]\n",
      "Get:24 http://deb.debian.org/debian buster/main amd64 libsm-dev amd64 2:1.2.3-1 [38.0 kB]\n",
      "Get:25 http://deb.debian.org/debian buster/main amd64 libxau-dev amd64 1:1.0.8-1+b2 [23.1 kB]\n",
      "Get:26 http://deb.debian.org/debian buster/main amd64 libxdmcp-dev amd64 1:1.1.2-3 [42.2 kB]\n",
      "Get:27 http://deb.debian.org/debian buster/main amd64 xtrans-dev all 1.3.5-1 [100 kB]\n",
      "Get:28 http://deb.debian.org/debian buster/main amd64 libxcb1-dev amd64 1.13.1-2 [174 kB]\n",
      "Get:29 http://deb.debian.org/debian buster/main amd64 libx11-dev amd64 2:1.6.7-1+deb10u2 [825 kB]\n",
      "Get:30 http://deb.debian.org/debian buster/main amd64 libxt-dev amd64 1:1.1.5-1+b3 [426 kB]\n",
      "Get:31 http://security.debian.org/debian-security buster/updates/main amd64 openjdk-11-jre amd64 11.0.18+10-1~deb10u1 [174 kB]\n",
      "Get:32 http://security.debian.org/debian-security buster/updates/main amd64 openjdk-11-jdk-headless amd64 11.0.18+10-1~deb10u1 [221 MB]\n",
      "Get:33 http://security.debian.org/debian-security buster/updates/main amd64 openjdk-11-jdk amd64 11.0.18+10-1~deb10u1 [7135 kB]\n",
      "Fetched 276 MB in 6s (48.8 MB/s)          \n",
      "Extracting templates from packages: 100%\n",
      "Selecting previously unselected package java-common.\n",
      "(Reading database ... 124194 files and directories currently installed.)\n",
      "Preparing to unpack .../00-java-common_0.71_all.deb ...\n",
      "Unpacking java-common (0.71) ...\n",
      "Selecting previously unselected package libnspr4:amd64.\n",
      "Preparing to unpack .../01-libnspr4_2%3a4.20-1_amd64.deb ...\n",
      "Unpacking libnspr4:amd64 (2:4.20-1) ...\n",
      "Selecting previously unselected package libnss3:amd64.\n",
      "Preparing to unpack .../02-libnss3_2%3a3.42.1-1+deb10u6_amd64.deb ...\n",
      "Unpacking libnss3:amd64 (2:3.42.1-1+deb10u6) ...\n",
      "Selecting previously unselected package libpcsclite1:amd64.\n",
      "Preparing to unpack .../03-libpcsclite1_1.8.24-1_amd64.deb ...\n",
      "Unpacking libpcsclite1:amd64 (1.8.24-1) ...\n",
      "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
      "Preparing to unpack .../04-openjdk-11-jre-headless_11.0.18+10-1~deb10u1_amd64.deb ...\n",
      "Unpacking openjdk-11-jre-headless:amd64 (11.0.18+10-1~deb10u1) ...\n",
      "Selecting previously unselected package default-jre-headless.\n",
      "Preparing to unpack .../05-default-jre-headless_2%3a1.11-71_amd64.deb ...\n",
      "Unpacking default-jre-headless (2:1.11-71) ...\n",
      "Selecting previously unselected package ca-certificates-java.\n",
      "Preparing to unpack .../06-ca-certificates-java_20190405_all.deb ...\n",
      "Unpacking ca-certificates-java (20190405) ...\n",
      "Selecting previously unselected package openjdk-11-jre:amd64.\n",
      "Preparing to unpack .../07-openjdk-11-jre_11.0.18+10-1~deb10u1_amd64.deb ...\n",
      "Unpacking openjdk-11-jre:amd64 (11.0.18+10-1~deb10u1) ...\n",
      "Selecting previously unselected package default-jre.\n",
      "Preparing to unpack .../08-default-jre_2%3a1.11-71_amd64.deb ...\n",
      "Unpacking default-jre (2:1.11-71) ...\n",
      "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
      "Preparing to unpack .../09-openjdk-11-jdk-headless_11.0.18+10-1~deb10u1_amd64.deb ...\n",
      "Unpacking openjdk-11-jdk-headless:amd64 (11.0.18+10-1~deb10u1) ...\n",
      "Selecting previously unselected package default-jdk-headless.\n",
      "Preparing to unpack .../10-default-jdk-headless_2%3a1.11-71_amd64.deb ...\n",
      "Unpacking default-jdk-headless (2:1.11-71) ...\n",
      "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
      "Preparing to unpack .../11-openjdk-11-jdk_11.0.18+10-1~deb10u1_amd64.deb ...\n",
      "Unpacking openjdk-11-jdk:amd64 (11.0.18+10-1~deb10u1) ...\n",
      "Selecting previously unselected package default-jdk.\n",
      "Preparing to unpack .../12-default-jdk_2%3a1.11-71_amd64.deb ...\n",
      "Unpacking default-jdk (2:1.11-71) ...\n",
      "Selecting previously unselected package fonts-dejavu-extra.\n",
      "Preparing to unpack .../13-fonts-dejavu-extra_2.37-1_all.deb ...\n",
      "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
      "Selecting previously unselected package libatk-wrapper-java.\n",
      "Preparing to unpack .../14-libatk-wrapper-java_0.33.3-22+deb10u1_all.deb ...\n",
      "Unpacking libatk-wrapper-java (0.33.3-22+deb10u1) ...\n",
      "Selecting previously unselected package libgtk2.0-common.\n",
      "Preparing to unpack .../15-libgtk2.0-common_2.24.32-3_all.deb ...\n",
      "Unpacking libgtk2.0-common (2.24.32-3) ...\n",
      "Selecting previously unselected package libgtk2.0-0:amd64.\n",
      "Preparing to unpack .../16-libgtk2.0-0_2.24.32-3_amd64.deb ...\n",
      "Unpacking libgtk2.0-0:amd64 (2.24.32-3) ...\n",
      "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
      "Preparing to unpack .../17-libatk-wrapper-java-jni_0.33.3-22+deb10u1_amd64.deb ...\n",
      "Unpacking libatk-wrapper-java-jni:amd64 (0.33.3-22+deb10u1) ...\n",
      "Selecting previously unselected package libgail18:amd64.\n",
      "Preparing to unpack .../18-libgail18_2.24.32-3_amd64.deb ...\n",
      "Unpacking libgail18:amd64 (2.24.32-3) ...\n",
      "Selecting previously unselected package libgail-common:amd64.\n",
      "Preparing to unpack .../19-libgail-common_2.24.32-3_amd64.deb ...\n",
      "Unpacking libgail-common:amd64 (2.24.32-3) ...\n",
      "Selecting previously unselected package libgtk2.0-bin.\n",
      "Preparing to unpack .../20-libgtk2.0-bin_2.24.32-3_amd64.deb ...\n",
      "Unpacking libgtk2.0-bin (2.24.32-3) ...\n",
      "Selecting previously unselected package xorg-sgml-doctools.\n",
      "Preparing to unpack .../21-xorg-sgml-doctools_1%3a1.11-1_all.deb ...\n",
      "Unpacking xorg-sgml-doctools (1:1.11-1) ...\n",
      "Selecting previously unselected package x11proto-dev.\n",
      "Preparing to unpack .../22-x11proto-dev_2018.4-4_all.deb ...\n",
      "Unpacking x11proto-dev (2018.4-4) ...\n",
      "Selecting previously unselected package x11proto-core-dev.\n",
      "Preparing to unpack .../23-x11proto-core-dev_2018.4-4_all.deb ...\n",
      "Unpacking x11proto-core-dev (2018.4-4) ...\n",
      "Selecting previously unselected package libice-dev:amd64.\n",
      "Preparing to unpack .../24-libice-dev_2%3a1.0.9-2_amd64.deb ...\n",
      "Unpacking libice-dev:amd64 (2:1.0.9-2) ...\n",
      "Selecting previously unselected package libpthread-stubs0-dev:amd64.\n",
      "Preparing to unpack .../25-libpthread-stubs0-dev_0.4-1_amd64.deb ...\n",
      "Unpacking libpthread-stubs0-dev:amd64 (0.4-1) ...\n",
      "Selecting previously unselected package libsm-dev:amd64.\n",
      "Preparing to unpack .../26-libsm-dev_2%3a1.2.3-1_amd64.deb ...\n",
      "Unpacking libsm-dev:amd64 (2:1.2.3-1) ...\n",
      "Selecting previously unselected package libxau-dev:amd64.\n",
      "Preparing to unpack .../27-libxau-dev_1%3a1.0.8-1+b2_amd64.deb ...\n",
      "Unpacking libxau-dev:amd64 (1:1.0.8-1+b2) ...\n",
      "Selecting previously unselected package libxdmcp-dev:amd64.\n",
      "Preparing to unpack .../28-libxdmcp-dev_1%3a1.1.2-3_amd64.deb ...\n",
      "Unpacking libxdmcp-dev:amd64 (1:1.1.2-3) ...\n",
      "Selecting previously unselected package xtrans-dev.\n",
      "Preparing to unpack .../29-xtrans-dev_1.3.5-1_all.deb ...\n",
      "Unpacking xtrans-dev (1.3.5-1) ...\n",
      "Selecting previously unselected package libxcb1-dev:amd64.\n",
      "Preparing to unpack .../30-libxcb1-dev_1.13.1-2_amd64.deb ...\n",
      "Unpacking libxcb1-dev:amd64 (1.13.1-2) ...\n",
      "Selecting previously unselected package libx11-dev:amd64.\n",
      "Preparing to unpack .../31-libx11-dev_2%3a1.6.7-1+deb10u2_amd64.deb ...\n",
      "Unpacking libx11-dev:amd64 (2:1.6.7-1+deb10u2) ...\n",
      "Selecting previously unselected package libxt-dev:amd64.\n",
      "Preparing to unpack .../32-libxt-dev_1%3a1.1.5-1+b3_amd64.deb ...\n",
      "Unpacking libxt-dev:amd64 (1:1.1.5-1+b3) ...\n",
      "Setting up java-common (0.71) ...\n",
      "Setting up libpthread-stubs0-dev:amd64 (0.4-1) ...\n",
      "Setting up xtrans-dev (1.3.5-1) ...\n",
      "Setting up libnspr4:amd64 (2:4.20-1) ...\n",
      "Setting up libpcsclite1:amd64 (1.8.24-1) ...\n",
      "Setting up fonts-dejavu-extra (2.37-1) ...\n",
      "Setting up xorg-sgml-doctools (1:1.11-1) ...\n",
      "Setting up libgtk2.0-common (2.24.32-3) ...\n",
      "Setting up libatk-wrapper-java (0.33.3-22+deb10u1) ...\n",
      "Setting up x11proto-dev (2018.4-4) ...\n",
      "Setting up libxau-dev:amd64 (1:1.0.8-1+b2) ...\n",
      "Setting up libice-dev:amd64 (2:1.0.9-2) ...\n",
      "Setting up libsm-dev:amd64 (2:1.2.3-1) ...\n",
      "Setting up libgtk2.0-0:amd64 (2.24.32-3) ...\n",
      "Setting up libnss3:amd64 (2:3.42.1-1+deb10u6) ...\n",
      "Setting up libatk-wrapper-java-jni:amd64 (0.33.3-22+deb10u1) ...\n",
      "Setting up libxdmcp-dev:amd64 (1:1.1.2-3) ...\n",
      "Setting up x11proto-core-dev (2018.4-4) ...\n",
      "Setting up libgail18:amd64 (2.24.32-3) ...\n",
      "Setting up libgtk2.0-bin (2.24.32-3) ...\n",
      "Setting up libxcb1-dev:amd64 (1.13.1-2) ...\n",
      "Setting up libgail-common:amd64 (2.24.32-3) ...\n",
      "Setting up libx11-dev:amd64 (2:1.6.7-1+deb10u2) ...\n",
      "Setting up libxt-dev:amd64 (1:1.1.5-1+b3) ...\n",
      "Setting up default-jre-headless (2:1.11-71) ...\n",
      "Setting up openjdk-11-jre-headless:amd64 (11.0.18+10-1~deb10u1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
      "Setting up openjdk-11-jre:amd64 (11.0.18+10-1~deb10u1) ...\n",
      "Setting up openjdk-11-jdk-headless:amd64 (11.0.18+10-1~deb10u1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
      "Setting up default-jre (2:1.11-71) ...\n",
      "Setting up default-jdk-headless (2:1.11-71) ...\n",
      "Setting up openjdk-11-jdk:amd64 (11.0.18+10-1~deb10u1) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
      "Setting up ca-certificates-java (20190405) ...\n",
      "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
      "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
      "Adding debian:Buypass_Class_3_Root_CA.pem\n",
      "Adding debian:DigiCert_Global_Root_CA.pem\n",
      "Adding debian:GTS_Root_R2.pem\n",
      "Adding debian:SecureTrust_CA.pem\n",
      "Adding debian:Starfield_Class_2_CA.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R2.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
      "Adding debian:EC-ACC.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:XRamp_Global_CA_Root.pem\n",
      "Adding debian:thawte_Primary_Root_CA_-_G2.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
      "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
      "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
      "Adding debian:E-Tugra_Certification_Authority.pem\n",
      "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
      "Adding debian:AffirmTrust_Premium_ECC.pem\n",
      "Adding debian:GeoTrust_Primary_Certification_Authority_-_G3.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
      "Adding debian:Network_Solutions_Certificate_Authority.pem\n",
      "Adding debian:thawte_Primary_Root_CA_-_G3.pem\n",
      "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
      "Adding debian:VeriSign_Class_3_Public_Primary_Certification_Authority_-_G5.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
      "Adding debian:GeoTrust_Universal_CA_2.pem\n",
      "Adding debian:GTS_Root_R3.pem\n",
      "Adding debian:SecureSign_RootCA11.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
      "Adding debian:Staat_der_Nederlanden_Root_CA_-_G3.pem\n",
      "Adding debian:Certigna.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GA_CA.pem\n",
      "Adding debian:Verisign_Class_3_Public_Primary_Certification_Authority_-_G3.pem\n",
      "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
      "Adding debian:Secure_Global_CA.pem\n",
      "Adding debian:CA_Disig_Root_R2.pem\n",
      "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
      "Adding debian:QuoVadis_Root_CA_3.pem\n",
      "Adding debian:Izenpe.com.pem\n",
      "Adding debian:UCA_Extended_Validation_Root.pem\n",
      "Adding debian:GTS_Root_R1.pem\n",
      "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
      "Adding debian:Taiwan_GRCA.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
      "Adding debian:DigiCert_Global_Root_G3.pem\n",
      "Adding debian:Buypass_Class_2_Root_CA.pem\n",
      "Adding debian:Certigna_Root_CA.pem\n",
      "Adding debian:TrustCor_ECA-1.pem\n",
      "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
      "Adding debian:VeriSign_Class_3_Public_Primary_Certification_Authority_-_G4.pem\n",
      "Adding debian:Staat_der_Nederlanden_EV_Root_CA.pem\n",
      "Adding debian:Security_Communication_Root_CA.pem\n",
      "Adding debian:AffirmTrust_Networking.pem\n",
      "Adding debian:QuoVadis_Root_CA.pem\n",
      "Adding debian:ISRG_Root_X1.pem\n",
      "Adding debian:Amazon_Root_CA_2.pem\n",
      "Adding debian:ePKI_Root_Certification_Authority.pem\n",
      "Adding debian:emSign_Root_CA_-_G1.pem\n",
      "Adding debian:COMODO_Certification_Authority.pem\n",
      "Adding debian:Go_Daddy_Class_2_CA.pem\n",
      "Adding debian:QuoVadis_Root_CA_2.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
      "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:LuxTrust_Global_Root_2.pem\n",
      "Adding debian:Comodo_AAA_Services_root.pem\n",
      "Adding debian:Chambers_of_Commerce_Root_-_2008.pem\n",
      "Adding debian:TrustCor_RootCert_CA-1.pem\n",
      "Adding debian:Global_Chambersign_Root_-_2008.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
      "Adding debian:GlobalSign_Root_CA.pem\n",
      "Adding debian:EE_Certification_Centre_Root_CA.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
      "Adding debian:TWCA_Root_Certification_Authority.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
      "Adding debian:TrustCor_RootCert_CA-2.pem\n",
      "Adding debian:Actalis_Authentication_Root_CA.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority.pem\n",
      "Adding debian:UCA_Global_G2_Root.pem\n",
      "Adding debian:certSIGN_ROOT_CA.pem\n",
      "Adding debian:Amazon_Root_CA_1.pem\n",
      "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
      "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
      "Adding debian:DST_Root_CA_X3.pem\n",
      "Adding debian:Staat_der_Nederlanden_Root_CA_-_G2.pem\n",
      "Adding debian:Amazon_Root_CA_3.pem\n",
      "Adding debian:ACCVRAIZ1.pem\n",
      "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
      "Adding debian:GeoTrust_Universal_CA.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
      "Adding debian:Certum_Trusted_Network_CA.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_1.pem\n",
      "Adding debian:CFCA_EV_ROOT.pem\n",
      "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
      "Adding debian:GeoTrust_Global_CA.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
      "Adding debian:Trustis_FPS_Root_CA.pem\n",
      "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
      "Adding debian:Amazon_Root_CA_4.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
      "Adding debian:Atos_TrustedRoot_2011.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
      "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
      "Adding debian:GeoTrust_Primary_Certification_Authority_-_G2.pem\n",
      "Adding debian:VeriSign_Universal_Root_Certification_Authority.pem\n",
      "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
      "Adding debian:Security_Communication_RootCA2.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
      "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
      "Adding debian:DigiCert_Global_Root_G2.pem\n",
      "Adding debian:Cybertrust_Global_Root.pem\n",
      "Adding debian:thawte_Primary_Root_CA.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
      "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
      "Adding debian:AffirmTrust_Commercial.pem\n",
      "Adding debian:TWCA_Global_Root_CA.pem\n",
      "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
      "Adding debian:GTS_Root_R4.pem\n",
      "Adding debian:SZAFIR_ROOT_CA2.pem\n",
      "Adding debian:AffirmTrust_Premium.pem\n",
      "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
      "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
      "Adding debian:Sonera_Class_2_Root_CA.pem\n",
      "Adding debian:Baltimore_CyberTrust_Root.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2011.pem\n",
      "Adding debian:emSign_Root_CA_-_C1.pem\n",
      "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
      "Adding debian:GeoTrust_Primary_Certification_Authority.pem\n",
      "done.\n",
      "Setting up default-jdk (2:1.11-71) ...\n",
      "Processing triggers for man-db (2.8.5-2) ...\n",
      "Processing triggers for ca-certificates (20200601~deb10u2) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "\n",
      "done.\n",
      "done.\n",
      "Processing triggers for fontconfig (2.13.1-2) ...\n",
      "Processing triggers for mime-support (3.62) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for libc-bin (2.28-10+deb10u2) ...\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud notebooks requires dependencies to be installed with '--user'\n",
    "! pip3 install pyspark\n",
    "! pip3 install --upgrade google-cloud-pipeline-components kfp --user -q\n",
    "! pip3 install pip install google-auth==2.13.0\n",
    "# Install latest JDK\n",
    "! sudo apt-get update\n",
    "! sudo apt-get install default-jdk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3971db1f-db1f-4279-8380-45a6b195660f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-pipeline-components kfp --user -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58552a67-9012-4ba9-82e9-d34299cd6d15",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Once you've installed the additional packages, you may need to restart the notebook kernel so it can find the packages.\n",
    "\n",
    "Uncomment & Run this cell if you have installed anything from above commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c19b5e-e7d9-416f-ad12-edc19b6877e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#    import IPython\n",
    "#    app = IPython.Application.instance()\n",
    "#    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fa95e-b745-4649-8040-af99e7a4013c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 2:\n",
    "#### Set Google Cloud properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920dd937-7a43-4709-8297-f9c346d7897c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Overview**  \n",
    "This notebook shows how to build a Vertex AI Pipeline to run a Dataproc Template   \n",
    "using the DataprocPySparkBatchOp component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ef85e-4cac-464c-9ed4-a66ea9c4f4c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# User Configuration\n",
    "# User Inputs\n",
    "\n",
    "get_project_id = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = get_project_id[0]\n",
    "REGION = \"\"  # example \"us-west1\"\n",
    "GCS_STAGING_LOCATION = \"gs://<bucket_name>\" # example \"gs://my_bucket_name\"\n",
    "SUBNET = \"\" # example \"projects/<project-id>/regions/<region-id>/subnetworks/<subnet-name>\" \n",
    "INPUT_HIVE_DATABASE= \"\"\n",
    "INPUT_HIVE_TABLES= \"*\" # example \"table1,table2,table3...\" or \"*\"\n",
    "OUTPUT_BIGQUERY_DATASET= \"\"\n",
    "TEMP_BUCKET= \"<bucket_name>\"\n",
    "HIVE_METASTORE= \"\" # example \"thrift://shubu-hive2bqnb-m:9083\"\n",
    "\n",
    "## Change if needed\n",
    "HIVE_OUTPUT_MODE=\"overwrite\"\n",
    "MAX_PARALLELISM=10 # Controlls number of parallel Dataproc Serverless Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bc061bf-10f4-49ef-8f9b-acfa87ebf8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User Configuration\n",
    "# User Inputs\n",
    "\n",
    "get_project_id = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = get_project_id[0]\n",
    "REGION =\"us-central1\"\n",
    "GCS_STAGING_LOCATION = \"gs://test-shubu\" # example \"gs://my_bucket_name\"\n",
    "SUBNET = \"projects/yadavaja-sandbox/regions/us-west1/subnetworks/test-subnet1\" \n",
    "INPUT_HIVE_DATABASE= \"default\"\n",
    "INPUT_HIVE_TABLES= \"*\" # example \"table1,table2,table3...\" or \"*\"\n",
    "OUTPUT_BIGQUERY_DATASET= \"hive2bq\"\n",
    "TEMP_BUCKET= \"test-shubu\"\n",
    "HIVE_METASTORE= \"thrift://shubu-hive2bqnb-m:9083\" # example \"thrift://shubu-hive2bqnb-m:9083\"\n",
    "\n",
    "## Change if needed\n",
    "HIVE_OUTPUT_MODE=\"overwrite\"\n",
    "MAX_PARALLELISM=10 # Controlls number of parallel Dataproc Serverless Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454bb0de-3e0a-4daa-92ed-1319e1d9604d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 3:\n",
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7aede36-3a0a-43f7-8e4c-db2d8087e289",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime\n",
    "from google_cloud_pipeline_components.experimental.dataproc import DataprocPySparkBatchOp\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b33331-b529-47ac-a08d-0c166acd264e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 4:\n",
    "#### Change working directory to the Dataproc Templates python folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da510653",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/dataproc-templates/python\n"
     ]
    }
   ],
   "source": [
    "cur_path = Path(os.getcwd())\n",
    "WORKING_DIRECTORY = os.path.join(cur_path.parent.parent ,'python')\n",
    "\n",
    "# If the above code doesn't fetches the correct path please\n",
    "# provide complete path to python folder in your dataproc \n",
    "# template repo which you cloned \n",
    "\n",
    "# WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/python/\"\n",
    "print(WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e0e80a-e8b7-4e54-9f99-b7874e164978",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/dataproc-templates/python\n"
     ]
    }
   ],
   "source": [
    "%cd $WORKING_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bb252-c3e3-4d56-b1ad-f52a2db52869",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 5:\n",
    "#### Build Dataproc Templates python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfaa8c93-5e69-48fc-984a-f4fa3b28519b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/dist.py:547: UserWarning: Normalizing '0.2.0-beta' to '0.2.0b0'\n",
      "  warnings.warn(tmpl.format(**locals()))\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing google_dataproc_templates.egg-info/PKG-INFO\n",
      "writing dependency_links to google_dataproc_templates.egg-info/dependency_links.txt\n",
      "writing requirements to google_dataproc_templates.egg-info/requires.txt\n",
      "writing top-level names to google_dataproc_templates.egg-info/top_level.txt\n",
      "reading manifest file 'google_dataproc_templates.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'google_dataproc_templates.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "running install_lib\n",
      "running build_py\n",
      "copying dataproc_templates/template_name.py -> build/lib/dataproc_templates\n",
      "copying test/pubsublite/test_pubsublite_to_bigtable.py -> build/lib/test/pubsublite\n",
      "copying test/pubsublite/test_pubsublite_to_gcs.py -> build/lib/test/pubsublite\n",
      "copying dataproc_templates/redshift/redshift_to_gcs.py -> build/lib/dataproc_templates/redshift\n",
      "copying dataproc_templates/jdbc/jdbc_to_bigquery.py -> build/lib/dataproc_templates/jdbc\n",
      "copying dataproc_templates/jdbc/jdbc_to_gcs.py -> build/lib/dataproc_templates/jdbc\n",
      "copying dataproc_templates/jdbc/jdbc_to_jdbc.py -> build/lib/dataproc_templates/jdbc\n",
      "copying dataproc_templates/pubsublite/pubsublite_to_gcs.py -> build/lib/dataproc_templates/pubsublite\n",
      "copying dataproc_templates/pubsublite/__init__.py -> build/lib/dataproc_templates/pubsublite\n",
      "copying dataproc_templates/pubsublite/pubsublite_to_bigtable.py -> build/lib/dataproc_templates/pubsublite\n",
      "copying dataproc_templates/mongo/mongo_to_gcs.py -> build/lib/dataproc_templates/mongo\n",
      "copying dataproc_templates/snowflake/snowflake_to_gcs.py -> build/lib/dataproc_templates/snowflake\n",
      "copying dataproc_templates/gcs/gcs_to_jdbc.py -> build/lib/dataproc_templates/gcs\n",
      "copying dataproc_templates/gcs/gcs_to_mongo.py -> build/lib/dataproc_templates/gcs\n",
      "copying dataproc_templates/util/dataframe_writer_wrappers.py -> build/lib/dataproc_templates/util\n",
      "copying dataproc_templates/util/template_constants.py -> build/lib/dataproc_templates/util\n",
      "copying dataproc_templates/kafka/kafka_to_gcs.py -> build/lib/dataproc_templates/kafka\n",
      "copying dataproc_templates/kafka/kafka_to_bq.py -> build/lib/dataproc_templates/kafka\n",
      "copying dataproc_templates/azure/azure_blob_storage_to_bigquery.py -> build/lib/dataproc_templates/azure\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/test\n",
      "creating build/bdist.linux-x86_64/egg/test/s3\n",
      "copying build/lib/test/s3/__init__.py -> build/bdist.linux-x86_64/egg/test/s3\n",
      "copying build/lib/test/s3/test_s3_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/s3\n",
      "creating build/bdist.linux-x86_64/egg/test/redshift\n",
      "copying build/lib/test/redshift/__init__.py -> build/bdist.linux-x86_64/egg/test/redshift\n",
      "copying build/lib/test/redshift/test_redshift_to_gcs.py -> build/bdist.linux-x86_64/egg/test/redshift\n",
      "creating build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/test_hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/__init__.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "creating build/bdist.linux-x86_64/egg/test/hive/util\n",
      "copying build/lib/test/hive/util/test_hive_ddl_extractor.py -> build/bdist.linux-x86_64/egg/test/hive/util\n",
      "copying build/lib/test/hive/util/__init__.py -> build/bdist.linux-x86_64/egg/test/hive/util\n",
      "copying build/lib/test/hive/test_hive_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "creating build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_jdbc.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/__init__.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_gcs.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "creating build/bdist.linux-x86_64/egg/test/pubsublite\n",
      "copying build/lib/test/pubsublite/__init__.py -> build/bdist.linux-x86_64/egg/test/pubsublite\n",
      "copying build/lib/test/pubsublite/test_pubsublite_to_bigtable.py -> build/bdist.linux-x86_64/egg/test/pubsublite\n",
      "copying build/lib/test/pubsublite/test_pubsublite_to_gcs.py -> build/bdist.linux-x86_64/egg/test/pubsublite\n",
      "creating build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/test_bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "creating build/bdist.linux-x86_64/egg/test/mongo\n",
      "copying build/lib/test/mongo/__init__.py -> build/bdist.linux-x86_64/egg/test/mongo\n",
      "copying build/lib/test/mongo/test_mongo_to_gcs.py -> build/bdist.linux-x86_64/egg/test/mongo\n",
      "creating build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_mongo.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_text_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/__init__.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_gcs.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "creating build/bdist.linux-x86_64/egg/test/cassandra\n",
      "copying build/lib/test/cassandra/test_cassandra_to_gcs.py -> build/bdist.linux-x86_64/egg/test/cassandra\n",
      "copying build/lib/test/cassandra/__init__.py -> build/bdist.linux-x86_64/egg/test/cassandra\n",
      "copying build/lib/test/cassandra/test_cassandra_to_bq.py -> build/bdist.linux-x86_64/egg/test/cassandra\n",
      "creating build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/__init__.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/test_argument_parsing.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "creating build/bdist.linux-x86_64/egg/test/azure\n",
      "copying build/lib/test/azure/__init__.py -> build/bdist.linux-x86_64/egg/test/azure\n",
      "copying build/lib/test/azure/test_azure_blob_storage_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/azure\n",
      "creating build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/test_hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/__init__.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/s3\n",
      "copying build/lib/dataproc_templates/s3/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/s3\n",
      "copying build/lib/dataproc_templates/s3/s3_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/s3\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/redshift\n",
      "copying build/lib/dataproc_templates/redshift/redshift_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/redshift\n",
      "copying build/lib/dataproc_templates/redshift/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/redshift\n",
      "copying build/lib/dataproc_templates/template_name.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/hive_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hive/util\n",
      "copying build/lib/dataproc_templates/hive/util/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive/util\n",
      "copying build/lib/dataproc_templates/hive/util/hive_ddl_extractor.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive/util\n",
      "copying build/lib/dataproc_templates/hive/hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_jdbc.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite\n",
      "copying build/lib/dataproc_templates/pubsublite/pubsublite_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite\n",
      "copying build/lib/dataproc_templates/pubsublite/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite\n",
      "copying build/lib/dataproc_templates/pubsublite/pubsublite_to_bigtable.py -> build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "copying build/lib/dataproc_templates/mongo/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "copying build/lib/dataproc_templates/mongo/mongo_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/snowflake\n",
      "copying build/lib/dataproc_templates/snowflake/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/snowflake\n",
      "copying build/lib/dataproc_templates/snowflake/snowflake_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/snowflake\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_mongo.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/text_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "copying build/lib/dataproc_templates/cassandra/cassandra_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "copying build/lib/dataproc_templates/cassandra/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "copying build/lib/dataproc_templates/cassandra/cassandra_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/tracking.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/argument_parsing.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/dataframe_writer_wrappers.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/template_constants.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/dataframe_reader_wrappers.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/kafka\n",
      "copying build/lib/dataproc_templates/kafka/kafka_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/kafka\n",
      "copying build/lib/dataproc_templates/kafka/kafka_to_bq.py -> build/bdist.linux-x86_64/egg/dataproc_templates/kafka\n",
      "copying build/lib/dataproc_templates/kafka/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/kafka\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/azure\n",
      "copying build/lib/dataproc_templates/azure/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/azure\n",
      "copying build/lib/dataproc_templates/azure/azure_blob_storage_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/azure\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/base_template.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/s3/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/s3/test_s3_to_bigquery.py to test_s3_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/redshift/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/redshift/test_redshift_to_gcs.py to test_redshift_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_bigquery.py to test_hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/util/test_hive_ddl_extractor.py to test_hive_ddl_extractor.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_gcs.py to test_hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_jdbc.py to test_jdbc_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_gcs.py to test_jdbc_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_bigquery.py to test_jdbc_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/pubsublite/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/pubsublite/test_pubsublite_to_bigtable.py to test_pubsublite_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/pubsublite/test_pubsublite_to_gcs.py to test_pubsublite_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/test_bigquery_to_gcs.py to test_bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/mongo/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/mongo/test_mongo_to_gcs.py to test_mongo_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_mongo.py to test_gcs_to_mongo.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_jdbc.py to test_gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_text_to_bigquery.py to test_text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigquery.py to test_gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigtable.py to test_gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_gcs.py to test_gcs_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/cassandra/test_cassandra_to_gcs.py to test_cassandra_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/cassandra/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/cassandra/test_cassandra_to_bq.py to test_cassandra_to_bq.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/test_argument_parsing.py to test_argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/azure/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/azure/test_azure_blob_storage_to_bigquery.py to test_azure_blob_storage_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/test_hbase_to_gcs.py to test_hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/s3/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/s3/s3_to_bigquery.py to s3_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/redshift/redshift_to_gcs.py to redshift_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/redshift/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/template_name.py to template_name.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_gcs.py to hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/util/hive_ddl_extractor.py to hive_ddl_extractor.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_bigquery.py to hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_bigquery.py to jdbc_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_gcs.py to jdbc_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_jdbc.py to jdbc_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite/pubsublite_to_gcs.py to pubsublite_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite/pubsublite_to_bigtable.py to pubsublite_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/bigquery_to_gcs.py to bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/mongo/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/mongo/mongo_to_gcs.py to mongo_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/snowflake/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/snowflake/snowflake_to_gcs.py to snowflake_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_jdbc.py to gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigquery.py to gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_gcs.py to gcs_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigtable.py to gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_mongo.py to gcs_to_mongo.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/text_to_bigquery.py to text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/cassandra/cassandra_to_bigquery.py to cassandra_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/cassandra/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/cassandra/cassandra_to_gcs.py to cassandra_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/tracking.py to tracking.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/argument_parsing.py to argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/dataframe_writer_wrappers.py to dataframe_writer_wrappers.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/template_constants.py to template_constants.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/dataframe_reader_wrappers.py to dataframe_reader_wrappers.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/kafka/kafka_to_gcs.py to kafka_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/kafka/kafka_to_bq.py to kafka_to_bq.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/kafka/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/azure/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/azure/azure_blob_storage_to_bigquery.py to azure_blob_storage_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/hbase_to_gcs.py to hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/base_template.py to base_template.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/google_dataproc_templates-0.2.0b0-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Will rename output .egg file from dist/google_dataproc_templates-0.2.0b0-py3.7.egg to dist/dataproc_templates_distribution.egg\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_EGG_FILE = \"dist/dataproc_templates_distribution.egg\"\n",
    "! python ./setup.py bdist_egg --output=$PACKAGE_EGG_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528c874-b948-40c1-b57c-afff76d80b47",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 6:\n",
    "#### Copy package to the GCS bucket\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/storage.objectCreator\n",
    " - roles/storage.objectViewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd85ce-46f6-4b35-8997-2189cf1b2eee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!gsutil cp main.py $GCS_STAGING_LOCATION/\n",
    "!gsutil cp $PACKAGE_EGG_FILE $GCS_STAGING_LOCATION/dist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c02e65ec-3cf8-4ccc-a6fc-f2696ea21e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Configuration\n",
    "# User Inputs\n",
    "\n",
    "get_project_id = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = get_project_id[0]\n",
    "REGION =\"us-west1\"\n",
    "GCS_STAGING_LOCATION = \"gs://test-shubu\" # example \"gs://my_bucket_name\"\n",
    "SUBNET = \"projects/yadavaja-sandbox/regions/us-west1/subnetworks/test-subnet1\" \n",
    "INPUT_HIVE_DATABASE= \"default\"\n",
    "INPUT_HIVE_TABLES= \"*\" # example \"table1,table2,table3...\" or \"*\"\n",
    "OUTPUT_BIGQUERY_DATASET= \"hive2bq\"\n",
    "TEMP_BUCKET= \"test-shubu\"\n",
    "HIVE_METASTORE= \"thrift://shubu-hive2bqnb-m:9083\" # example \"thrift://shubu-hive2bqnb-m:9083\"\n",
    "\n",
    "## Change if needed\n",
    "HIVE_OUTPUT_MODE=\"overwrite\"\n",
    "MAX_PARALLELISM=10 # Controlls number of parallel Dataproc Serverless Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d8c5f-b28b-4fbb-b585-cb2f1d9c1915",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 7:\n",
    "#### Get Hive Tables \n",
    "In case user wants to load all the Hive tables from the database, we need to get the table list using the metastore.\n",
    "\n",
    "Below cell will fetch all tables from the Hive database by running a Spark SQL query using the provided Hive Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3751abe3-47b7-4b4a-81f4-498da750d884",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thrift://shubu-hive2bqnb-m:9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "23/04/28 19:20:42 WARN metastore: Failed to connect to the MetaStore Server...\n",
      "23/04/28 19:20:43 WARN metastore: Failed to connect to the MetaStore Server...\n",
      "23/04/28 19:20:44 WARN metastore: Failed to connect to the MetaStore Server...\n",
      "23/04/28 19:20:45 WARN HiveClientImpl: HiveClient got thrift exception, destroying client and retrying (0 tries remaining)\n",
      "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1567)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:408)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:408)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:176)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:174)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:123)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:123)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1056)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1042)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1034)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.listTables(V2SessionCatalog.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1742)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)\n",
      "\t... 68 more\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\n",
      "\t... 75 more\n",
      "Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.UnknownHostException: shubu-hive2bqnb-m\n",
      "\tat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:478)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3607)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3659)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3639)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1563)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1552)\n",
      "\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:408)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:408)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:146)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:176)\n",
      "\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:174)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:70)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:123)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:123)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1056)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1042)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1034)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.listTables(V2SessionCatalog.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.net.UnknownHostException: shubu-hive2bqnb-m\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\n",
      "\t... 83 more\n",
      ")\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:527)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\t... 80 more\n",
      "23/04/28 19:20:46 WARN HiveClientImpl: Deadline exceeded\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_8044/2708250190.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0menableHiveSupport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mTABLE_LIST_DF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"show tables in \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mINPUT_HIVE_DATABASE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mTABLE_LIST\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTABLE_LIST_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tableName\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Table Sets to Migrate: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m             \u001b[0mlitArgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1323\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(HIVE_METASTORE)\n",
    "if INPUT_HIVE_TABLES==\"*\":\n",
    "    #os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    #os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "    spark=SparkSession.builder \\\n",
    "          .master(\"local\")\\\n",
    "          .appName(\"Spark Job to get HIVE table list\") \\\n",
    "          .config(\"hive.metastore.uris\",HIVE_METASTORE) \\\n",
    "          .enableHiveSupport() \\\n",
    "          .getOrCreate()  \n",
    "    TABLE_LIST_DF=spark.sql(\"show tables in \"+INPUT_HIVE_DATABASE)\n",
    "    TABLE_LIST=TABLE_LIST_DF.select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "    print(\"Table Sets to Migrate: \")\n",
    "    print(TABLE_LIST)\n",
    "    spark.stop()\n",
    "else:\n",
    "    TABLE_LIST=INPUT_HIVE_TABLES.split(\",\")\n",
    "    print(\"Table Sets to Migrate: \")\n",
    "    print(TABLE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef898a20-b870-4216-8780-ff3c6d3546f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5fbc306-ca17-4bc7-a855-7afcc9ee2324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: ./../../python: No such file or directory\n",
      "thrift://shubu-hive2bqnb-m:9083\n"
     ]
    }
   ],
   "source": [
    "#%%bash\n",
    "!cd ./../../python\n",
    "os.environ[\"GCP_PROJECT\"]=get_project_id[0]\n",
    "os.environ[\"REGION\"]=REGION\n",
    "os.environ[\"GCS_STAGING_LOCATION\"]=GCS_STAGING_LOCATION\n",
    "os.environ[\"SUBNET\"]=SUBNET\n",
    "os.environ[\"HIVE_METASTORE\"]=HIVE_METASTORE\n",
    "os.environ[\"\"]=\n",
    "!echo $HIVE_METASTORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36339d20-3039-4cd5-bba9-01a3d2bb6b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: ./../../python: No such file or directory\n",
      "total 68\n",
      "drwxr-xr-x  2 jupyter jupyter 4096 Apr  9 18:03 bin\n",
      "-rw-r--r--  1 jupyter jupyter  677 Apr  9 18:03 MANIFEST.in\n",
      "-rw-r--r--  1 jupyter jupyter 3327 Apr  9 18:03 setup.py\n",
      "-rw-r--r--  1 jupyter jupyter  668 Apr  9 18:03 setup.cfg\n",
      "-rw-r--r--  1 jupyter jupyter  241 Apr  9 18:03 release-please-config.json\n",
      "drwxr-xr-x 15 jupyter jupyter 4096 Apr  9 18:03 test\n",
      "drwxr-xr-x  4 jupyter jupyter 4096 Apr  9 18:57 build\n",
      "-rw-r--r--  1 jupyter jupyter 9618 Apr 28 18:15 README.md\n",
      "-rw-r--r--  1 jupyter jupyter   27 Apr 28 18:15 version.py\n",
      "-rw-r--r--  1 jupyter jupyter  150 Apr 28 18:15 requirements.txt\n",
      "-rw-r--r--  1 jupyter jupyter 6145 Apr 28 18:15 main.py\n",
      "drwxr-xr-x 16 jupyter jupyter 4096 Apr 28 18:15 dataproc_templates\n",
      "drwxr-xr-x  2 jupyter jupyter 4096 Apr 28 18:54 google_dataproc_templates.egg-info\n",
      "drwxr-xr-x  2 jupyter jupyter 4096 Apr 28 18:54 dist\n",
      "GCP_PROJECT=yadavaja-sandbox\n",
      "REGION=us-central1\n",
      "GCS_STAGING_LOCATION=gs://test-shubu\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/dist.py:547: UserWarning: Normalizing '0.2.0-beta' to '0.2.0b0'\n",
      "  warnings.warn(tmpl.format(**locals()))\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing google_dataproc_templates.egg-info/PKG-INFO\n",
      "writing dependency_links to google_dataproc_templates.egg-info/dependency_links.txt\n",
      "writing requirements to google_dataproc_templates.egg-info/requires.txt\n",
      "writing top-level names to google_dataproc_templates.egg-info/top_level.txt\n",
      "reading manifest file 'google_dataproc_templates.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'google_dataproc_templates.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/test\n",
      "creating build/bdist.linux-x86_64/egg/test/s3\n",
      "copying build/lib/test/s3/__init__.py -> build/bdist.linux-x86_64/egg/test/s3\n",
      "copying build/lib/test/s3/test_s3_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/s3\n",
      "creating build/bdist.linux-x86_64/egg/test/redshift\n",
      "copying build/lib/test/redshift/__init__.py -> build/bdist.linux-x86_64/egg/test/redshift\n",
      "copying build/lib/test/redshift/test_redshift_to_gcs.py -> build/bdist.linux-x86_64/egg/test/redshift\n",
      "creating build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/test_hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/__init__.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "creating build/bdist.linux-x86_64/egg/test/hive/util\n",
      "copying build/lib/test/hive/util/test_hive_ddl_extractor.py -> build/bdist.linux-x86_64/egg/test/hive/util\n",
      "copying build/lib/test/hive/util/__init__.py -> build/bdist.linux-x86_64/egg/test/hive/util\n",
      "copying build/lib/test/hive/test_hive_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "creating build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_jdbc.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/__init__.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_gcs.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "creating build/bdist.linux-x86_64/egg/test/pubsublite\n",
      "copying build/lib/test/pubsublite/__init__.py -> build/bdist.linux-x86_64/egg/test/pubsublite\n",
      "copying build/lib/test/pubsublite/test_pubsublite_to_bigtable.py -> build/bdist.linux-x86_64/egg/test/pubsublite\n",
      "copying build/lib/test/pubsublite/test_pubsublite_to_gcs.py -> build/bdist.linux-x86_64/egg/test/pubsublite\n",
      "creating build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/test_bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "creating build/bdist.linux-x86_64/egg/test/mongo\n",
      "copying build/lib/test/mongo/__init__.py -> build/bdist.linux-x86_64/egg/test/mongo\n",
      "copying build/lib/test/mongo/test_mongo_to_gcs.py -> build/bdist.linux-x86_64/egg/test/mongo\n",
      "creating build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_mongo.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_text_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/__init__.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_gcs.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "creating build/bdist.linux-x86_64/egg/test/cassandra\n",
      "copying build/lib/test/cassandra/test_cassandra_to_gcs.py -> build/bdist.linux-x86_64/egg/test/cassandra\n",
      "copying build/lib/test/cassandra/__init__.py -> build/bdist.linux-x86_64/egg/test/cassandra\n",
      "copying build/lib/test/cassandra/test_cassandra_to_bq.py -> build/bdist.linux-x86_64/egg/test/cassandra\n",
      "creating build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/__init__.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/test_argument_parsing.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "creating build/bdist.linux-x86_64/egg/test/azure\n",
      "copying build/lib/test/azure/__init__.py -> build/bdist.linux-x86_64/egg/test/azure\n",
      "copying build/lib/test/azure/test_azure_blob_storage_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/azure\n",
      "creating build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/test_hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/__init__.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/s3\n",
      "copying build/lib/dataproc_templates/s3/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/s3\n",
      "copying build/lib/dataproc_templates/s3/s3_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/s3\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/redshift\n",
      "copying build/lib/dataproc_templates/redshift/redshift_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/redshift\n",
      "copying build/lib/dataproc_templates/redshift/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/redshift\n",
      "copying build/lib/dataproc_templates/template_name.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/hive_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hive/util\n",
      "copying build/lib/dataproc_templates/hive/util/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive/util\n",
      "copying build/lib/dataproc_templates/hive/util/hive_ddl_extractor.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive/util\n",
      "copying build/lib/dataproc_templates/hive/hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_jdbc.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite\n",
      "copying build/lib/dataproc_templates/pubsublite/pubsublite_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite\n",
      "copying build/lib/dataproc_templates/pubsublite/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite\n",
      "copying build/lib/dataproc_templates/pubsublite/pubsublite_to_bigtable.py -> build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "copying build/lib/dataproc_templates/mongo/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "copying build/lib/dataproc_templates/mongo/mongo_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/snowflake\n",
      "copying build/lib/dataproc_templates/snowflake/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/snowflake\n",
      "copying build/lib/dataproc_templates/snowflake/snowflake_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/snowflake\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_mongo.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/text_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "copying build/lib/dataproc_templates/cassandra/cassandra_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "copying build/lib/dataproc_templates/cassandra/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "copying build/lib/dataproc_templates/cassandra/cassandra_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/tracking.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/argument_parsing.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/dataframe_writer_wrappers.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/template_constants.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/dataframe_reader_wrappers.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/kafka\n",
      "copying build/lib/dataproc_templates/kafka/kafka_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/kafka\n",
      "copying build/lib/dataproc_templates/kafka/kafka_to_bq.py -> build/bdist.linux-x86_64/egg/dataproc_templates/kafka\n",
      "copying build/lib/dataproc_templates/kafka/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/kafka\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/azure\n",
      "copying build/lib/dataproc_templates/azure/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/azure\n",
      "copying build/lib/dataproc_templates/azure/azure_blob_storage_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/azure\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/base_template.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/s3/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/s3/test_s3_to_bigquery.py to test_s3_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/redshift/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/redshift/test_redshift_to_gcs.py to test_redshift_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_bigquery.py to test_hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/util/test_hive_ddl_extractor.py to test_hive_ddl_extractor.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_gcs.py to test_hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_jdbc.py to test_jdbc_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_gcs.py to test_jdbc_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_bigquery.py to test_jdbc_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/pubsublite/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/pubsublite/test_pubsublite_to_bigtable.py to test_pubsublite_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/pubsublite/test_pubsublite_to_gcs.py to test_pubsublite_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/test_bigquery_to_gcs.py to test_bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/mongo/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/mongo/test_mongo_to_gcs.py to test_mongo_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_mongo.py to test_gcs_to_mongo.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_jdbc.py to test_gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_text_to_bigquery.py to test_text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigquery.py to test_gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigtable.py to test_gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_gcs.py to test_gcs_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/cassandra/test_cassandra_to_gcs.py to test_cassandra_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/cassandra/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/cassandra/test_cassandra_to_bq.py to test_cassandra_to_bq.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/test_argument_parsing.py to test_argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/azure/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/azure/test_azure_blob_storage_to_bigquery.py to test_azure_blob_storage_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/test_hbase_to_gcs.py to test_hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/s3/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/s3/s3_to_bigquery.py to s3_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/redshift/redshift_to_gcs.py to redshift_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/redshift/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/template_name.py to template_name.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_gcs.py to hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/util/hive_ddl_extractor.py to hive_ddl_extractor.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_bigquery.py to hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_bigquery.py to jdbc_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_gcs.py to jdbc_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_jdbc.py to jdbc_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite/pubsublite_to_gcs.py to pubsublite_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/pubsublite/pubsublite_to_bigtable.py to pubsublite_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/bigquery_to_gcs.py to bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/mongo/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/mongo/mongo_to_gcs.py to mongo_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/snowflake/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/snowflake/snowflake_to_gcs.py to snowflake_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_jdbc.py to gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigquery.py to gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_gcs.py to gcs_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigtable.py to gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_mongo.py to gcs_to_mongo.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/text_to_bigquery.py to text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/cassandra/cassandra_to_bigquery.py to cassandra_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/cassandra/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/cassandra/cassandra_to_gcs.py to cassandra_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/tracking.py to tracking.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/argument_parsing.py to argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/dataframe_writer_wrappers.py to dataframe_writer_wrappers.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/template_constants.py to template_constants.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/dataframe_reader_wrappers.py to dataframe_reader_wrappers.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/kafka/kafka_to_gcs.py to kafka_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/kafka/kafka_to_bq.py to kafka_to_bq.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/kafka/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/azure/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/azure/azure_blob_storage_to_bigquery.py to azure_blob_storage_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/hbase_to_gcs.py to hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/base_template.py to base_template.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/google_dataproc_templates-0.2.0b0-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Will rename output .egg file from dist/google_dataproc_templates-0.2.0b0-py3.7.egg to dist/dataproc_templates_distribution.egg\n",
      "Triggering Spark Submit job\n",
      "gcloud beta dataproc batches submit pyspark ./bin/../main.py --version=1.1 --project=yadavaja-sandbox --region=us-central1 --jars=file:///usr/lib/spark/external/spark-avro.jar --labels=job_type=dataproc_template --deps-bucket=gs://test-shubu --py-files=./bin/../dist/dataproc_templates_distribution.egg --subnet=projects/yadavaja-sandbox/regions/us-west1/subnetworks/test-subnet1 --properties=spark.hadoop.hive.metastore.uris=thrift://shubu-hive2bqnb-m:9083 -- --template=HIVEDDLEXTRACTOR --hive.ddl.extractor.input.database=default --hive.ddl.extractor.output.path=gs://test-shubu/hiveddl/output\n",
      "Batch [dc11751bc00548ef8a6b6d6a81676f35] submitted.\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.dataproc.batches.submit.pyspark) Batch job is FAILED. Detail: Invalid value for field 'resource.networkInterfaces[0].subnetwork': 'https://www.googleapis.com/compute/v1/projects/yadavaja-sandbox/regions/us-west1/subnetworks/test-subnet1'. Instance must be in the same region as the subnetwork.\n",
      "Running auto diagnostics on the batch. It may take few minutes before diagnostics output is available. Please check diagnostics output by running 'gcloud dataproc batches describe' command.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# export GCP_PROJECT=get_project_id[0]\n",
    "# export REGION=REGION\n",
    "# export GCS_STAGING_LOCATION=GCS_STAGING_LOCATION\n",
    "# export SUBNET=SUBNET\n",
    "! ./bin/start.sh \\\n",
    "    --properties=spark.hadoop.hive.metastore.uris=\"thrift://shubu-hive2bqnb-m:9083\" \\\n",
    "    -- --template=HIVEDDLEXTRACTOR \\\n",
    "    --hive.ddl.extractor.input.database=\"default\" \\\n",
    "    --hive.ddl.extractor.output.path=\"gs://test-shubu/hiveddl/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbaf266-91a0-46e2-8a88-b594b9301c83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 8:\n",
    "\n",
    "Split Hive Tables list based on MAX_PARALLELISM value provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b170e58-7acf-4aae-9619-2361eef0ed12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "COMPLETE_LIST = copy.deepcopy(TABLE_LIST)\n",
    "PARALLEL_JOBS = len(TABLE_LIST)//MAX_PARALLELISM\n",
    "JOB_LIST = []\n",
    "while len(COMPLETE_LIST) > 0:\n",
    "    SUB_LIST = []\n",
    "    for i in range(MAX_PARALLELISM):\n",
    "        if len(COMPLETE_LIST)>0 :\n",
    "            SUB_LIST.append(COMPLETE_LIST[0])\n",
    "            COMPLETE_LIST.pop(0)\n",
    "        else:\n",
    "            break\n",
    "    JOB_LIST.append(SUB_LIST)\n",
    "print(\"List of tables for execution : \")\n",
    "print(JOB_LIST)+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5a0b9-d2b2-4072-aa4f-3748609f7cdc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 9:\n",
    "\n",
    "Set Dataproc Template Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73197165-0ea0-4732-841d-8ba4bcd8a94d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\"\n",
    "MAIN_PYTHON_FILE = GCS_STAGING_LOCATION + \"/main.py\"\n",
    "JARS = [\"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\"]\n",
    "PYTHON_FILE_URIS = [GCS_STAGING_LOCATION + \"/dist/dataproc_templates_distribution.egg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332168ba-6e69-4f6e-aa03-38b4965b5304",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Step 10:\n",
    "#### Build pipeline and run Dataproc Template on Vertex AI Pipelines to migrate Hive tables to BigQuery\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/dataproc.editor\n",
    " - roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c13bf-c121-4465-88fc-04778ef35a36",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "runtime_prop={}\n",
    "runtime_prop['spark.hadoop.hive.metastore.uris']=HIVE_METASTORE\n",
    "runtime_prop['mapreduce.fileoutputcommitter.marksuccessfuljobs'] = \"false\"\n",
    "\n",
    "def migrate_hive(EXECUTION_LIST):\n",
    "    EXECUTION_LIST = EXECUTION_LIST\n",
    "    aiplatform.init(project=PROJECT_ID, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name=\"hive-to-bq-pyspark\",\n",
    "        description=\"Pipeline to migrate tables from hive to bq\",\n",
    "    )\n",
    "    def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        location: str = REGION,\n",
    "        main_python_file_uri: str = MAIN_PYTHON_FILE,\n",
    "        python_file_uris: list = PYTHON_FILE_URIS,\n",
    "        jar_file_uris: list = JARS,\n",
    "        subnetwork_uri: str = SUBNET\n",
    "    ):\n",
    "        for table in EXECUTION_LIST:\n",
    "            BATCH_ID = \"hive2bq-{}-{}\".format(table,datetime.now().strftime(\"%s\")).replace('_','-')\n",
    "            TEMPLATE_SPARK_ARGS = [\n",
    "                                    \"--template=HIVETOBIGQUERY\",\n",
    "                                    \"--hive.bigquery.input.database={}\".format(INPUT_HIVE_DATABASE),\n",
    "                                    \"--hive.bigquery.input.table={}\".format(table),\n",
    "                                    \"--hive.bigquery.output.table={}\".format(table),\n",
    "                                    \"--hive.bigquery.output.dataset={}\".format(OUTPUT_BIGQUERY_DATASET),\n",
    "                                    \"--hive.bigquery.output.mode={}\".format(HIVE_OUTPUT_MODE),\n",
    "                                    \"--hive.bigquery.temp.bucket.name={}\".format(TEMP_BUCKET)                                    ]\n",
    "            _ = DataprocPySparkBatchOp(\n",
    "                project=project_id,\n",
    "                location=location,\n",
    "                batch_id=BATCH_ID,\n",
    "                main_python_file_uri=main_python_file_uri,\n",
    "                python_file_uris=python_file_uris,\n",
    "                jar_file_uris=jar_file_uris,\n",
    "                subnetwork_uri=subnetwork_uri,\n",
    "                runtime_config_properties=runtime_prop,\n",
    "                runtime_config_version=\"1.1\", # issue 665\n",
    "                args=TEMPLATE_SPARK_ARGS,\n",
    "            )\n",
    "            time.sleep(5)\n",
    "\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "    pipeline = aiplatform.PipelineJob(\n",
    "            display_name=\"pipeline\",\n",
    "            template_path=\"pipeline.json\",\n",
    "            pipeline_root=PIPELINE_ROOT,\n",
    "            enable_caching=False,\n",
    "            )\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba4453-ddf9-4f35-901e-ab20433ae7fa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Step 11:\n",
    "\n",
    "Run Dataproc Batch Template based on Hive Tables list calculated in Step 8.\n",
    "\n",
    "The below cell will call function migrate_hive to migrate tables using dataproc serverless batch job and also add an entry in Audit Table for each Table Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580bcc2-9302-48dc-8a47-7ccac603fbd7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "AUDIT_DICT={}\n",
    "AUDIT_DF = pd.DataFrame(columns=[\"Source_DB_Name\",\"Source_Table_Set\",\"Target_DB_Name\",\"Target_Table_Set\",\"Job_Start_Time\",\"Job_End_Time\",\"Job_Status\"])\n",
    " \n",
    "for execution_list in JOB_LIST:\n",
    "    print(\"\\n\\nLoading Table Set: \"+str(execution_list))\n",
    "    AUDIT_DICT[\"Source_DB_Name\"]=INPUT_HIVE_DATABASE\n",
    "    AUDIT_DICT[\"Source_Table_Set\"]='|'.join(execution_list)\n",
    "    AUDIT_DICT[\"Target_DB_Name\"]=OUTPUT_BIGQUERY_DATASET\n",
    "    AUDIT_DICT[\"Target_Table_Set\"]='|'.join(execution_list)\n",
    "    AUDIT_DICT[\"Job_Start_Time\"]=str(datetime.now())\n",
    "    try:\n",
    "        migrate_hive(execution_list)\n",
    "    except Exception:\n",
    "        AUDIT_DICT[\"Job_Status\"]=\"FAIL\"\n",
    "        print(\"\\n\\nSome Error Occured while loading Table Set: \"+str(execution_list))\n",
    "    else:\n",
    "        AUDIT_DICT[\"Job_Status\"]=\"PASS\"\n",
    "        print(\"\\n\\nLoaded Table Set: \"+str(execution_list))\n",
    "\n",
    "    AUDIT_DICT[\"Job_End_Time\"]=str(datetime.now())\n",
    "    AUDIT_DF=AUDIT_DF.append(AUDIT_DICT, ignore_index = True)\n",
    "    \n",
    "if AUDIT_DF.empty:\n",
    "    print(\"Audit Dataframe is Empty\")\n",
    "else:\n",
    "    print(AUDIT_DF)\n",
    "    AUDIT_DF.to_csv(\"gs://\"+TEMP_BUCKET+\"/audit/audit_file_{}.csv\".format(str(datetime.now())),index=False,header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5417b-ff58-492e-9957-356644627234",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m106",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m106"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
