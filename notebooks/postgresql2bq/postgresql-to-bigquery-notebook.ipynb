{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5876b8a-4fc8-43d5-93f6-fbd19fb2c433",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PostgreSQL to BigQuery Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9e6d2-f721-48cc-a6a7-01787aa9362c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b8102d-df42-448a-9df4-35ae6b8fae07",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "* [DataprocPySparkBatchOp reference](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html) \n",
    "* [Kubeflow SDK Overview](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)\n",
    "* [Dataproc Serverless in Vertex AI Pipelines tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)\n",
    "* [Build a Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)\n",
    "\n",
    "This notebook is built to run a Vertex AI User-Managed Notebook using the default Compute Engine Service Account. Check the Dataproc Serverless in Vertex AI Pipelines tutorial linked above to learn how to setup a different Service Account.\n",
    "\n",
    "## Permissions\n",
    "Make sure that the service account used to run the notebook has the following roles:\n",
    "\n",
    "* roles/aiplatform.serviceAgent\n",
    "* roles/aiplatform.customCodeServiceAgent\n",
    "* roles/storage.objectCreator\n",
    "* roles/storage.objectViewer\n",
    "* roles/dataproc.editor\n",
    "* roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06a8de-c8b0-4ada-81c7-54606c0afdb9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 1: Install Libraries\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE: </b>Run Step 1 one time for each new notebook instance</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258dae17-19f2-410a-8cc9-f5980ccc30f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install SQLAlchemy\n",
    "!pip3 install --upgrade google-cloud-pipeline-components kfp --user -q\n",
    "!pip3 install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedd295-09d0-48f1-b7c9-57d87b178502",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update -y\n",
    "!sudo apt-get install default-jdk -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7126d-fc9c-4f04-8031-cbd835f891c6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Once you've installed the additional packages, you may need to restart the notebook kernel so it can find the packages.\n",
    "\n",
    "Uncomment & Run this cell if you have installed anything from above commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74245439-10f3-4631-b8cd-4c077318b737",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import IPython\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe84b1-eb32-4cc2-a4e2-f0bd98da5b90",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f225135-cfe5-4ddd-9659-ead437aa414c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import sys, os\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import DataprocPySparkBatchOp\n",
    "except ModuleNotFoundError:\n",
    "    from google_cloud_pipeline_components.v1.dataproc import DataprocPySparkBatchOp\n",
    "    \n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "import psycopg2\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0cf264-6eef-441d-a0a4-0c7bdc3d5b96",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 3: Assign Parameters\n",
    "\n",
    "## Step 3.1 Common Parameters\n",
    "\n",
    "GCP_PROJECT : GCP project-id\n",
    "\n",
    "REGION : GCP region\n",
    "\n",
    "GCS_STAGING_LOCATION : Cloud Storage staging location to be used for this notebook to store artifacts\n",
    "\n",
    "SUBNET : VPC subnet\n",
    "\n",
    "JARS : list of jars. For this notebook postgre connectora and postgres connectorjar is required in addition with the dataproc template\n",
    "\n",
    "MAX_PARALLELISM : Parameter for number of jobs to run in parallel default value is 5\n",
    "\n",
    "SERVICE_ACCOUNT: Custom service account email to use for vertex ai pipeline and dataproc job with above mentioned permissions\n",
    "\n",
    "POSTGRESQL_TO_BIGQUERY_JOBS : List of bigquery job IDs that will be created by Vertex AI pipelines to migrate data from source to BQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46af62-62b2-4ad3-87e1-ff232f524e64",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "IS_PARAMETERIZED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad4f98-8924-4ab9-8a7a-948cd82d1d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IS_PARAMETERIZED:\n",
    "    GCP_PROJECT = \"\"\n",
    "    REGION = \"\" # eg: us-central1 (any valid GCP region)\n",
    "    GCS_STAGING_LOCATION = \"\" # eg: gs://my-staging-bucket/sub-folder\n",
    "    SUBNET = \"projects/{project}/regions/{region}/subnetworks/{subnet}\"\n",
    "    MAX_PARALLELISM = 5 # max number of tables which will migrated parallelly \n",
    "    SERVICE_ACCOUNT = \"\" \n",
    "    \n",
    "# Do not change this parameter unless you want to refer below JARS from new location\n",
    "JARS = [GCS_STAGING_LOCATION + \"/jars/postgresql-42.2.6.jar\", GCS_STAGING_LOCATION + \"/jars/spark-bigquery-with-dependencies_2.12-0.27.0.jar\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee427d7c-edcb-46b2-a825-624db784aff0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "POSTGRESQL_TO_BIGQUERY_JOBS = []\n",
    "# If SERVICE_ACCOUNT is not specified it will take the one attached to Notebook\n",
    "if SERVICE_ACCOUNT == '':\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "    print(\"Service Account: \",SERVICE_ACCOUNT)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129a976-154d-4627-aa08-6430eac9b3fc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 3.2 PostgreSQL Parameters\n",
    "\n",
    "POSTGRES_HOST : PostgreSQL instance ip address\n",
    "\n",
    "POSTGRES_PORT : PostgreSQL instance port\n",
    "\n",
    "POSTGRES_USERNAME : PostgreSQL username\n",
    "\n",
    "POSTGRES_PASSWORD : PostgreSQL password\n",
    "\n",
    "POSTGRES_DATABASE : name of database that you want to migrate\n",
    "\n",
    "POSTGRESQL_TABLE_LIST: leave list empty for migrating complete database else provide tables as ['dbo.table1','sys.table2']. If this parameter is not empty, leave POSTGRESQL_SCHEMA_LIST empty.\n",
    "\n",
    "POSTGRESQL_SCHEMA_LIST: leave list empty for migrating complete database else provide schema as ['schema1','schema2'] for migrating all tables in specific schemas. If this parameter is not empty, leave POSTGRESQL_TABLE_LIST empty.\n",
    "\n",
    "BIGQUERY_DATASET: name of the dataset to migrate\n",
    "\n",
    "BIGQUERY_MODE:  output write mode (Default: overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c6c38-df60-4ef6-a893-7bba3a6060fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_PARAMETERIZED:\n",
    "    POSTGRESQL_HOST = \"\"\n",
    "    POSTGRESQL_PORT = \"\"\n",
    "    POSTGRESQL_USERNAME = \"\"\n",
    "    POSTGRESQL_PASSWORD = \"\"\n",
    "    POSTGRESQL_DATABASE = \"\"\n",
    "    POSTGRESQL_TABLE_LIST = [] # leave list empty for migrating complete database else provide tables as ['table1','table2']\n",
    "    POSTGRESQL_SCHEMA_LIST = [] # leave list empty for migrating complete database else provide schema as ['schema1','schema2'] for migrating all tables in specific schemas. If this parameter is not empty, leave POSTGRESQL_TABLE_LIST empty.\n",
    "\n",
    "    BIGQUERY_DATASET = \"\"\n",
    "    BIGQUERY_MODE = \"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd7d7f-b70e-4f09-8b7d-c96038cc892c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 3.3 Notebook Configuration Parameters\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>NOTE: </b>Below variables should not be changed unless required</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960dba0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_path = Path(os.getcwd())\n",
    "\n",
    "if IS_PARAMETERIZED:\n",
    "    WORKING_DIRECTORY = os.path.join(cur_path.parent ,'python')\n",
    "else:\n",
    "    WORKING_DIRECTORY = os.path.join(cur_path.parent.parent ,'python')\n",
    "\n",
    "# If the above code doesn't fetches the correct path please\n",
    "# provide complete path to python folder in your dataproc \n",
    "# template repo which you cloned \n",
    "\n",
    "# WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/python/\"\n",
    "print(WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17741a-37e6-4657-bb3b-11b584270015",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PY_JDBC_DRIVER = \"postgresql+psycopg2\"\n",
    "JDBC_DRIVER = \"org.postgresql.Driver\"\n",
    "JDBC_URL=\"jdbc:postgresql://{0}:{1}/{2}?user={3}&password={4}&reWriteBatchedInserts=True\".format(POSTGRESQL_HOST,POSTGRESQL_PORT,POSTGRESQL_DATABASE,POSTGRESQL_USERNAME,POSTGRESQL_PASSWORD)\n",
    "MAIN_CLASS = \"com.google.cloud.dataproc.templates.main.DataProcTemplate\"\n",
    "PACKAGE_EGG_FILE = \"dataproc_templates_distribution.egg\"\n",
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9323378-426e-4593-8b22-e6b20b64bfe9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Step 4: Generate PostgreSQL Table List\n",
    "\n",
    "This step creates list of tables for migration. \n",
    "\n",
    "* If POSTGRESQL_TABLE_LIST and POSTGRESQL_SCHEMA_LIST are kept empty, then all the tables in the POSTGRESQL_DATABASE are listed for migration.\n",
    "\n",
    "* If POSTGRESQL_SCHEMA_LIST is non empty, then all tables associated with the mentioned schemas will be listed for migration\n",
    "\n",
    "* If POSTGRESQL_TABLE_LIST is non empty, then the provided list of tables are selected for migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d75bb9-8d10-4f0e-989b-8b81727ada7f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# removing defaut value if the list is empty\n",
    "if len(POSTGRESQL_SCHEMA_LIST) == 1 and POSTGRESQL_SCHEMA_LIST[0] == '':\n",
    "    POSTGRESQL_SCHEMA_LIST.pop()\n",
    "\n",
    "\n",
    "if len(POSTGRESQL_TABLE_LIST) == 1 and POSTGRESQL_TABLE_LIST[0] == '':\n",
    "    POSTGRESQL_TABLE_LIST.pop()\n",
    "    \n",
    "if POSTGRESQL_SCHEMA_LIST and POSTGRESQL_TABLE_LIST:\n",
    "    print(POSTGRESQL_SCHEMA_LIST , POSTGRESQL_TABLE_LIST)\n",
    "    print(POSTGRESQL_SCHEMA_LIST and POSTGRESQL_TABLE_LIST)\n",
    "    sys.exit(\"Please provide values for either POSTGRESQL_SCHEMA_LIST OR POSTGRESQL_TABLE_LIST. Non empty values for both the values at the same time are not accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f433106-453d-445f-bcda-8aad4efc0d95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DB = sqlalchemy.create_engine(\n",
    "            sqlalchemy.engine.url.URL.create(\n",
    "                drivername=PY_JDBC_DRIVER,\n",
    "                username=POSTGRESQL_USERNAME,\n",
    "                password=POSTGRESQL_PASSWORD,\n",
    "                database=POSTGRESQL_DATABASE,\n",
    "                host=POSTGRESQL_HOST,\n",
    "                port=POSTGRESQL_PORT\n",
    "              )\n",
    "            )\n",
    "\n",
    "with DB.connect() as conn:\n",
    "        print(\"connected to database\")\n",
    "        if not POSTGRESQL_TABLE_LIST and not POSTGRESQL_SCHEMA_LIST: # Migrate all possible tables from database\n",
    "            results = conn.execute(text('select TABLE_SCHEMA,TABLE_NAME from INFORMATION_SCHEMA.Tables')).fetchall()\n",
    "\n",
    "        elif POSTGRESQL_SCHEMA_LIST and not POSTGRESQL_TABLE_LIST: # Only Migrate tables associated with the provided schema list\n",
    "            results = conn.execute(text(\"select TABLE_SCHEMA,TABLE_NAME from INFORMATION_SCHEMA.Tables where TABLE_SCHEMA in ('{}');\".format(\"','\".join(POSTGRESQL_SCHEMA_LIST)))).fetchall()\n",
    "\n",
    "        # when POSTGRESQL_TABLE_LIST is already not empty, only mentioned tables will be migrated\n",
    "\n",
    "        print(\"Total Tables = \", len(results))\n",
    "        for row in results:\n",
    "            POSTGRESQL_TABLE_LIST.append(row[0]+\".\"+row[1])\n",
    "\n",
    "        print(\"list of tables for migration :\")\n",
    "        print(POSTGRESQL_TABLE_LIST)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359b268-ceec-4694-91ec-63f8469040dc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 5: Get Primary Keys for partitioning the tables\n",
    "\n",
    "This step fetches primary key from POSTGRESQL_DATABASE for the tables listed for migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346e717-5bd3-471f-938e-b64fc1721b3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with DB.connect() as conn:\n",
    "\n",
    "    POSTGRESQL_TABLE_PRIMARY_KEYS = {}\n",
    "    for table in POSTGRESQL_TABLE_LIST:\n",
    "            primary_keys = []\n",
    "            results = conn.execute(text(\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS T JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE K ON K.CONSTRAINT_NAME=T.CONSTRAINT_NAME  WHERE  K.TABLE_NAME='{0}'  AND K.TABLE_SCHEMA='{1}' AND T.CONSTRAINT_TYPE='PRIMARY KEY';\".format(table.split(\".\")[1],table.split(\".\")[0]))).fetchall()\n",
    "            # print(results)\n",
    "            for row in results:\n",
    "                primary_keys.append(row[0])\n",
    "            if primary_keys:\n",
    "                POSTGRESQL_TABLE_PRIMARY_KEYS[table] = \",\".join(primary_keys)\n",
    "            else:\n",
    "                POSTGRESQL_TABLE_PRIMARY_KEYS[table] = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baf6f6-2c68-4c63-adfd-f474aea6c48e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pkDF = pd.DataFrame({\"table\" : POSTGRESQL_TABLE_LIST, \"primary_keys\": list(POSTGRESQL_TABLE_PRIMARY_KEYS.values())})\n",
    "print(\"Below are identified primary keys for migrating postgresql table to bigquery:\")\n",
    "pkDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba873f01-cda3-4954-aac6-6ddf7df54642",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 6: Create JAR files and Upload to Cloud Storage\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE: </b> Run Step 6 one time for each new notebook instance</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603c8f5-0dbd-4fac-abcd-e6df133133f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%cd $WORKING_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bed57-868d-4f87-8311-33359004d0a0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get JDBC Connector jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e378d-1da3-4c61-a671-f1d8b9c53d65",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://jdbc.postgresql.org/download/postgresql-42.2.6.jar\n",
    "\n",
    "wget https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/0.27.0/spark-bigquery-with-dependencies_2.12-0.27.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2ab48-638d-41bb-a29a-4361fcc0a138",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build Dataproc Templates python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965a564-9a68-4092-82bc-904661ba01e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! python ./setup.py bdist_egg --output=$PACKAGE_EGG_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7c84b-a310-4c77-ae96-0a444a989e67",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Copying JAR files to GCS_STAGING_LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70baf3-2b11-4f84-b948-1ea70fb932ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! gsutil cp main.py $GCS_STAGING_LOCATION/\n",
    "! gsutil cp -r $PACKAGE_EGG_FILE $GCS_STAGING_LOCATION/\n",
    "! gsutil cp postgresql-42.2.6.jar $GCS_STAGING_LOCATION/jars/postgresql-42.2.6.jar\n",
    "! gsutil cp spark-bigquery-with-dependencies_2.12-0.27.0.jar $GCS_STAGING_LOCATION/jars/spark-bigquery-with-dependencies_2.12-0.27.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53007b43-28c7-41db-8698-e70f4a84b8ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 7: Calculate Parallel Jobs for POSTGRESQL to BigQuery\n",
    "\n",
    "This step uses MAX_PARALLELISM parameter to calculate number of parallel jobs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e3ca3-4381-4191-b8ee-43659ba85940",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate parallel jobs:\n",
    "COMPLETE_LIST = copy.deepcopy(POSTGRESQL_TABLE_LIST)\n",
    "PARALLEL_JOBS = len(POSTGRESQL_TABLE_LIST)//MAX_PARALLELISM\n",
    "JOB_LIST = []\n",
    "while len(COMPLETE_LIST) > 0:\n",
    "    SUB_LIST = []\n",
    "    for i in range(MAX_PARALLELISM):\n",
    "        if len(COMPLETE_LIST)>0 :\n",
    "            SUB_LIST.append(COMPLETE_LIST[0])\n",
    "            COMPLETE_LIST.pop(0)\n",
    "        else:\n",
    "            break\n",
    "    JOB_LIST.append(SUB_LIST)\n",
    "print(\"list of tables for execution : \")\n",
    "print(JOB_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e663cc-a269-4beb-87e1-740f772e9eb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 8: Get Row Count of Tables and identify Partition Columns\n",
    "\n",
    "This step uses PARTITION_THRESHOLD (default value is 1 million) parameter and any table having rows greater than PARTITION_THRESHOLD will be partitioned based on Primary Keys\n",
    "Get Primary keys for all tables to be migrated and find an integer column to partition on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b871e-c975-4a0b-bc51-7069ce77e51e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PARTITION_THRESHOLD = 1000000\n",
    "CHECK_PARTITION_COLUMN_LIST={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2055755-60ad-4371-a936-bfbfa8aaedb8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with DB.connect() as conn:\n",
    "    for table in POSTGRESQL_TABLE_LIST:\n",
    "        results = conn.execute(text(\"SELECT count(1) FROM {}\".format(table))).fetchall()\n",
    "        # print(results)\n",
    "        if results[0][0]>PARTITION_THRESHOLD and len(POSTGRESQL_TABLE_PRIMARY_KEYS.get(table).split(\",\")[0])>0:\n",
    "            column_list=POSTGRESQL_TABLE_PRIMARY_KEYS.get(table).split(\",\")\n",
    "            for column in column_list:\n",
    "                results_datatype = DB.execute(\"SELECT DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = '{0}' AND TABLE_NAME   = '{1}' AND COLUMN_NAME  = '{2}'\".format(table.split(\".\")[0],table.split(\".\")[1],column)).fetchall()      \n",
    "                # print(results_datatype)\n",
    "                if results_datatype[0][0]==\"int\":\n",
    "                    lowerbound = DB.execute(\"SELECT min({0}) from {1}\".format(column,table)).fetchall()\n",
    "                    upperbound = DB.execute(\"SELECT max({0}) from {1}\".format(column,table)).fetchall()\n",
    "                    numberPartitions = math.ceil((upperbound[0][0]-lowerbound[0][0])/PARTITION_THRESHOLD)\n",
    "                    CHECK_PARTITION_COLUMN_LIST[table]=[column,lowerbound[0][0],upperbound[0][0],numberPartitions]\n",
    "                    print(CHECK_PARTITION_COLUMN_LIST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d7523-4f87-48e7-bbe5-d1206b817c3d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 9: Execute Pipeline to Migrate tables from POSTGRESQL to BIGQUERY\n",
    "\n",
    "* BIGQUERY_DATASET : Target dataset in Bigquery\n",
    "* BIGQUERY_MODE : Mode of operation at target <append|overwrite|ignore|errorifexists> (default overwrite)\n",
    "* TEMP_GCS_BUCKET : Bucket name for dataproc job staging\n",
    "* PYTHON_FILE_URIS : Path to PACKAGE_EGG_FILE\n",
    "* MAIN_PYTHON_CLASS : Path to main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e607f2-c6c0-4abe-b4ff-ce34c83fcbf2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BIGQUERY_DATASET=\"python_dataproc_templates\"\n",
    "BIGQUERY_MODE = \"overwrite\"  # append/overwrite\n",
    "TEMP_GCS_BUCKET=\"python-dataproc-templates-temp-bq\"\n",
    "PYTHON_FILE_URIS = [ GCS_STAGING_LOCATION + \"/dataproc_templates_distribution.egg\" ]\n",
    "MAIN_PYTHON_CLASS = GCS_STAGING_LOCATION + \"/main.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f287fa2-8136-4c2b-a4ff-12cce31aa0dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def migrate_postgresql_to_bigquery(EXECUTION_LIST):\n",
    "    EXECUTION_LIST = EXECUTION_LIST\n",
    "    aiplatform.init(project=GCP_PROJECT,staging_bucket=TEMP_GCS_BUCKET)\n",
    "    \n",
    "    @dsl.pipeline(\n",
    "        name=\"python-postgresql-to-bigquery-pyspark\",\n",
    "        description=\"Pipeline to get data from PostgreSQL to BigQuery\",\n",
    "    )\n",
    "    def pipeline(\n",
    "        PROJECT_ID: str = GCP_PROJECT,\n",
    "        LOCATION: str = REGION,\n",
    "        MAIN_PYTHON_CLASS: str = MAIN_PYTHON_CLASS,\n",
    "        JAR_FILE_URIS: list = JARS,\n",
    "        SUBNETWORK_URI: str = SUBNET,\n",
    "        SERVICE_ACCOUNT: str = SERVICE_ACCOUNT,\n",
    "        PYTHON_FILE_URIS: list = PYTHON_FILE_URIS\n",
    "    ):\n",
    "        for table in EXECUTION_LIST:\n",
    "            BATCH_ID = \"postgresql2bigquery-{}\".format(datetime.now().strftime(\"%s\"))\n",
    "            POSTGRESQL_TO_BIGQUERY_JOBS.append(BATCH_ID)\n",
    "            if table in CHECK_PARTITION_COLUMN_LIST.keys():\n",
    "                TEMPLATE_SPARK_ARGS = [\n",
    "                    \"--template=JDBCTOBIGQUERY\",\n",
    "                    \"--jdbc.bigquery.input.url={}\".format(JDBC_URL),\n",
    "                    \"--jdbc.bigquery.input.driver={}\".format(JDBC_DRIVER),\n",
    "                    \"--jdbc.bigquery.input.table={}\".format(table),\n",
    "                    \"--jdbc.bigquery.output.mode={}\".format(BIGQUERY_MODE),\n",
    "                    \"--jdbc.bigquery.output.table={}\".format(table.split('.')[1]),\n",
    "                    \"--jdbc.bigquery.temp.bucket.name={}\".format(TEMP_GCS_BUCKET),\n",
    "                    \"--jdbc.bigquery.output.dataset={}\".format(BIGQUERY_DATASET),\n",
    "                    \"--jdbc.bigquery.input.partitioncolumn={}\".format(CHECK_PARTITION_COLUMN_LIST[table][0]),\n",
    "                    \"--jdbc.bigquery.input.lowerbound={}\".format(CHECK_PARTITION_COLUMN_LIST[table][1]),\n",
    "                    \"--jdbc.bigquery.input.upperbound={}\".format(CHECK_PARTITION_COLUMN_LIST[table][2]),\n",
    "                    \"--jdbc.bigquery.numpartitions={}\".format(CHECK_PARTITION_COLUMN_LIST[table][3])\n",
    "                ]\n",
    "            else:\n",
    "                TEMPLATE_SPARK_ARGS = [\n",
    "                    \"--template=JDBCTOBIGQUERY\",\n",
    "                    \"--jdbc.bigquery.input.url={}\".format(JDBC_URL),\n",
    "                    \"--jdbc.bigquery.input.driver={}\".format(JDBC_DRIVER),\n",
    "                    \"--jdbc.bigquery.input.table={}\".format(table),\n",
    "                    \"--jdbc.bigquery.output.mode={}\".format(BIGQUERY_MODE),\n",
    "                    \"--jdbc.bigquery.output.table={}\".format(table.split('.')[1]),\n",
    "                    \"--jdbc.bigquery.temp.bucket.name={}\".format(TEMP_GCS_BUCKET),\n",
    "                    \"--jdbc.bigquery.output.dataset={}\".format(BIGQUERY_DATASET)\n",
    "                ]\n",
    "\n",
    "            _ = DataprocPySparkBatchOp(\n",
    "                project=PROJECT_ID,\n",
    "                location=LOCATION,\n",
    "                batch_id=BATCH_ID,\n",
    "                main_python_file_uri=MAIN_PYTHON_CLASS,\n",
    "                jar_file_uris=JAR_FILE_URIS,\n",
    "                python_file_uris=PYTHON_FILE_URIS,\n",
    "                subnetwork_uri=SUBNETWORK_URI,\n",
    "                service_account=SERVICE_ACCOUNT,\n",
    "                runtime_config_version=\"1.1\", # issue 665\n",
    "                args=TEMPLATE_SPARK_ARGS\n",
    "                )\n",
    "            time.sleep(3)\n",
    "\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "    pipeline = aiplatform.PipelineJob(\n",
    "        display_name=\"pipeline\",\n",
    "        template_path=\"pipeline.json\",\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        enable_caching=False,\n",
    "        )\n",
    "    pipeline.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1511c7-b899-4862-9e27-11071167febf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for execution_list in JOB_LIST:\n",
    "    print(execution_list)\n",
    "    migrate_postgresql_to_bigquery(execution_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abfd4b-3658-4108-a28a-43d9adc28fe7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 10: Get status for tables migrated from PostgreSQL to BIGQUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b11938-7875-4c15-acf4-111f7b15b3f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_bearer_token():\n",
    "    \n",
    "    try:\n",
    "        #Defining Scope\n",
    "        CREDENTIAL_SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "\n",
    "        #Assigning credentials and project value\n",
    "        credentials, project_id = google.auth.default(scopes=CREDENTIAL_SCOPES)\n",
    "\n",
    "        #Refreshing credentials data\n",
    "        credentials.refresh(requests.Request())\n",
    "\n",
    "        #Get refreshed token\n",
    "        token = credentials.token\n",
    "        if token:\n",
    "            return (token,200)\n",
    "        else:\n",
    "            return \"Bearer token not generated\"\n",
    "    except Exception as error:\n",
    "        return (\"Bearer token not generated. Error : {}\".format(error),500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34feb439-3081-4719-90cd-320994b1422b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.auth.transport import requests\n",
    "import google\n",
    "\n",
    "token = get_bearer_token()\n",
    "token = get_bearer_token()\n",
    "if token[1] == 200:\n",
    "    print(\"Bearer token generated\")\n",
    "else:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffebab-b7fc-4456-a9f6-487a7b9fb18b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "postgresql_to_bigquery_status = []\n",
    "job_status_url = \"https://dataproc.googleapis.com/v1/projects/{}/locations/{}/batches/{}\"\n",
    "for job in POSTGRESQL_TO_BIGQUERY_JOBS:\n",
    "    auth = \"Bearer \" + token[0]\n",
    "    url = job_status_url.format(GCP_PROJECT,REGION,job)\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json; charset=UTF-8',\n",
    "      'Authorization': auth \n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    postgresql_to_bigquery_status.append(response.json()['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63b7e2-ef71-4dda-a7bd-d421390f00ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "statusDF = pd.DataFrame({\"table\" : POSTGRESQL_TABLE_LIST,\"postgresql_to_bigquery_job\" : POSTGRESQL_TO_BIGQUERY_JOBS, \"postgresql_to_bigquery_status\" : postgresql_to_bigquery_status})\n",
    "statusDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34e553-e49c-4848-8a46-df565cb5267f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 11: Validate row counts of migrated tables from SQL Server to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec2df4-a190-4e41-9c3e-d1f59cfb76e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "postgresql_row_count = []\n",
    "bq_row_count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c5fa1-f3a2-4322-b3e9-d71e34e9e4de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get postgresql table counts\n",
    "DB = sqlalchemy.create_engine(\n",
    "            sqlalchemy.engine.url.URL.create(\n",
    "                drivername=PY_JDBC_DRIVER,\n",
    "                username=POSTGRESQL_USERNAME,\n",
    "                password=POSTGRESQL_PASSWORD,\n",
    "                database=POSTGRESQL_DATABASE,\n",
    "                host=POSTGRESQL_HOST,\n",
    "                port=POSTGRESQL_PORT\n",
    "              )\n",
    "            )\n",
    "\n",
    "with DB.connect() as conn:\n",
    "    for table in POSTGRESQL_TABLE_LIST:\n",
    "        results = conn.execute(text(\"select count(*) from {}\".format(table))).fetchall()\n",
    "        for row in results:\n",
    "            postgresql_row_count.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f4420-cd44-4e77-9df7-11bff3733eed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "for table in POSTGRESQL_TABLE_LIST:\n",
    "    results = client.query(\"SELECT row_count FROM {}.__TABLES__ where table_id = '{}'\".format(BIGQUERY_DATASET,table.split('.')[1]))\n",
    "    for row in results:\n",
    "        bq_row_count.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c823f-f574-47d3-b4c6-094858763926",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "statusDF['postgresql_row_count'] = postgresql_row_count \n",
    "statusDF['bq_row_count'] = bq_row_count \n",
    "statusDF"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
