# Common Properties.
## GCP project id.
project.id=
## GCS staging bucket
gcs.staging.bucket.path=
## Spark Context log level
log.level=INFO

# HiveToGCS Template properties.
## GCS output path.
hive.gcs.output.path=
## Name of hive input table.
hive.input.table=
## Hive input db name.
hive.input.db=
## Optional, GCS output format. avro/csv/parquet/json/orc
hive.gcs.output.format=avro
## Optional, column to partition hive data.
hive.partition.col=
## Optional: Write mode to gcs append/overwrite/errorifexists/ignore
hive.gcs.save.mode=overwrite
## Optional: Below mentioned two properties needs to be used whenever there is a need to perform sql based transformations before loading data
hive.gcs.temp.table=
hive.gcs.temp.query=


# HiveToBQ Template properties.
# Required -Bigquery table name
hivetobq.bigquery.location=
#Required property
hivetobq.sql=
#Optional - Write mode to use while writing output to BQ. Supported values are - Append/Overwrite/ErrorIfExists/Ignore
hivetobq.write.mode=append
#Required Temporary directory to export bigquery data
hivetobq.temp.gcs.bucket=
## Optional: Below mentioned two properties needs to be used whenever there is a need to perform sql based transformations before loading data
hivetobq.temp.table=
hivetobq.temp.query=

# JDBCToBQ Template properties.
# Required -Bigquery table name
jdbctobq.bigquery.location=
# Required - JDBC URL and properties like username,password etc.,
jdbctobq.jdbc.url=
#Required property
jdbctobq.jdbc.driver.class.name=
#Optional property
jdbctobq.jdbc.fetchsize=
#Optional property
jdbctobq.jdbc.sessioninitstatement=
#Required property
jdbctobq.sql=
#optional properties to partition sql, if one is specified then all needs to be specified
jdbctobq.sql.partitionColumn=
jdbctobq.sql.lowerBound=
jdbctobq.sql.upperBound=
jdbctobq.sql.numPartitions=
#Optional - Write mode to use while writing output to BQ. Supported values are - Append/Overwrite/ErrorIfExists/Ignore
jdbctobq.write.mode=
#Required Temporary directory to export bigquery data
jdbctobq.temp.gcs.bucket=
## Optional: Below mentioned two properties needs to be used whenever there is a need to perform sql based transformations before loading data
jdbc.bq.temp.table=
jdbc.bq.temp.query=

#JDBCTOGCS Template properties
# Required - GCS output location
jdbctogcs.output.location=
# Optional - Default output format is csv. Example values avro,csv,parquet
jdbctogcs.output.format=
# Optional - Default is overwrite. Supported values are - Append/Overwrite/ErrorIfExists/Ignore
jdbctogcs.write.mode=
# Required - JDBC URL and properties like username,password etc.,
jdbctogcs.jdbc.url=
#Required property
jdbctogcs.jdbc.driver.class.name=
#Optional property
jdbctogcs.jdbc.fetchsize=
#Optional property
jdbctogcs.jdbc.sessioninitstatement=
#Required property
jdbctogcs.sql=
jdbctogcs.sql.file=
#optional properties to partition sql, if one is specified then all needs to be specified
jdbctogcs.sql.partitionColumn=
jdbctogcs.sql.lowerBound=
jdbctogcs.sql.upperBound=
jdbctogcs.sql.numPartitions=
# Optional property. column on which the output to be partitioned
jdbctogcs.output.partition.col=
## Optional: Below mentioned two properties needs to be used whenever there is a need to perform sql based transformations before loading data
jdbctogcs.temp.table=
jdbctogcs.temp.query=

#JDBCTOSPANNER Template properties
jdbctospanner.jdbc.url=
#Required property
jdbctospanner.jdbc.driver.class.name=
#Optional property
jdbctospanner.jdbc.fetchsize=
#Optional property
jdbctospanner.jdbc.sessioninitstatement=
#Required property
jdbctospanner.sql=
jdbctospanner.sql.file=
#optional properties to partition sql, if one is specified then all needs to be specified
jdbctospanner.sql.partitionColumn=
jdbctospanner.sql.lowerBound=
jdbctospanner.sql.upperBound=
jdbctospanner.sql.numPartitions=
jdbctospanner.output.instance=
jdbctospanner.output.database=
jdbctospanner.output.table=
jdbctospanner.output.saveMode=ErrorIfExists
jdbctospanner.output.primaryKey=
jdbctospanner.output.batch.size=1000
## Optional: Below mentioned two properties needs to be used whenever there is a need to perform sql based transformations before loading data
jdbctospanner.temp.table=
jdbctospanner.temp.query=

# WordCount properties.
## Format of input file to be processed.
word.count.input.format=
## input path for word count.
word.count.input.path=
## input path for word count.
word.count.output.path=


# GCSToBigTable properties.
## Bigtable instance ud.
bigtable.instance.id=

## BigTable output table name.
bigtable.output.table.name=
## Column from input source to be used as key column in Bigtable.
bigtable.key.col=
## BigTable column family name.
bigtable.col.family.name=

# PubSubToBQ properties
## Project that contains the input Pub/Sub subscription to be read
pubsub.input.project.id=
## PubSub subscription name
pubsub.input.subscription=
## Stream timeout, for how long the subscription will be read
pubsub.timeout.ms=60000
## Streaming duration, how often wil writes to BQ be triggered
pubsub.streaming.duration.seconds=15
## Number of streams that will read from Pub/Sub subscription in parallel
pubsub.total.receivers=5
## Project that contains the output table
pubsub.bq.output.project.id=
## BigQuery output dataset
pubsub.bq.output.dataset=
## BigQuery output table
pubsub.bq.output.table=
## Number of records to be written per message to BigQuery
pubsub.bq.batch.size=1000

# PubSubToBigTable properties
## reuse pubsub.input.project.id, pubsub.input.subscription, pubsub.timeout.ms, pubsub.streaming.duration.seconds, pubsub.total.receivers from PubSubToBQ properties
## Project that contains the output table
pubsub.bigtable.output.project.id=
## BigTable Instance Id
pubsub.bigtable.output.instance.id=
## BigTable output table
pubsub.bigtable.output.table=
# PubSubLiteToBigTable properties
## Project that contains the output table
pubsublite.bigtable.output.project.id=
## BigTable Instance Id
pubsublite.bigtable.output.instance.id=
## BigTable output table
pubsublite.bigtable.output.table=
pubsublite.checkpoint.location=
## Project that contains the input Pub/Sub Lite subscription to be read
pubsublite.input.project.id=
## PubSub Lite subscription path
pubsublite.input.subscription=
## Stream timeout, for how long the subscription will be read
pubsublite.timeout.ms=60000
## Streaming duration, how often will writes to Bigtable be triggered
pubsublite.streaming.duration.seconds=15

#GCStoBigQuery properties
# supported formats avro,parquet,csv
gcs.bigquery.input.format=
#gcs file location
gcs.bigquery.input.location=
gcs.bigquery.output.dataset=
gcs.bigquery.output.table=
gcs.bigquery.temp.bucket.name=
gcs.bigquery.output.mode=Append

#GCStoBigTable Java properties
# supported formats avro,parquet,csv
gcs.bigtable.input.format=
#gcs file location
gcs.bigtable.input.location=
#Details of Bigtable
gcs.bigtable.output.instance.id=
gcs.bigtable.output.project.id=
gcs.bigtable.table.name=
gcs.bigtable.column.family=
# Mention the column family name that is part of the bigtable


# Spanner To GCS properties
spanner.gcs.input.spanner.id=
spanner.gcs.input.database.id=
spanner.gcs.input.table.id=
spanner.gcs.output.gcs.path=
spanner.gcs.output.gcs.saveMode=
spanner.gcs.output.gcs.format=
#optional properties to partition sql, if one is specified then all needs to be specified
spanner.gcs.input.sql.partitionColumn=
spanner.gcs.input.sql.lowerBound=
spanner.gcs.input.sql.upperBound=
spanner.gcs.input.sql.numPartitions=
## Optional: Below mentioned two properties needs to be used whenever there is a need to perform sql based transformations before loading data
spanner.gcs.temp.table=
spanner.gcs.temp.query=

# Cassandra To GCS properties
cassandratogcs.input.keyspace=
cassandratogcs.input.table=
cassandratogcs.input.host=
cassandratogcs.output.format=
cassandratogcs.output.savemode=
cassandratogcs.output.path=
cassandratogcs.input.catalog.name=casscon

# Cassandra To BQ properties
cassandratobq.input.keyspace=
cassandratobq.input.table=
cassandratobq.input.host=
cassandratobq.bigquery.location=
cassandratobq.output.mode=
cassandratobq.temp.gcs.location=
cassandratobq.input.catalog.name=casscon

# S3 to BQ
s3.bq.access.key=
s3.bq.secret.key=
s3.bq.input.format=
s3.bq.input.location=
s3.bq.output.dataset.name=
s3.bq.output.table.name=
s3.bq.output.mode=Append
s3.bq.ld.temp.bucket.name=

# Bigquery to GCS
bigquery.gcs.input.table=
bigquery.gcs.output.format=
bigquery.gcs.output.location=
bigquery.gcs.output.mode=Append

# GCS to Spanner
gcs.spanner.input.format=
gcs.spanner.input.location=
gcs.spanner.output.instance=
gcs.spanner.output.database=
gcs.spanner.output.table=
gcs.spanner.output.saveMode=ErrorIfExists
gcs.spanner.output.primaryKey=
gcs.spanner.output.batchInsertSize=1000

# GCS to JDBC
gcs.jdbc.input.format=
gcs.jdbc.input.location=
gcs.jdbc.output.driver=
gcs.jdbc.output.url=
gcs.jdbc.output.table=
gcs.jdbc.output.saveMode=ErrorIfExists
gcs.jdbc.output.batchInsertSize=1000

# GCS to GCS
gcs.gcs.input.location=
gcs.gcs.input.format=
gcs.gcs.output.location=
gcs.gcs.output.format=
gcs.gcs.write.mode=
gcs.gcs.temp.table=
gcs.gcs.temp.query=

# GCS to Mongo
gcs.mongodb.input.format=
gcs.mongodb.input.location=
gcs.mongodb.output.uri=
gcs.mongodb.output.database=
gcs.mongodb.output.collection=
gcs.mongodb.batch.size=512
gcs.mongodb.output.mode=append

# PubSub to GCS
## Project that contains the input Pub/Sub subscription to be read
pubsubtogcs.input.project.id=yadavaja-sandbox
## PubSub subscription name
pubsubtogcs.input.subscription=
## Stream timeout, for how long the subscription will be read
pubsubtogcs.timeout.ms=60000
## Streaming duration, how often wil writes to GCS be triggered
pubsubtogcs.streaming.duration.seconds=15
## Number of streams that will read from Pub/Sub subscription in parallel
pubsubtogcs.total.receivers=5
## GCS bucket URL
pubsubtogcs.gcs.bucket.name=
## Number of records to be written per message to GCS
pubsubtogcs.batch.size=1000
## PubSub to GCS supported formats are: AVRO, JSON
pubsubtogcs.gcs.output.data.format=

#Hbase to GCS
## The supported file formats are: avro, csv, parquet, json, orc
hbasetogcs.output.fileformat=json
## The supported save modes are: append, overwrite, errorifexists, ignore
hbasetogcs.output.savemode=append
## Output GCS Bucket path
hbasetogcs.output.path=
## Catalog for HBase table
hbasetogcs.table.catalog=

#Redshift to GCS
## Input Connection URL String to Redshift
redshift.aws.input.url=
## Input Table name to be pulled from Redshift
redshift.aws.input.table=
## Temp Directory on AWS side for keeping the data before moving it to GCS
redshift.aws.input.temp.dir=
## IAM roles for performing this data migration activity
redshift.aws.input.iam.role=
## Access Key to access Redshift on AWS
redshift.aws.input.access.key=
## Secret Key to access Redshift on AWS
redshift.aws.input.secret.key=
## File Format in which the data needs to be written on GCS
redshift.gcs.output.file.format=
## File location where the data needs to be written over GCS
redshift.gcs.output.file.location=
## File Save mode to be opted for writing data onto GCS Location
redshift.gcs.output.mode=
## Temp Table name to perform spark sql transformations
redshift.gcs.temp.table=
## Temp Query to perform spark sql transformations
redshift.gcs.temp.query=

# Dataplex GCS to BQ
dataplex.gcs.bq.target.dataset=
dataplex.gcs.bq.target.asset=
dataplex.gcs.bq.target.entity=
dataplex.gcs.bq.save.mode="errorifexists"
dataplex.gcs.bq.incremental.partition.copy="yes"

#Kafka To PubSub
kafka.pubsub.bootstrap.servers=
kafka.pubsub.input.topic=
kafka.pubsub.starting.offset=earliest
kafka.pubsub.output.projectId=
kafka.pubsub.output.topic=
kafka.pubsub.checkpoint.location=
kafka.pubsub.await.termination.timeout=420000

# Kafka To BQ
## Kafka servers
kafka.bq.bootstrap.servers=
## Kafka topics
kafka.bq.topic=
## BigQuery output dataset
kafka.bq.dataset=
## BigQuery output table
kafka.bq.table=
## GCS bucket name, for storing temporary files
kafka.bq.temp.gcs.bucket=
## GCS location for maintaining checkpoint
kafka.bq.checkpoint.location=
## Offset to start reading from
kafka.bq.starting.offset=earliest
## Waits for specified time in ms before termination of stream
kafka.bq.await.termination.timeout=420000
## Fails the job when data is lost. Accepted values: true, false
kafka.bq.fail.on.dataloss=false
## Ouptut mode for writing data. Accepted values: 'append', 'complete', 'update'
kafka.bq.stream.output.mode=append


#Kafka Common Reusable Properties
#Kafka source message format. Accepted values 'json','bytes'
kafka.message.format=
#Kafka bootstrap server address
kafka.bootstrap.servers=
#Kafka Topic to subscribe
kafka.topic=
#Offset values to consume from . Accepted value 'earliest','latest','json string {"topicA":{"0":23,"1":-1},"topicB":{"0":-2}}'
kafka.starting.offset=earliest
#gcs url to pointing to schema for parsing JSON message
kafka.schema.url=

#Kafka To GCS
#output file location
kafka.gcs.output.location=
## Ouptut file format for writing data. Accepted values: 'parquet', 'csv', 'avro', 'orc', 'json'
kafka.gcs.output.format=parquet
## Ouptut mode for writing data. Accepted values: 'append', 'complete', 'update'
kafka.gcs.output.mode=append
## Waits for specified time in ms before termination of stream
kafka.gcs.await.termination.timeout.ms=600000

# Snowflake To GCS
snowflake.gcs.sfurl=
snowflake.gcs.sfuser=
snowflake.gcs.sfpassword=
snowflake.gcs.sfdatabase=
snowflake.gcs.sfschema=
snowflake.gcs.sfwarehouse=
snowflake.gcs.autopushdown=on
snowflake.gcs.table=
snowflake.gcs.query=
snowflake.gcs.output.location=
snowflake.gcs.output.format=
snowflake.gcs.output.mode=overwrite
snowflake.gcs.output.partitionColumn=

# Mongo To GCS
mongo.gcs.output.location=
mongo.gcs.output.format=
mongo.gcs.output.mode=append
mongo.gcs.input.uri=
mongo.gcs.input.database=
mongo.gcs.input.collection=

# JDBC To JDBC
jdbctojdbc.input.url=
jdbctojdbc.input.driver=
jdbctojdbc.input.table=
jdbctojdbc.input.fetchsize=
jdbctojdbc.input.partitioncolumn=
jdbctojdbc.input.lowerbound=
jdbctojdbc.input.upperbound=
jdbctojdbc.numpartitions=
jdbctojdbc.output.url=
jdbctojdbc.output.driver=
jdbctojdbc.output.table=
jdbctojdbc.output.create.table.option=
jdbctojdbc.output.mode=Append
jdbctojdbc.output.batch.size=1000
jdbctojdbc.sessioninitstatement=
jdbctojdbc.output.primary.key=
jdbctojdbc.temp.view.name=
jdbctojdbc.sql.query=

# Text To Bigquery
text.bigquery.input.location=
text.bigquery.input.compression=none
text.bigquery.input.delimiter=
text.bigquery.output.dataset=
text.bigquery.output.table=
text.bigquery.output.mode=
text.bigquery.temp.bucket=

# Kafka TO BQ Dstream
kafka.bq.batch.interval=60000
# Kafka TO GCS Dstream
kafka.gcs.batch.interval=60000
kafka.gcs.write.mode=Append