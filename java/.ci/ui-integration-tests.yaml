# Copyright (C) 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

substitutions:
  _GCP_PROJECT: 'dataproc-templates'
  _REGION: 'us-central1'
  _ENV_TEST_SPANNER_ID: 'spanner-integration-test'
  _ENV_TEST_HIVE_METASTORE_URIS: 'thrift://10.115.64.27:9083'
  _SUBNET: 'projects/${_GCP_PROJECT}/regions/${_REGION}/subnetworks/default'
  _JAR_PATH: 'gs://dataproc-templates-binaries/latest/java/dataproc-templates.jar'
  _SERVICE_ACCOUNT: 'dataproc-templates-cicd@dataproc-templates.iam.gserviceaccount.com'

availableSecrets:
  secretManager:
    - versionName: projects/$PROJECT_ID/secrets/TEST_JDBC_URL/versions/latest
      env: 'TEST_JDBC_URL'

steps:
    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      waitFor: ['-']
      id: spanner-to-gcs
      entrypoint: bash
      args:
        - -c
        - |
          gcloud dataproc batches submit spark \
              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="$_GCP_PROJECT" --region="$_REGION" \
              --jars="$_JAR_PATH" \
              -- --template=SPANNERTOGCS \
              --templateProperty project.id="$_GCP_PROJECT" \
              --templateProperty spanner.gcs.input.spanner.id="$_ENV_TEST_SPANNER_ID" \
              --templateProperty spanner.gcs.input.database.id="spark-ci-db" \
              --templateProperty "spanner.gcs.input.table.id=(select id,name from badges_ui_tests where name = 'Teacher' limit 10000)" \
              --templateProperty spanner.gcs.output.gcs.path="gs://dataproc-templates_cloudbuild/ui-testing/SPANNERTOGCS/parquet" \
              --templateProperty spanner.gcs.output.gcs.saveMode="Overwrite" \
              --templateProperty spanner.gcs.output.gcs.format="parquet"

    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      waitFor: ['-']
      id: gcs-to-bigquery-avro
      entrypoint: bash
      args:
        - -c
        - |
          gcloud dataproc batches submit spark \
              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
              --jars="${_JAR_PATH},file:///usr/lib/spark/connector/spark-avro.jar" \
              -- --template=GCSTOBIGQUERY \
              --templateProperty project.id="${_GCP_PROJECT}" \
              --templateProperty gcs.bigquery.input.location="gs://dataproc-templates_cloudbuild/ui-testing/gcstobigquery/cities.avro" \
              --templateProperty gcs.bigquery.input.format="avro" \
              --templateProperty gcs.bigquery.output.dataset="dataproc_templates" \
              --templateProperty gcs.bigquery.output.table="ui_gcstobq_avro" \
              --templateProperty gcs.bigquery.output.mode="Overwrite" \
              --templateProperty gcs.bigquery.temp.bucket.name="dataproc-templates"


    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      waitFor: ['-']
      id: gcs-to-spanner
      entrypoint: bash
      args:
        - -c
        - |
          JAR_PATH="$_JAR_PATH"
          gcloud dataproc batches submit spark \
              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
              --jars="${_JAR_PATH},file:///usr/lib/spark/connector/spark-avro.jar" \
              -- --template GCSTOSPANNER \
              --templateProperty project.id="${_GCP_PROJECT}" \
              --templateProperty gcs.spanner.input.format="avro" \
              --templateProperty gcs.spanner.input.location="gs://dataproc-templates_cloudbuild/ui-testing/gcstospanner/emp.avro" \
              --templateProperty gcs.spanner.output.instance="${_ENV_TEST_SPANNER_ID}" \
              --templateProperty gcs.spanner.output.database="spark-ci-db" \
              --templateProperty gcs.spanner.output.table="employee_ui_tests" \
              --templateProperty gcs.spanner.output.saveMode="Overwrite" \
              --templateProperty gcs.spanner.output.primaryKey="empno"

    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      waitFor: ['-']
      id: gcs-to-gcs-avro-to-csv
      entrypoint: bash
      args:
        - -c
        - |
          gcloud dataproc batches submit spark \
              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
              --jars="${_JAR_PATH},file:///usr/lib/spark/connector/spark-avro.jar" \
              -- --template=GCSTOGCS \
              --templateProperty project.id="${_GCP_PROJECT}" \
              --templateProperty gcs.gcs.input.location="gs://dataproc-templates_cloudbuild/ui-testing/gcstogcs/emp.avro" \
              --templateProperty gcs.gcs.input.format="avro" \
              --templateProperty gcs.gcs.output.location="gs://dataproc-templates_cloudbuild/ui-testing/gcstogcs/output/" \
              --templateProperty gcs.gcs.output.format="csv" \
              --templateProperty gcs.gcs.write.mode="overwrite" \
              --templateProperty gcs.gcs.temp.table="dataset" \
              --templateProperty "gcs.gcs.temp.query=select * from global_temp.dataset where sal>1500"

    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      waitFor: ['-']
      id: gcs-to-gcs-csv-to-avro
      entrypoint: bash
      args:
        - -c
        - |
          gcloud dataproc batches submit spark \
              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
              --jars="${_JAR_PATH},file:///usr/lib/spark/connector/spark-avro.jar" \
              -- --template=GCSTOGCS \
              --templateProperty project.id="${_GCP_PROJECT}" \
              --templateProperty gcs.gcs.input.location="gs://dataproc-templates_cloudbuild/ui-testing/gcstogcs/input/cities.csv" \
              --templateProperty gcs.gcs.input.format="csv" \
              --templateProperty gcs.gcs.output.location="gs://dataproc-templates_cloudbuild/ui-testing/gcstogcs/output/avro" \
              --templateProperty gcs.gcs.output.format="avro" \
              --templateProperty gcs.gcs.write.mode="overwrite"

    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      waitFor: ['-']
      id: gcs-to-jdbc
      secretEnv: ['TEST_JDBC_URL']
      script: |
        #!/usr/bin/env bash
          gcloud dataproc batches submit spark \
              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
              --project="${_GCP_PROJECT}" --region="${_REGION}" --version="1.2" --service-account=${_SERVICE_ACCOUNT} \
              --jars="${_JAR_PATH},file:///usr/lib/spark/connector/spark-avro.jar,gs://dataproc-templates_cloudbuild/integration-testing/jars/mysql-connector-java.jar" \
              --subnet="${_SUBNET}" \
              -- --template=GCSTOJDBC \
              --templateProperty project.id="${_GCP_PROJECT}" \
              --templateProperty gcs.jdbc.input.location="gs://dataproc-templates_cloudbuild/integration-testing/gcstojdbc/emp.avro" \
              --templateProperty gcs.jdbc.input.format="avro" \
              --templateProperty gcs.jdbc.output.saveMode="Overwrite" \
              --templateProperty gcs.jdbc.output.url="${TEST_JDBC_URL}" \
              --templateProperty gcs.jdbc.output.table="ui_test_avro" \
              --templateProperty gcs.jdbc.output.driver="com.mysql.jdbc.Driver"

    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
      waitFor: ['-']
      id: hive-to-bigquery
      entrypoint: bash
      args:
        - -c
        - |
          gcloud dataproc batches submit spark \
              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
              --jars="${_JAR_PATH}" \
              --properties="spark.hadoop.hive.metastore.uris=${_ENV_TEST_HIVE_METASTORE_URIS}" \
              -- --template HIVETOBIGQUERY \
              --templateProperty hivetobq.bigquery.location="${_GCP_PROJECT}.dataproc_templates.ui_test_hivetobq" \
              --templateProperty hivetobq.sql="select * from test_db.employee" \
              --templateProperty hivetobq.temp.gcs.bucket="gs://dataproc-templates_cloudbuild/integration-templates_cloudbuild/ui-testing/hivetobigquery/" \
              --templateProperty hivetobq.write.mode="Overwrite"

#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: hive-to-gcs
#      entrypoint: bash
#      args:
#        - -c
#        - |
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
#              --jars="${_JAR_PATH},file:///usr/lib/spark/connector/spark-avro.jar" \
#              --subnet="${_SUBNET}" \
#              --properties="spark.hadoop.hive.metastore.uris=${_ENV_TEST_HIVE_METASTORE_URIS}" \
#              -- --template=HIVETOGCS \
#              --templateProperty hive.input.table="employee" \
#              --templateProperty hive.input.db="test_db" \
#              --templateProperty hive.gcs.output.path="gs://dataproc-templates/integration-testing/output/HIVETOGCS" \
#              --templateProperty hive.gcs.save.mode="overwrite"
#
#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: spanner-to-gcs-csv
#      entrypoint: bash
#      args:
#        - -c
#        - |
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
#              --jars="${_JAR_PATH}" \
#              -- --template=SPANNERTOGCS \
#              --templateProperty project.id="${_GCP_PROJECT}" \
#              --templateProperty spanner.gcs.input.spanner.id="${_ENV_TEST_SPANNER_ID}" \
#              --templateProperty spanner.gcs.input.database.id="test-db" \
#              --templateProperty spanner.gcs.input.table.id="shakespeare" \
#              --templateProperty spanner.gcs.output.gcs.path="gs://dataproc-templates/integration-testing/output/SPANNERTOGCS/csv" \
#              --templateProperty spanner.gcs.output.gcs.saveMode="Overwrite" \
#              --templateProperty spanner.gcs.output.gcs.format="csv"
#
#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: spanner-to-gcs-avro
#      entrypoint: bash
#      args:
#        - -c
#        - |
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
#              --jars="${_JAR_PATH},file:///usr/lib/spark/connector/spark-avro.jar" \
#              -- --template=SPANNERTOGCS \
#              --templateProperty project.id="${_GCP_PROJECT}" \
#              --templateProperty spanner.gcs.input.spanner.id="${_ENV_TEST_SPANNER_ID}" \
#              --templateProperty spanner.gcs.input.database.id="test-db" \
#              --templateProperty spanner.gcs.input.table.id="shakespeare" \
#              --templateProperty spanner.gcs.output.gcs.path="gs://dataproc-templates/integration-testing/output/SPANNERTOGCS/avro" \
#              --templateProperty spanner.gcs.output.gcs.saveMode="Overwrite" \
#              --templateProperty spanner.gcs.output.gcs.format="avro"
#
#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: gcs-to-bigquery-csv
#      entrypoint: bash
#      args:
#        - -c
#        - |
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
#              --jars="${_JAR_PATH}" \
#              -- --template=GCSTOBIGQUERY \
#              --templateProperty project.id="${_GCP_PROJECT}" \
#              --templateProperty gcs.bigquery.input.location="gs://dataproc-templates/integration-testing/gcstobigquery/cities.csv" \
#              --templateProperty gcs.bigquery.input.format="csv" \
#              --templateProperty gcs.bigquery.output.dataset="dataproc_templates" \
#              --templateProperty gcs.bigquery.output.table="gcs_bq_cities" \
#              --templateProperty gcs.bigquery.output.mode="Overwrite" \
#              --templateProperty gcs.bigquery.temp.bucket.name="dataproc-templates"
#
#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: jdbc-to-jdbc
#      entrypoint: bash
#      secretEnv: [ 'TEST_JDBC_URL' ]
#      script: |
#          #!/usr/bin/env bash
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --project="${_GCP_PROJECT}" --region="${_REGION}" --version="1.2" --service-account=${_SERVICE_ACCOUNT} \
#              --jars="${_JAR_PATH},gs://dataproc-templates/jars/mysql-connector-java.jar" \
#              --subnet="${_SUBNET}" \
#              -- --template JDBCTOJDBC \
#              --templateProperty project.id="${_GCP_PROJECT}" \
#              --templateProperty jdbctojdbc.input.url="${TEST_JDBC_URL}" \
#              --templateProperty jdbctojdbc.input.driver="com.mysql.cj.jdbc.Driver" \
#              --templateProperty jdbctojdbc.input.table="employee" \
#              --templateProperty jdbctojdbc.output.url="${TEST_JDBC_URL}" \
#              --templateProperty jdbctojdbc.output.driver="com.mysql.cj.jdbc.Driver" \
#              --templateProperty jdbctojdbc.output.table="employee_output" \
#              --templateProperty jdbctojdbc.output.mode="Overwrite"
#
#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: pubsub-to-gcs
#      entrypoint: bash
#      args:
#        - -c
#        - |
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
#              --jars="${_JAR_PATH}" \
#              -- --template=PUBSUBTOGCS \
#              --templateProperty pubsubtogcs.input.project.id="${_GCP_PROJECT}" \
#              --templateProperty pubsubtogcs.input.subscription="pubsubtogcsv3-sub" \
#              --templateProperty pubsubtogcs.gcs.bucket.name="pubsubtogcs_dev" \
#              --templateProperty pubsubtogcs.gcs.output.data.format="JSON" \
#              --templateProperty pubsubtogcs.batch.size="50"
#
#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: jdbc-to-bq
#      entrypoint: bash
#      secretEnv: [ 'TEST_JDBC_URL' ]
#      args:
#        - -c
#        - |
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
#              --jars="${_JAR_PATH},gs://dataproc-templates/jars/mysql-connector-java.jar" \
#              --subnet="${_SUBNET}" \
#              -- --template=JDBCTOBIGQUERY \
#              --templateProperty jdbctobq.bigquery.location="${_GCP_PROJECT}.dataproc_templates.jdbctobq" \
#              --templateProperty jdbctobq.jdbc.url="${TEST_JDBC_URL}" \
#              --templateProperty jdbctobq.jdbc.driver.class.name="com.mysql.jdbc.Driver" \
#              --templateProperty jdbctobq.write.mode="overwrite" \
#              --templateProperty jdbctobq.temp.gcs.bucket="dataproc-templates/integration-testing/output/JDBCTOBIGQUERY" \
#              --templateProperty "jdbctobq.sql=select * from test.employee"
#
#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: jdbc-to-spanner
#      entrypoint: bash
#      secretEnv: [ 'TEST_JDBC_URL' ]
#      args:
#        - -c
#        - |
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --version="1.2" --service-account=${_SERVICE_ACCOUNT} --project="${_GCP_PROJECT}" --region="${_REGION}" \
#              --jars="${_JAR_PATH},gs://dataproc-templates/jars/mysql-connector-java.jar" \
#              --subnet="${_SUBNET}" \
#              -- --template=JDBCTOSPANNER \
#              --templateProperty project.id="${_GCP_PROJECT}" \
#              --templateProperty jdbctospanner.jdbc.url="${TEST_JDBC_URL}" \
#              --templateProperty jdbctospanner.jdbc.driver.class.name="com.mysql.jdbc.Driver" \
#              --templateProperty "jdbctospanner.sql=select * from test.employee" \
#              --templateProperty jdbctospanner.output.instance="${_ENV_TEST_SPANNER_ID}" \
#              --templateProperty jdbctospanner.output.database="spark-ci-db" \
#              --templateProperty jdbctospanner.output.table="employee" \
#              --templateProperty jdbctospanner.output.saveMode="Overwrite" \
#              --templateProperty jdbctospanner.output.primaryKey="empno"
#
#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: jdbc-to-gcs-csv
#      entrypoint: bash
#      secretEnv: [ 'TEST_JDBC_URL' ]
#      args:
#        - -c
#        - |
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --project="${_GCP_PROJECT}" --region="${_REGION}" --version="1.2" --service-account=${_SERVICE_ACCOUNT} \
#              --jars="${_JAR_PATH},gs://dataproc-templates/jars/mysql-connector-java.jar" \
#              --subnet="${_SUBNET}" \
#              -- --template=JDBCTOGCS \
#              --templateProperty project.id="${_GCP_PROJECT}" \
#              --templateProperty jdbctogcs.jdbc.url="${TEST_JDBC_URL}" \
#              --templateProperty jdbctogcs.jdbc.driver.class.name="com.mysql.jdbc.Driver" \
#              --templateProperty jdbctogcs.output.format="csv" \
#              --templateProperty jdbctogcs.output.location="gs://dataproc-templates/integration-testing/output/JDBCTOGCS/csv" \
#              --templateProperty "jdbctogcs.sql=select * from test.employee" \
#              --templateProperty jdbctogcs.write.mode="overwrite"
#
#    - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
#      waitFor: ['-']
#      id: jdbc-to-gcs-avro
#      entrypoint: bash
#      secretEnv: [ 'TEST_JDBC_URL' ]
#      args:
#        - -c
#        - |
#          JAR_PATH=$(cat /workspace/jar_path.txt)
#          gcloud dataproc batches submit spark \
#              --class=com.google.cloud.dataproc.templates.main.DataProcTemplate \
#              --project="${_GCP_PROJECT}" --region="${_REGION}" --version="1.2" --service-account=${_SERVICE_ACCOUNT} \
#              --jars="${_JAR_PATH},gs://dataproc-templates/jars/mysql-connector-java.jar,file:///usr/lib/spark/connector/spark-avro.jar" \
#              --subnet="${_SUBNET}" \
#              -- --template=JDBCTOGCS \
#              --templateProperty project.id="${_GCP_PROJECT}" \
#              --templateProperty jdbctogcs.jdbc.url="${TEST_JDBC_URL}" \
#              --templateProperty jdbctogcs.jdbc.driver.class.name="com.mysql.jdbc.Driver" \
#              --templateProperty jdbctogcs.output.format="avro" \
#              --templateProperty jdbctogcs.output.location="gs://dataproc-templates/integration-testing/output/JDBCTOGCS/avro" \
#              --templateProperty "jdbctogcs.sql=select * from test.employee" \
#              --templateProperty jdbctogcs.write.mode="overwrite"


options:
    logging: CLOUD_LOGGING_ONLY
    automapSubstitutions: true
timeout: 3600s


