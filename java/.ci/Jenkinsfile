// Helper function to check for specific file changes
def shouldRunOnChange() {
    // If the current branch is main, always run the stages
    if (env.GIT_BRANCH_LOCAL == 'main') {
        echo "[Change Detection] Current branch is 'main'. Stage will run."
        return true
    }
    try {
        echo "[Change Detection] Checking for changes in relevant files..."
        def checkScript = '''
            if git rev-parse --verify HEAD~1 >/dev/null 2>&1; then
                # Not the first commit, diff against parent
                git diff --name-only HEAD~1 HEAD
            else
                # First commit, list all files in the commit (as all are "new")
                git diff-tree --no-commit-id --name-only -r HEAD
            fi | grep -Eq ".java$|java/pom.xml|java/.ci/Jenkinsfile|java/.ci/UiJenkinsfile"
        '''
        // sh(...) returns 0 if grep finds a match (success), non-zero otherwise.
        def exitCode = sh(script: checkScript, returnStatus: true)
        echo "[Change Detection] Script exit code: ${exitCode} (0 means changes detected and stage should run)"
        return exitCode == 0
    } catch (Exception e) {
        echo "[Change Detection] Error during change detection: ${e.getMessage()}. Defaulting to false (skip stage)."
        return false // Default to not running the stage if there's an error in the check
    }
}

def stageRetryCount = 3

pipeline {
    
    agent any

    environment {
        DATAPROC_TELEPORT_WEBHOOK_URL = credentials('dataproc-teleport-webhook-url')

        TEST_JDBC_URL = credentials('env-test-jdbc-url')

        GIT_BRANCH_LOCAL = sh (
            script: "echo $branchName | sed -e 's|origin/||g' | sed -e 's|^null\$|main|'",  // Remove "origin/" and set the default branch to main
            returnStdout: true
        ).trim()
        
        MAVEN_HOME = "/var/lib/jenkins/tools/hudson.tasks.Maven_MavenInstallation/maven"
        PATH = "$PATH:$MAVEN_HOME/bin"

        GCS_STAGING_LOCATION = sh (script: '''
            CURRENT_BRANCH=`echo $branchName | sed -e 's|origin/||g' | sed -e 's|^null\$|main|'`
            if [ $CURRENT_BRANCH != "main" ];then
            echo "$GCS_STAGING_LOCATION/$(uuidgen)"
            else
            echo $GCS_STAGING_LOCATION
            fi
            '''.stripIndent(),
            returnStdout: true
        ).trim()
    }
    stages {
        stage('Prepare Environment'){
            parallel{
                stage('Checkout') {
                    steps{
                        git branch: "${GIT_BRANCH_LOCAL}", changelog: false, poll: false, url: 'https://github.com/GoogleCloudPlatform/dataproc-templates/'    
                    }
                }
                stage('Reset Resources'){
                    steps {
                            catchError {
                                sh '''
                                    gcloud pubsub topics publish pubsubtogcsv3 --message='{"Name": "NewMsg", "Age": 10}'
                                    gcloud pubsub topics publish test-pubsub-bq --message='{"Name": "Another message", "Age": 18}' 2> /dev/null || true

                                    gsutil rm -r gs://dataproc-templates/integration-testing/checkpoint/KAFKATOBQ 2> /dev/null || true
                                    gsutil rm -r gs://dataproc-templates/integration-testing/output/KAFKATOGCS 2> /dev/null || true
                                    gsutil rm -r gs://dataproc-templates/integration-testing/output/KAFKATOGCS_DStream 2> /dev/null || true
                                    gsutil rm -r gs://dataproc-templates/integration-testing/checkpoint/PUBSUBTOBQ 2> /dev/null || true

cat > /tmp/employeecatalog.json << EOF
{
  "table": {"name": "employee"},
  "rowkey": "id_rowkey",
  "columns": {
    "key": {"cf": "rowkey", "col": "id_rowkey", "type": "string"},
    "name": {"cf": "personal", "col": "name", "type": "string"},
    "address": {"cf": "personal", "col": "address", "type": "string"},
    "empno": {"cf": "professional", "col": "empno", "type": "string"}
  }
}
EOF

                                    cat /tmp/employeecatalog.json
                                    gsutil cp /tmp/employeecatalog.json gs://dataproc-templates/conf/

                                '''
                            }
                    }
                }
                stage('Cluster Creation'){
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE == "CLUSTER" }
                    }
                    steps{
                        sh '''
                            if gcloud dataproc clusters list --region=$REGION --project=$GCP_PROJECT | grep -q $CLUSTER; then
                                echo "Cluster $CLUSTER already exists."
                            else
                                echo "Cluster $CLUSTER does not exist. Creating now..."
                                gcloud dataproc clusters create $CLUSTER \
                                --region $REGION \
                                --subnet $SUBNET \
                                --no-address \
                                --master-machine-type n1-standard-2 \
                                --master-boot-disk-size 500 \
                                --num-workers 2 \
                                --worker-machine-type n1-standard-2 \
                                --worker-boot-disk-size 500 \
                                --image-version 2.2-debian12 \
                                --optional-components ZOOKEEPER \
                                --max-idle 1800s \
                                --project $GCP_PROJECT
                            fi
                        '''
                    }
                }
            }
        }   
        //Deploy one time so that build is copied to GCS location
        stage('JDBC TO BQ'){
        when {
                        expression { return shouldRunOnChange() }
             }
            steps {
                retry(count: stageRetryCount) {
                    sh '''
                        export JARS="gs://dataproc-templates/jars/mysql-connector-java.jar"
                        
                        cd java
                        
                        bin/start.sh \
                        -- --template JDBCTOBIGQUERY \
                        --templateProperty jdbctobq.bigquery.location=$GCP_PROJECT.dataproc_templates.jdbctobq \
                        --templateProperty jdbctobq.jdbc.url="$TEST_JDBC_URL" \
                        --templateProperty jdbctobq.jdbc.driver.class.name=com.mysql.jdbc.Driver \
                        --templateProperty jdbctobq.sql="select * from test.employee" \
                        --templateProperty jdbctobq.write.mode=overwrite \
                        --templateProperty jdbctobq.temp.gcs.bucket=dataproc-templates/integration-testing/output/JDBCTOBIGQUERY 
                    '''
                }
            }
        }
        stage('Parallel Execution 1'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('HIVE TO BIGQUERY'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true

                                
                                
                                cd java
                                
                                bin/start.sh \
                                --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                -- --template HIVETOBIGQUERY \
                                --templateProperty hivetobq.bigquery.location=$GCP_PROJECT.dataproc_templates.test \
                                --templateProperty hivetobq.sql="select * from default.employee" \
                                --templateProperty hivetobq.write.mode=Overwrite \
                                --templateProperty hivetobq.temp.gcs.bucket=dataproc-templates/integration-testing/output/HIVETOBIGQUERY
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 2'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('PUBSUB TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
            
                                export SKIP_BUILD=true

                                
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template PUBSUBTOGCS \
                                --templateProperty pubsubtogcs.input.project.id=$GCP_PROJECT \
                                --templateProperty pubsubtogcs.input.subscription=pubsubtogcsv3-sub \
                                --templateProperty pubsubtogcs.gcs.bucket.name=gs://pubsubtogcs_dev/dpjson/ \
                                --templateProperty pubsubtogcs.gcs.output.data.format=json \
                                --templateProperty pubsubtogcs.batch.size=50
                            ''' 
                        }
                    }
                }
            stage('S3 TO BigQuery (avro)'){
                                steps {
                                    retry(count: stageRetryCount) {
                                    withCredentials([usernamePassword(credentialsId: 'aws-s3-ro-credentials',
                                    passwordVariable: 'S3_SECRET', usernameVariable: 'S3_KEY')]) {
                                        sh '''
                                            export SKIP_BUILD=true

                                            cd java

                                            bin/start.sh \
                                            -- --template S3TOBIGQUERY \
                                            --templateProperty project.id=$GCP_PROJECT \
                                            --templateProperty s3.bq.access.key=$S3_KEY \
                                            --templateProperty s3.bq.secret.key=$S3_SECRET \
                                            --templateProperty s3.bq.input.format=avro \
                                            --templateProperty s3.bq.input.location=s3a://dataproc-templates-integration-tests/cities.avro \
                                            --templateProperty s3.bq.output.dataset.name=dataproc_templates \
                                            --templateProperty s3.bq.output.table.name=s3_to_bq_avro \
                                            --templateProperty s3.bq.output.mode=Overwrite \
                                            --templateProperty s3.bq.ld.temp.bucket.name=dataproc-templates
                                        '''
                                       }
                                    }
                                }
                            }
            }
        }
        stage('Parallel Execution 3'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('Hive TO GCS'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true

                                
                                
                                cd java
                                
                                bin/start.sh \
                                --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                -- --template HIVETOGCS \
                                --templateProperty hive.input.table=employee \
                                --templateProperty hive.input.db=default \
                                --templateProperty hive.gcs.output.path=gs://dataproc-templates/integration-testing/output/HIVETOGCS
                            '''
                        }
                    }
                }
                stage('KAFKA TO BQ'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''                                

                                export SKIP_BUILD=true
                                export SPARK_PROPERTIES="dataproc.sparkBqConnector.uri=gs://spark-lib/bigquery/spark-3.3-bigquery-0.41.0.jar"
                                
                                cd java
                                
                                bin/start.sh \
                                -- \
                                --template KAFKATOBQ \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty kafka.bq.checkpoint.location=gs://dataproc-templates/integration-testing/checkpoint/KAFKATOBQ \
                                --templateProperty kafka.bq.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                --templateProperty kafka.bq.topic=integration-test-kafka-bq \
                                --templateProperty kafka.bq.dataset=kafkatobq \
                                --templateProperty kafka.bq.table=integration-test \
                                --templateProperty kafka.bq.temp.gcs.bucket=dataproc-templates-kafkatobq \
                                --templateProperty kafka.bq.await.termination.timeout=60000
                            '''
                        }
                    }
                }
                stage('PUBSUB TO BQ'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''                                
                                export SKIP_BUILD=true

                                
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template PUBSUBTOBQ \
                                --templateProperty pubsub.input.subscription=test-subscriber-pubsub-bq \
                                --templateProperty pubsub.input.project.id=$GCP_PROJECT \
                                --templateProperty pubsub.bq.output.project.id=$GCP_PROJECT \
                                --templateProperty pubsub.bq.output.dataset=dataproc_templates \
                                --templateProperty pubsub.bq.output.table=pubsubtobq \
                                --templateProperty pubsub.bq.output.gcs.checkpoint=gs://dataproc-templates/integration-testing/checkpoint/PUBSUBTOBQ
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 4'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('CASSANDRA TO GCS'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true

                                
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template CASSANDRATOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty cassandratogcs.input.keyspace=testkeyspace \
                                --templateProperty cassandratogcs.input.table=emp \
                                --templateProperty cassandratogcs.input.host=$ENV_TEST_CASSANDRA_HOST \
                                --templateProperty cassandratogcs.output.format=csv \
                                --templateProperty cassandratogcs.output.savemode=Overwrite \
                                --templateProperty cassandratogcs.output.path=gs://dataproc-templates/integration-testing/output/CASSANDRATOGCS/csv
                            '''
                        }
                    }
                }
                stage('CASSANDRA TO BigQuery'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true

                                
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template CASSANDRATOBQ \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty cassandratobq.input.keyspace=testkeyspace \
                                --templateProperty cassandratobq.input.table=emp \
                                --templateProperty cassandratobq.input.host=$ENV_TEST_CASSANDRA_HOST \
                                --templateProperty cassandratobq.bigquery.location=dataproc_templates.cassandra_bq_emp_table \
                                --templateProperty cassandratobq.output.mode=Overwrite \
                                --templateProperty cassandratobq.temp.gcs.location=temp-bucket-for-files
                            '''
                        }
                    }
                }
                stage('DATAPLEX GCS TO BIGQUERY'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true

                                
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template DATAPLEXGCSTOBQ \
                                --templateProperty=project.id=$GCP_PROJECT \
                                --templateProperty=dataplex.gcs.bq.target.entity=projects/$GCP_PROJECT/locations/us-west1/lakes/dataproc-templates-test-lake/zones/dataplex-gcs-to-bq/entities/dataplex_gcs_to_bq \
                                --templateProperty=gcs.bigquery.temp.bucket.name=dataplex-gcs-to-bq \
                                --templateProperty=dataplex.gcs.bq.save.mode=overwrite \
                                --templateProperty=dataplex.gcs.bq.incremental.partition.copy=no \
                                --dataplexEntity=projects/$GCP_PROJECT/locations/us-west1/lakes/dataproc-templates-test-lake/zones/dataplex-gcs-to-bq/entities/trips_parquet
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 5'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('KAFKA TO GCS'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true

                                
                                    
                                cd java

                                bin/start.sh \
                                -- --template KAFKATOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS/csv \
                                --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                --templateProperty kafka.topic=integration-test-kafka-gcs \
                                --templateProperty kafka.starting.offset=earliest \
                                --templateProperty kafka.gcs.output.format=csv \
                                --templateProperty kafka.message.format=bytes \
                                --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                            '''
                        }
                    }
                }

                // stage('HBASE TO GCS (Automated)(avro)') {
                //     when {
                //         // Run this stage only if JOB_TYPE is not set to CLUSTER
                //         expression { env.JOB_TYPE != "CLUSTER" }
                //     }
                //     steps{
                //         retry(count: stageRetryCount) {
                //             sh '''

                //                 gsutil cp gs://python-dataproc-templates/surjitsh/hbase-site.xml .
                //                 gcloud auth configure-docker us-docker.pkg.dev
                //                 export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                //                 export IMAGE_NAME_VERSION=hbase-to-gcs:1.0.3
                //                 export HBASE_SITE_PATH=../hbase-site.xml
                //                 export IMAGE=us-docker.pkg.dev/${GCP_PROJECT}/dataproc-cicd/${IMAGE_NAME_VERSION}
                //                 export JARS="gs://deps-dataproc-template/hbase-spark-protocol-shaded-1.0.1-spark_3.2-scala_2.12.jar,gs://deps-dataproc-template/hbase-spark-1.0.1-spark_3.2-scala_2.12.jar,gs://deps-dataproc-template/protobuf-java-2.5.0.jar,gs://deps-dataproc-template/htrace-core4-4.2.0-incubating.jar"

                //                 export SKIP_IMAGE_BUILD=TRUE
                //                 export SKIP_BUILD=true

                //                 cd java

                //                 bin/start.sh \
                //                 --container-image=$IMAGE \
                //                 --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'  \
                //                 -- --template HBASETOGCS \
                //                 --templateProperty hbasetogcs.output.fileformat=avro \
                //                 --templateProperty hbasetogcs.output.savemode=overwrite \
                //                 --templateProperty hbasetogcs.output.path=gs://dataproc-templates/integration-testing/output/HBASETOGCS_automated/avro  \
                //                 --templateProperty hbasetogcs.table.catalog=$CATALOG

                //             '''
                //         }
                //     }
                // }
            }
        }
        stage('Parallel Execution 6'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('GCS TO BIGTABLE'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''  
                                export SKIP_BUILD=true

                                

                                cd java

                                bin/start.sh \
                                -- --template GCSTOBIGTABLE \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty gcs.bigtable.input.location=gs://dataproc-templates/data/parquet/ \
                                --templateProperty gcs.bigtable.input.format=parquet \
                                --templateProperty gcs.bigtable.output.instance.id=$ENV_TEST_BIGTABLE_INSTANCE \
                                --templateProperty gcs.bigtable.output.project.id=$GCP_PROJECT \
                                --templateProperty gcs.bigtable.catalog.location="gs://dataproc-templates/conf/employeecatalog.json"
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 7'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('Cassandra to GCS (avro)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true

                                
                                    
                                cd java

                                bin/start.sh \
                                    -- --template CASSANDRATOGCS \
                                    --templateProperty project.id=$GCP_PROJECT \
                                    --templateProperty cassandratogcs.input.keyspace=testkeyspace \
                                    --templateProperty cassandratogcs.input.table=emp \
                                    --templateProperty cassandratogcs.input.host=$ENV_TEST_CASSANDRA_HOST \
                                    --templateProperty cassandratogcs.output.format=avro \
                                    --templateProperty cassandratogcs.output.savemode=Overwrite \
                                    --templateProperty cassandratogcs.output.path=gs://dataproc-templates/integration-testing/output/CASSANDRATOGCS/java_avro
                            '''
                        }
                    }
                }
                stage('KAFKA TO GCS (avro)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true

                                
                                    
                                cd java

                                bin/start.sh \
                                -- --template KAFKATOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS/avro \
                                --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                --templateProperty kafka.topic=integration-test-kafka-gcs-avro \
                                --templateProperty kafka.starting.offset=earliest \
                                --templateProperty kafka.gcs.output.format=avro \
                                --templateProperty kafka.message.format=bytes \
                                --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 8'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('BIGQUERY TO GCS (csv)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true

                                
                                    
                                cd java

                                bin/start.sh \
                                -- --template BIGQUERYTOGCS \
                                --templateProperty bigquery.gcs.input.table="dataproc_templates.bqtogcs" \
                                --templateProperty bigquery.gcs.output.format="csv" \
                                --templateProperty bigquery.gcs.output.mode="Overwrite" \
                                --templateProperty bigquery.gcs.output.location="gs://dataproc-templates/integration-testing/output/BIGQUERYTOGCS/csv/"
                            '''
                        }
                    }
                }

                stage('JDBC To JDBC'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java.jar"
                                export SKIP_BUILD=true
                                    
                                cd java

                                bin/start.sh \
                                -- --template JDBCTOJDBC \
                                --templateProperty jdbctojdbc.input.url="$TEST_JDBC_URL" \
                                --templateProperty jdbctojdbc.input.driver="com.mysql.cj.jdbc.Driver" \
                                --templateProperty jdbctojdbc.input.table="employee" \
                                --templateProperty jdbctojdbc.output.url="$TEST_JDBC_URL" \
                                --templateProperty jdbctojdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                --templateProperty jdbctojdbc.output.table="employee_output" \
                                --templateProperty jdbctojdbc.output.mode="Overwrite" 
                            '''
                        }
                    }
                }
                stage('GCS TO Mongo') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''

                                export SKIP_BUILD=true
                                export JARS="gs://dataproc-templates/integration-testing/jars/mongo-java-driver-3.9.1.jar,gs://dataproc-templates/integration-testing/jars/mongo-spark-connector_2.12-2.4.0.jar"

                                cd java

                                bin/start.sh \
                                -- --template GCSTOMONGO \
                                --templateProperty gcs.mongodb.input.format=avro \
                                --templateProperty gcs.mongodb.input.location=gs://dataproc-templates/data/avro/empavro \
                                --templateProperty gcs.mongodb.output.uri="$ENV_TEST_MONGO_DB_URI" \
                                --templateProperty gcs.mongodb.output.database=demo \
                                --templateProperty gcs.mongodb.output.collection=test \
                                --templateProperty gcs.mongo.output.mode=overwrite

                            '''
                        }
                    }
                }
                // stage('HBASE TO GCS (Manual)(avro)') {
                //     when {
                //         // Run this stage only if JOB_TYPE is not set to CLUSTER
                //         expression { env.JOB_TYPE != "CLUSTER" }
                //     }
                //     steps{
                //         retry(count: stageRetryCount) {
                //             sh '''
                //                 export SKIP_BUILD=true
                //                 export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                //                 export JARS="gs://deps-dataproc-template/hbase-spark-protocol-shaded-1.0.1-spark_3.2-scala_2.12.jar,gs://deps-dataproc-template/hbase-spark-1.0.1-spark_3.2-scala_2.12.jar,gs://deps-dataproc-template/protobuf-java-2.5.0.jar,gs://deps-dataproc-template/htrace-core4-4.2.0-incubating.jar"

                //                 cd java

                //                 bin/start.sh \
                //                 --container-image=us-docker.pkg.dev/${GCP_PROJECT}/dataproc-cicd/hbase-to-gcs:1.0.3 \
                //                 --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/' \
                //                 -- --template=HBASETOGCS \
                //                 --templateProperty hbasetogcs.output.fileformat=avro \
                //                 --templateProperty hbasetogcs.output.savemode=overwrite \
                //                 --templateProperty hbasetogcs.output.path=gs://dataproc-templates/integration-testing/output/HBASETOGCS_manual/avro  \
                //                 --templateProperty hbasetogcs.table.catalog=$CATALOG
                //             '''
                //         }
                //     }
                // }
            }
        }
        stage('Parallel Execution 9'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('KAFKA TO BQ via Dstream'){
                     steps {
                         retry(count: stageRetryCount) {
                             sh '''

                                  export SKIP_BUILD=true
                                  
                                  cd java
                                  bin/start.sh \
                                  -- \
                                  --template KafkaToBQDstream \
                                  --templateProperty project.id=$GCP_PROJECT \
                                  --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                  --templateProperty kafka.topic=integration-test-kafka-bq-dstream \
                                  --templateProperty kafka.bq.dataset=kafkatobq \
                                  --templateProperty kafka.starting.offset=latest \
                                  --templateProperty kafka.bq.batch.interval=60000 \
                                  --templateProperty kafka.bq.consumer.group.id=testgroup \
                                  --templateProperty kafka.bq.stream.output.mode=append \
                                  --templateProperty kafka.bq.table=integration-test \
                                  --templateProperty kafka.bq.temp.gcs.bucket=dataproc-templates-kafkatobq \
                                  --templateProperty kafka.bq.await.termination.timeout=60000
                              '''
                         }
                     }
                }
                stage('KAFKA TO GCS via Dstream (bytes)'){
                     steps {
                         retry(count: stageRetryCount) {
                             sh '''

                                  export SKIP_BUILD=true
                                  
                                  cd java
                                  bin/start.sh \
                                  -- \
                                  --template KafkaToGCSDstream \
                                  --templateProperty project.id=$GCP_PROJECT \
                                  --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                  --templateProperty kafka.topic=integration-test-kafka-gcs-dstream \
                                  --templateProperty kafka.starting.offset=latest \
                                  --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS_DStream/bytes \
                                  --templateProperty kafka.gcs.batch.interval=60000 \
                                  --templateProperty kafka.gcs.consumer.group.id=testgroupgcs \
                                  --templateProperty kafka.gcs.write.mode=append \
                                  --templateProperty kafka.message.format=bytes \
                                  --templateProperty kafka.gcs.output.format=parquet \
                                  --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                             '''
                         }
                     }
                }
                stage('KAFKA TO GCS via Dstream (json)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''

                                 export SKIP_BUILD=true
                                 
                                 cd java
                                 bin/start.sh \
                                 -- \
                                 --template KafkaToGCSDstream \
                                 --templateProperty project.id=$GCP_PROJECT \
                                 --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                 --templateProperty kafka.topic=integration-test-json-kafka-gcs-dstream \
                                 --templateProperty kafka.starting.offset=latest \
                                 --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS_DStream/json \
                                 --templateProperty kafka.gcs.batch.interval=60000 \
                                 --templateProperty kafka.gcs.consumer.group.id=testgroupjson \
                                 --templateProperty kafka.gcs.write.mode=append \
                                 --templateProperty kafka.message.format=json \
                                 --templateProperty kafka.schema.url=gs://dataproc-templates/integration-testing/schema.json \
                                 --templateProperty kafka.gcs.output.format=parquet \
                                 --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                            '''
                        }
                    }
                }
                stage('MONGO TO BIGQUERY'){
                    steps{
                        retry(count: stageRetryCount){
                            sh'''

                                export SKIP_BUILD=true
                                export JARS="gs://dataproc-templates/integration-testing/jars/mongo-java-driver-3.9.1.jar,gs://dataproc-templates/integration-testing/jars/mongo-spark-connector_2.12-2.4.0.jar"

                                cd java
                                bin/start.sh \
                                -- \
                                --template MongoToBQ \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty mongo.bq.input.uri="$ENV_TEST_MONGO_DB_URI" \
                                --templateProperty mongo.bq.input.database=demo \
                                --templateProperty mongo.bq.input.collection=dummyusers \
                                --templateProperty mongo.bq.output.dataset=dataproc_templates \
                                --templateProperty mongo.bq.output.table=mongotobq \
                                --templateProperty mongo.bq.output.mode=Append \
                                --templateProperty mongo.bq.temp.bucket.name=dataproc-templates/integration-testing/mongotobq
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 10'){
        when {
                        expression { return shouldRunOnChange() }
             }
            parallel{
                stage('GCS TO SPANNER POSTGRESQL DIALECT') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''

                                gcloud spanner databases execute-sql pgsqltest --instance=$ENV_TEST_SPANNER_ID --sql='DELETE FROM badges;'
                                export SKIP_BUILD=true
                                

                                cd java

                                bin/start.sh \
                                    -- --template GCSTOSPANNER \
                                    --templateProperty project.id=$GCP_PROJECT \
                                    --templateProperty gcs.spanner.input.format=avro \
                                    --templateProperty gcs.spanner.input.location=gs://dataproc-templates/data/avro/empavro \
                                    --templateProperty gcs.spanner.output.instance=$ENV_TEST_SPANNER_ID \
                                    --templateProperty gcs.spanner.output.database=pgsqltest   \
                                    --templateProperty gcs.spanner.output.table=badges \
                                    --templateProperty gcs.spanner.output.saveMode=Append \
                                    --templateProperty gcs.spanner.output.primaryKey=empno \
                                    --templateProperty spanner.jdbc.dialect=postgresql
                            '''
                        }
                    }
                }
                stage('BIGQUERY TO JDBC') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''

                                export JARS="gs://dataproc-templates/jars/mysql-connector-java.jar"
                                export SKIP_BUILD=true

                                cd java

                                bin/start.sh \
                                -- --template BIGQUERYTOJDBC \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty bigquery.jdbc.input.table="$GCP_PROJECT.dataproc_templates.emp_table" \
                                --templateProperty bigquery.jdbc.dataset.name='dataproc_templates' \
                                --templateProperty bigquery.jdbc.url="$TEST_JDBC_URL" \
                                --templateProperty bigquery.jdbc.output.table='emp_table' \
                                --templateProperty bigquery.jdbc.batch.size=10 \
                                --templateProperty bigquery.jdbc.output.driver='com.mysql.jdbc.Driver' \
                                --templateProperty bigquery.jdbc.output.mode='Overwrite'
                            '''
                        }
                    }
                }
                stage('JDBC TO SPANNER POSTGRESQL DIALECT'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''

                                gcloud spanner databases execute-sql pgsqltest --instance=$ENV_TEST_SPANNER_ID --sql='DELETE FROM pgdialect;'
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java.jar"
                                export SKIP_BUILD=true

                                cd java

                                bin/start.sh \
                                -- --template JDBCTOSPANNER \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty jdbctospanner.jdbc.url="$TEST_JDBC_URL" \
                                --templateProperty jdbctospanner.jdbc.driver.class.name=com.mysql.jdbc.Driver \
                                --templateProperty jdbctospanner.sql="select * from test.pgdialect" \
                                --templateProperty jdbctospanner.output.instance=$ENV_TEST_SPANNER_ID \
                                --templateProperty jdbctospanner.output.database=pgsqltest \
                                --templateProperty jdbctospanner.output.table=pgdialect \
                                --templateProperty jdbctospanner.output.saveMode=Append \
                                --templateProperty jdbctospanner.output.primaryKey='empno' \
                                --templateProperty spanner.jdbc.dialect=postgresql
                            '''
                        }
                    }
                }
                // stage('HBASE TO GCS (Manual)') {
                //     when {
                //         // Run this stage only if JOB_TYPE is not set to CLUSTER
                //         expression { env.JOB_TYPE != "CLUSTER" }
                //     }
                //     steps{
                //         retry(count: stageRetryCount) {
                //             sh '''
                //                 export SKIP_BUILD=true
                //                 export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                //                 export JARS="gs://deps-dataproc-template/hbase-spark-protocol-shaded-1.0.1-spark_3.2-scala_2.12.jar,gs://deps-dataproc-template/hbase-spark-1.0.1-spark_3.2-scala_2.12.jar,gs://deps-dataproc-template/protobuf-java-2.5.0.jar,gs://deps-dataproc-template/htrace-core4-4.2.0-incubating.jar"

                //                 cd java

                //                 bin/start.sh \
                //                 --container-image=us-docker.pkg.dev/${GCP_PROJECT}/dataproc-cicd/hbase-to-gcs:1.0.3 \
                //                 --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/' \
                //                 -- --template=HBASETOGCS \
                //                 --templateProperty hbasetogcs.output.fileformat=csv \
                //                 --templateProperty hbasetogcs.output.savemode=overwrite \
                //                 --templateProperty hbasetogcs.output.path=gs://dataproc-templates/integration-testing/output/HBASETOGCS_manual/csv  \
                //                 --templateProperty hbasetogcs.table.catalog=$CATALOG
                //             '''
                //         }
                //     }
                // }
                // stage('HBASE TO GCS (Automated)') {
                //     when {
                //         // Run this stage only if JOB_TYPE is not set to CLUSTER
                //         expression { env.JOB_TYPE != "CLUSTER" }
                //     }
                //     steps{
                //         retry(count: stageRetryCount) {
                //             sh '''
                //                 gsutil cp gs://python-dataproc-templates/surjitsh/hbase-site.xml .
                //                 gcloud auth configure-docker us-docker.pkg.dev
                //                 export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                //                 export IMAGE_NAME_VERSION=hbase-to-gcs:1.0.3
                //                 export HBASE_SITE_PATH=../hbase-site.xml
                //                 export IMAGE=us-docker.pkg.dev/${GCP_PROJECT}/dataproc-cicd/${IMAGE_NAME_VERSION}
                //                 export JARS="gs://deps-dataproc-template/hbase-spark-protocol-shaded-1.0.1-spark_3.2-scala_2.12.jar,gs://deps-dataproc-template/hbase-spark-1.0.1-spark_3.2-scala_2.12.jar,gs://deps-dataproc-template/protobuf-java-2.5.0.jar,gs://deps-dataproc-template/htrace-core4-4.2.0-incubating.jar"

                //                 export SKIP_IMAGE_BUILD=TRUE
                //                 export SKIP_BUILD=true

                //                 cd java

                //                 bin/start.sh \
                //                 --container-image=$IMAGE \
                //                 --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'  \
                //                 -- --template HBASETOGCS \
                //                 --templateProperty hbasetogcs.output.fileformat=csv \
                //                 --templateProperty hbasetogcs.output.savemode=overwrite \
                //                 --templateProperty hbasetogcs.output.path=gs://dataproc-templates/integration-testing/output/HBASETOGCS_automated/csv  \
                //                 --templateProperty hbasetogcs.table.catalog=$CATALOG

                //             '''
                //         }
                //     }
                // }
            }
        }
    }
    post {
        always{
            script {
                if( env.GIT_BRANCH_LOCAL == 'main' ){
                    googlechatnotification url: DATAPROC_TELEPORT_WEBHOOK_URL,
    				message: 'Jenkins: ${JOB_NAME}\nBuild status is ${BUILD_STATUS}\nSee ${BUILD_URL}\n',
    				notifyFailure: 'true',
    				notifyAborted: 'true',
    				notifyUnstable: 'true',
    				notifyNotBuilt: 'true',
    				notifyBackToNormal: 'true'
                }
            }
            catchError {
                sh '''
                if [ $GIT_BRANCH_LOCAL != "main" ];then
                    gsutil rm -r $GCS_STAGING_LOCATION 2> /dev/null || true
                fi
                '''
            }
        }
    }
}