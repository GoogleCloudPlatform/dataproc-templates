def stageRetryCount = 3

pipeline {
    
    agent any

    environment {
        DATAPROC_TELEPORT_WEBHOOK_URL = credentials('dataproc-teleport-webhook-url')

        TEST_JDBC_URL = credentials('env-test-jdbc-url')

        GIT_BRANCH_LOCAL = sh (
            script: "echo $branchName | sed -e 's|origin/||g' | sed -e 's|^null\$|main|'",  // Remove "origin/" and set the default branch to main
            returnStdout: true
        ).trim()
        
        MAVEN_HOME = "/var/lib/jenkins/tools/hudson.tasks.Maven_MavenInstallation/maven"
        PATH = "$PATH:$MAVEN_HOME/bin"

        GCS_STAGING_LOCATION = sh (script: '''
            CURRENT_BRANCH=`echo $branchName | sed -e 's|origin/||g' | sed -e 's|^null\$|main|'`
            if [ $CURRENT_BRANCH != "main" ];then
            echo "$GCS_STAGING_LOCATION/$(uuidgen)"
            else
            echo $GCS_STAGING_LOCATION
            fi
            '''.stripIndent(),
            returnStdout: true
        ).trim()
    }
    stages {
        stage('Prepare Environment'){
            parallel{
                stage('Checkout') {
                    steps{
                        git branch: "${GIT_BRANCH_LOCAL}", changelog: false, poll: false, url: 'https://github.com/GoogleCloudPlatform/dataproc-templates/'    
                    }
                }
                stage('Reset Resources'){
                    steps {
                            catchError {
                                sh '''
                                    gcloud pubsub topics publish pubsubtogcsv3 --message='{"Name": "NewMsg", "Age": 10}'
                                    gcloud pubsub topics publish test-pubsub-bq --message='{"Name": "Another message", "Age": 18}' 2> /dev/null || true

                                    gsutil rm -r gs://dataproc-templates/integration-testing/checkpoint/KAFKATOBQ 2> /dev/null || true
                                    gsutil rm -r gs://dataproc-templates/integration-testing/output/KAFKATOGCS 2> /dev/null || true
                                    gsutil rm -r gs://dataproc-templates/integration-testing/output/KAFKATOGCS_DStream 2> /dev/null || true

                                '''
                            }
                    }
                }
                stage('Cluster Creation'){
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE == "CLUSTER" }
                    }
                    steps{
                        sh '''
                            if gcloud dataproc clusters list --region=$REGION --project=$GCP_PROJECT | grep -q $CLUSTER; then
                                echo "Cluster $CLUSTER already exists."
                            else
                                echo "Cluster $CLUSTER does not exist. Creating now..."
                                gcloud dataproc clusters create $CLUSTER \
                                --region $REGION \
                                --subnet $SUBNET \
                                --no-address \
                                --master-machine-type n1-standard-2 \
                                --master-boot-disk-size 500 \
                                --num-workers 2 \
                                --worker-machine-type n1-standard-2 \
                                --worker-boot-disk-size 500 \
                                --image-version 2.1-debian11 \
                                --optional-components ZOOKEEPER \
                                --max-idle 1800s \
                                --project $GCP_PROJECT
                            fi
                        '''
                    }
                }
            }
        }   
        //Deploy one time so that build is copied to GCS location
        stage('JDBC TO BQ'){
            steps {
                retry(count: stageRetryCount) {
                    sh '''
                        export JARS=gs://dataproc-templates/jars/mysql-connector-java.jar
                        
                        cd java
                        
                        bin/start.sh \
                        -- --template JDBCTOBIGQUERY \
                        --templateProperty jdbctobq.bigquery.location=$GCP_PROJECT.dataproc_templates.jdbctobq \
                        --templateProperty jdbctobq.jdbc.url="$TEST_JDBC_URL" \
                        --templateProperty jdbctobq.jdbc.driver.class.name=com.mysql.jdbc.Driver \
                        --templateProperty jdbctobq.sql="select * from test.employee" \
                        --templateProperty jdbctobq.write.mode=overwrite \
                        --templateProperty jdbctobq.temp.gcs.bucket=dataproc-templates/integration-testing/output/JDBCTOBIGQUERY 
                    '''
                }
            }
        }
        stage('Parallel Execution 1'){
            parallel{
                stage('GCS TO BIGQUERY(avro)') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template GCSTOBIGQUERY \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty gcs.bigquery.input.location="gs://dataproc-templates/integration-testing/gcstobigquery/cities.avro" \
                                --templateProperty gcs.bigquery.input.format=avro \
                                --templateProperty gcs.bigquery.output.dataset="dataproc_templates" \
                                --templateProperty gcs.bigquery.output.table="gcs_to_bq_avro" \
                                --templateProperty gcs.bigquery.output.mode="Overwrite" \
                                --templateProperty gcs.bigquery.temp.bucket.name=dataproc-templates
                            '''
                        }
                    }
                }
                stage('GCS TO SPANNER') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true

                                cd java
                                
                                bin/start.sh \
                                    -- --template GCSTOSPANNER \
                                    --templateProperty project.id=$GCP_PROJECT \
                                    --templateProperty gcs.spanner.input.format=avro \
                                    --templateProperty gcs.spanner.input.location=gs://dataproc-templates/data/avro/empavro \
                                    --templateProperty gcs.spanner.output.instance=$ENV_TEST_SPANNER_ID \
                                    --templateProperty gcs.spanner.output.database=spark-ci-db   \
                                    --templateProperty gcs.spanner.output.table=badges \
                                    --templateProperty gcs.spanner.output.saveMode=Overwrite \
                                    --templateProperty gcs.spanner.output.primaryKey=empno
                            '''
                        }
                    }
                }
                stage('SPANNER TO GCS'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''

                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template SPANNERTOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty spanner.gcs.input.spanner.id=$ENV_TEST_SPANNER_ID \
                                --templateProperty spanner.gcs.input.database.id=spark-ci-db \
                                --templateProperty "spanner.gcs.input.table.id=(select id,name from badges2 where name = 'Teacher' limit 10000)" \
                                --templateProperty spanner.gcs.output.gcs.path=dataproc-templates/integration-testing/output/SPANNERTOGCS/parquet \
                                --templateProperty spanner.gcs.output.gcs.saveMode=Overwrite \
                                --templateProperty spanner.gcs.output.gcs.format=parquet
                            '''
                        }
                    }
                }
                stage('HIVE TO BIGQUERY'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                -- --template HIVETOBIGQUERY \
                                --templateProperty hivetobq.bigquery.location=$GCP_PROJECT.dataproc_templates.test \
                                --templateProperty hivetobq.sql="select * from default.employee" \
                                --templateProperty hivetobq.write.mode=Overwrite \
                                --templateProperty hivetobq.temp.gcs.bucket=dataproc-templates/integration-testing/output/HIVETOBIGQUERY
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 2'){
            parallel{
                stage('GCS TO JDBC') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                export JARS=gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar
                                export SKIP_BUILD=true

                                cd java
                                
                                bin/start.sh \
                                    -- --template GCSTOJDBC \
                                    --templateProperty project.id=$GCP_PROJECT \
                                    --templateProperty gcs.jdbc.input.location=gs://dataproc-templates/data/avro/empavro \
                                    --templateProperty gcs.jdbc.input.format=avro \
                                    --templateProperty gcs.jdbc.output.table=avrodemo \
                                    --templateProperty gcs.jdbc.output.saveMode=Overwrite   \
                                    --templateProperty gcs.jdbc.output.url="$TEST_JDBC_URL" \
                                    --templateProperty gcs.jdbc.output.driver='com.mysql.jdbc.Driver'
                            '''
                        }
                    }
                }
                stage('PUBSUB TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
            
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template PUBSUBTOGCS \
                                --templateProperty pubsubtogcs.input.project.id=$GCP_PROJECT \
                                --templateProperty pubsubtogcs.input.subscription=pubsubtogcsv3-sub \
                                --templateProperty pubsubtogcs.gcs.bucket.name=pubsubtogcs_dev \
                                --templateProperty pubsubtogcs.gcs.output.data.format=JSON \
                                --templateProperty pubsubtogcs.batch.size=50
                            ''' 
                        }
                    }
                }
                stage('JDBC TO GCS (csv)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export JARS=gs://dataproc-templates/jars/mysql-connector-java.jar
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template JDBCTOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty jdbctogcs.jdbc.url="$TEST_JDBC_URL" \
                                --templateProperty jdbctogcs.jdbc.driver.class.name=com.mysql.jdbc.Driver \
                                --templateProperty jdbctogcs.output.location=gs://dataproc-templates/integration-testing/output/JDBCTOGCS/csv \
                                --templateProperty jdbctogcs.output.format=csv \
                                --templateProperty jdbctogcs.write.mode=overwrite \
                                --templateProperty jdbctogcs.sql="select * from test.employee" 
                            '''
                        }
                    }
                }
                stage('JDBC TO GCS (avro)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export JARS=gs://dataproc-templates/jars/mysql-connector-java.jar
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template JDBCTOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty jdbctogcs.jdbc.url="$TEST_JDBC_URL" \
                                --templateProperty jdbctogcs.jdbc.driver.class.name=com.mysql.jdbc.Driver \
                                --templateProperty jdbctogcs.output.location=gs://dataproc-templates/integration-testing/output/JDBCTOGCS/avro \
                                --templateProperty jdbctogcs.output.format=avro \
                                --templateProperty jdbctogcs.write.mode=overwrite \
                                --templateProperty jdbctogcs.sql="select * from test.employee" 
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 3'){
            parallel{
                stage('Hive TO GCS'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                -- --template HIVETOGCS \
                                --templateProperty hive.input.table=employee \
                                --templateProperty hive.input.db=default \
                                --templateProperty hive.gcs.output.path=gs://dataproc-templates/integration-testing/output/HIVETOGCS
                            '''
                        }
                    }
                }
                stage('KAFKA TO BQ'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''                                

                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- \
                                --template KAFKATOBQ \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty kafka.bq.checkpoint.location=gs://dataproc-templates/integration-testing/checkpoint/KAFKATOBQ \
                                --templateProperty kafka.bq.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                --templateProperty kafka.bq.topic=integration-test-kafka-bq \
                                --templateProperty kafka.bq.dataset=kafkatobq \
                                --templateProperty kafka.bq.table=integration-test \
                                --templateProperty kafka.bq.temp.gcs.bucket=dataproc-templates-kafkatobq \
                                --templateProperty kafka.bq.await.termination.timeout=60000
                            '''
                        }
                    }
                }
                stage('GCS TO GCS (avro)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export JARS=gs://dataproc-templates/jars/mysql-connector-java.jar
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template GCSTOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty gcs.gcs.input.location=gs://dataproc-templates/data/avro/empavro \
                                --templateProperty gcs.gcs.input.format=avro \
                                --templateProperty gcs.gcs.output.location=gs://dataproc-templates/integration-testing/output/GCSTOGCS/avro \
                                --templateProperty gcs.gcs.output.format=csv \
                                --templateProperty gcs.gcs.write.mode=overwrite \
                                --templateProperty gcs.gcs.temp.table=dataset \
                                --templateProperty gcs.gcs.temp.query='select * from global_temp.dataset where sal>1500'
                            '''
                        }
                    }
                }
                stage('PUBSUB TO BQ'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''                                
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template PUBSUBTOBQ \
                                --templateProperty pubsub.input.subscription=test-subscriber-pubsub-bq \
                                --templateProperty pubsub.input.project.id=$GCP_PROJECT \
                                --templateProperty pubsub.bq.output.project.id=$GCP_PROJECT \
                                --templateProperty pubsub.bq.output.dataset=dataproc_templates \
                                --templateProperty pubsub.bq.output.table=pubsubtobq
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 4'){
            parallel{
                stage('JDBC TO SPANNER'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''

                                export JARS=gs://dataproc-templates/jars/mysql-connector-java.jar
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template JDBCTOSPANNER \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty jdbctospanner.jdbc.url="$TEST_JDBC_URL" \
                                --templateProperty jdbctospanner.jdbc.driver.class.name=com.mysql.jdbc.Driver \
                                --templateProperty jdbctospanner.sql="select * from test.employee" \
                                --templateProperty jdbctospanner.output.instance=$ENV_TEST_SPANNER_ID \
                                --templateProperty jdbctospanner.output.database=spark-ci-db \
                                --templateProperty jdbctospanner.output.table=employee \
                                --templateProperty jdbctospanner.output.saveMode=Overwrite \
                                --templateProperty jdbctospanner.output.primaryKey='empno'
                            '''
                        }
                    }
                }
                stage('CASSANDRA TO GCS'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template CASSANDRATOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty cassandratogcs.input.keyspace=testkeyspace \
                                --templateProperty cassandratogcs.input.table=emp \
                                --templateProperty cassandratogcs.input.host=$ENV_TEST_CASSANDRA_HOST \
                                --templateProperty cassandratogcs.output.format=csv \
                                --templateProperty cassandratogcs.output.savemode=Overwrite \
                                --templateProperty cassandratogcs.output.path=gs://dataproc-templates/integration-testing/output/CASSANDRATOGCS/csv
                            '''
                        }
                    }
                }
                stage('CASSANDRA TO BigQuery'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template CASSANDRATOBQ \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty cassandratobq.input.keyspace=testkeyspace \
                                --templateProperty cassandratobq.input.table=emp \
                                --templateProperty cassandratobq.input.host=$ENV_TEST_CASSANDRA_HOST \
                                --templateProperty cassandratobq.bigquery.location=dataproc_templates.cassandra_bq_emp_table \
                                --templateProperty cassandratobq.output.mode=Overwrite \
                                --templateProperty cassandratobq.temp.gcs.location=temp-bucket-for-files
                            '''
                        }
                    }
                }
                stage('DATAPLEX GCS TO BIGQUERY'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template DATAPLEXGCSTOBQ \
                                --templateProperty=project.id=$GCP_PROJECT \
                                --templateProperty=dataplex.gcs.bq.target.entity=projects/$GCP_PROJECT/locations/us-west1/lakes/dataproc-templates-test-lake/zones/dataplex-gcs-to-bq/entities/dataplex_gcs_to_bq \
                                --templateProperty=gcs.bigquery.temp.bucket.name=dataplex-gcs-to-bq \
                                --templateProperty=dataplex.gcs.bq.save.mode=overwrite \
                                --templateProperty=dataplex.gcs.bq.incremental.partition.copy=no \
                                --dataplexEntity=projects/$GCP_PROJECT/locations/us-west1/lakes/dataproc-templates-test-lake/zones/dataplex-gcs-to-bq/entities/trips_parquet
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 5'){
            parallel{
                stage('KAFKA TO GCS'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                    
                                cd java

                                bin/start.sh \
                                -- --template KAFKATOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS/csv \
                                --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                --templateProperty kafka.topic=integration-test-kafka-gcs \
                                --templateProperty kafka.starting.offset=earliest \
                                --templateProperty kafka.gcs.output.format=csv \
                                --templateProperty kafka.message.format=bytes \
                                --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                            '''
                        }
                    }
                }
                stage('HBASE TO GCS (Manual)') {
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE != "CLUSTER" }
                    }
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                                export JARS="gs://deps-dataproc-template/hbase-shaded-mapreduce-2.4.12.jar,gs://deps-dataproc-template/hbase-client-2.4.12.jar,gs://deps-dataproc-template/htrace-core4-4.2.0-incubating.jar,file:///usr/lib/spark/external/hbase-spark-protocol-shaded.jar,file:///usr/lib/spark/external/hbase-spark.jar"
                                
                                bin/start.sh \
                                --container-image=gcr.io/$GCP_PROJECT/hbase-to-gcs:1.0.1 \
                                --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/' \
                                -- --template=HBASETOGCS \
                                --templateProperty hbasetogcs.output.fileformat=csv \
                                --templateProperty hbasetogcs.output.savemode=overwrite \
                                --templateProperty hbasetogcs.output.path=gs://dataproc-templates/integration-testing/output/HBASETOGCS_manual/csv  \
                                --templateProperty hbasetogcs.table.catalog=$CATALOG
                            '''
                        }
                    }
                }
                stage('HBASE TO GCS (Automated)') {
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE != "CLUSTER" }
                    }
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                gsutil cp gs://python-dataproc-templates/surjitsh/hbase-site.xml .
                                gcloud auth configure-docker
                                export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                                export IMAGE_NAME_VERSION=hbase-to-gcs:1.0.1
                                export HBASE_SITE_PATH=../hbase-site.xml
                                export IMAGE=gcr.io/${GCP_PROJECT}/${IMAGE_NAME_VERSION}
                                export SKIP_IMAGE_BUILD=TRUE
                                
                                cd java
                                
                                bin/start.sh \
                                --container-image=$IMAGE \
                                --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'  \
                                -- --template HBASETOGCS \
                                --templateProperty hbasetogcs.output.fileformat=csv \
                                --templateProperty hbasetogcs.output.savemode=overwrite \
                                --templateProperty hbasetogcs.output.path=gs://dataproc-templates/integration-testing/output/HBASETOGCS_automated/csv  \
                                --templateProperty hbasetogcs.table.catalog=$CATALOG
                                
                            '''
                        }
                    }
                }
                stage('GCS TO GCS (csv)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template GCSTOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty gcs.gcs.input.location=gs://dataproc-templates/integration-testing/gcstogcs/csvtoavro/csv/cities.csv \
                                --templateProperty gcs.gcs.input.format=csv \
                                --templateProperty gcs.gcs.output.location=gs://dataproc-templates/integration-testing/output/GCSTOGCS/avro \
                                --templateProperty gcs.gcs.output.format=avro \
                                --templateProperty gcs.gcs.write.mode=overwrite
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 6'){
            parallel{
                stage('GCS TO BIGTABLE'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''  
                                export SKIP_BUILD=true

                                cd java

                                bin/start.sh \
                                -- --template GCSTOBIGTABLE \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty gcs.bigtable.input.location=gs://dataproc-templates/data/csv/GCSToBigTable_cities.csv \
                                --templateProperty gcs.bigtable.input.format=csv \
                                --templateProperty gcs.bigtable.output.instance.id=$ENV_TEST_BIGTABLE_INSTANCE \
                                --templateProperty gcs.bigtable.output.project.id=$GCP_PROJECT \
                                --templateProperty gcs.bigtable.table.name=test \
                                --templateProperty gcs.bigtable.column.family=Name
                            '''
                        }
                    }
                }
                stage('SPANNER TO GCS (csv)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                    
                                cd java

                                bin/start.sh \
                                -- --template SPANNERTOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty spanner.gcs.input.spanner.id=$ENV_TEST_SPANNER_ID \
                                --templateProperty spanner.gcs.input.database.id=test-db \
                                --templateProperty spanner.gcs.input.table.id=shakespeare \
                                --templateProperty spanner.gcs.output.gcs.path=gs://dataproc-templates/integration-testing/output/SPANNERTOGCS/csv \
                                --templateProperty spanner.gcs.output.gcs.saveMode=Overwrite \
                                --templateProperty spanner.gcs.output.gcs.format=csv
                            '''
                        }
                    }
                }
                stage('SPANNER TO GCS (avro)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                    
                                cd java

                                bin/start.sh \
                                -- --template SPANNERTOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty spanner.gcs.input.spanner.id=$ENV_TEST_SPANNER_ID \
                                --templateProperty spanner.gcs.input.database.id=test-db \
                                --templateProperty spanner.gcs.input.table.id=shakespeare \
                                --templateProperty spanner.gcs.output.gcs.path=gs://dataproc-templates/integration-testing/output/SPANNERTOGCS/avro \
                                --templateProperty spanner.gcs.output.gcs.saveMode=Overwrite \
                                --templateProperty spanner.gcs.output.gcs.format=avro
                            '''
                        }
                    }
                }
                stage('GCS TO BIGQUERY(csv)') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                bin/start.sh \
                                -- --template GCSTOBIGQUERY \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty gcs.bigquery.input.location=gs://dataproc-templates/integration-testing/gcstobigquery/cities.csv \
                                --templateProperty gcs.bigquery.input.format=csv \
                                --templateProperty gcs.bigquery.output.dataset=dataproc_templates \
                                --templateProperty gcs.bigquery.output.table=gcs_bq_cities \
                                --templateProperty gcs.bigquery.temp.bucket.name=dataproc-templates
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 7'){
            parallel{
                stage('Cassandra to GCS (avro)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                    
                                cd java

                                bin/start.sh \
                                    -- --template CASSANDRATOGCS \
                                    --templateProperty project.id=$GCP_PROJECT \
                                    --templateProperty cassandratogcs.input.keyspace=testkeyspace \
                                    --templateProperty cassandratogcs.input.table=emp \
                                    --templateProperty cassandratogcs.input.host=$ENV_TEST_CASSANDRA_HOST \
                                    --templateProperty cassandratogcs.output.format=avro \
                                    --templateProperty cassandratogcs.output.savemode=Overwrite \
                                    --templateProperty cassandratogcs.output.path=gs://dataproc-templates/integration-testing/output/CASSANDRATOGCS/java_avro
                            '''
                        }
                    }
                }
                stage('KAFKA TO GCS (avro)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                    
                                cd java

                                bin/start.sh \
                                -- --template KAFKATOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS/avro \
                                --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                --templateProperty kafka.topic=integration-test-kafka-gcs-avro \
                                --templateProperty kafka.starting.offset=earliest \
                                --templateProperty kafka.gcs.output.format=avro \
                                --templateProperty kafka.message.format=bytes \
                                --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                            '''
                        }
                    }
                }
                stage('BIGQUERY TO GCS (avro)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                    
                                cd java
                                
                                export JARS=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar

                                bin/start.sh \
                                -- --template=BIGQUERYTOGCS \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty bigquery.gcs.input.table=$GCP_PROJECT:dataproc_templates.emp_table \
                                --templateProperty bigquery.gcs.output.format=avro \
                                --templateProperty bigquery.gcs.output.mode=Overwrite \
                                --templateProperty bigquery.gcs.output.location=gs://dataproc-templates/integration-testing/output/BIGQUERYTOGCS/avro
                            '''
                        }
                    }
                }
                stage('TEXT TO BIGQUERY'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                    
                                cd java

                                bin/start.sh \
                                -- --template TEXTTOBIGQUERY \
                                --templateProperty project.id=$GCP_PROJECT \
                                --templateProperty text.bigquery.input.location=gs://dataproc-templates/data/deflate \
                                --templateProperty text.bigquery.input.compression=deflate \
                                --templateProperty text.bigquery.input.delimiter=, \
                                --templateProperty text.bigquery.output.dataset=dataproc_templates \
                                --templateProperty text.bigquery.output.table=texttobigquery \
                                --templateProperty text.bigquery.output.mode=Overwrite \
                                --templateProperty text.bigquery.temp.bucket=dataproc-templates/texttobigquery
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 8'){
            parallel{
                stage('BIGQUERY TO GCS (csv)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                    
                                cd java

                                bin/start.sh \
                                -- --template BIGQUERYTOGCS \
                                --templateProperty bigquery.gcs.input.table="dataproc_templates.bqtogcs" \
                                --templateProperty bigquery.gcs.output.format="csv" \
                                --templateProperty bigquery.gcs.output.mode="Overwrite" \
                                --templateProperty bigquery.gcs.output.location="gs://dataproc-templates/integration-testing/output/BIGQUERYTOGCS/csv/"
                            '''
                        }
                    }
                }
                stage('HBASE TO GCS (Manual)(avro)') {
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE != "CLUSTER" }
                    }
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                
                                cd java
                                
                                export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                                export JARS="gs://deps-dataproc-template/hbase-shaded-mapreduce-2.4.12.jar,gs://deps-dataproc-template/hbase-client-2.4.12.jar,gs://deps-dataproc-template/htrace-core4-4.2.0-incubating.jar,file:///usr/lib/spark/external/hbase-spark-protocol-shaded.jar,file:///usr/lib/spark/external/hbase-spark.jar"
                                
                                bin/start.sh \
                                --container-image=gcr.io/$GCP_PROJECT/hbase-to-gcs:1.0.1 \
                                --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/' \
                                -- --template=HBASETOGCS \
                                --templateProperty hbasetogcs.output.fileformat=avro \
                                --templateProperty hbasetogcs.output.savemode=overwrite \
                                --templateProperty hbasetogcs.output.path=gs://dataproc-templates/integration-testing/output/HBASETOGCS_manual/avro  \
                                --templateProperty hbasetogcs.table.catalog=$CATALOG
                            '''
                        }
                    }
                }
                stage('HBASE TO GCS (Automated)(avro)') {
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE != "CLUSTER" }
                    }
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                gsutil cp gs://python-dataproc-templates/surjitsh/hbase-site.xml .
                                gcloud auth configure-docker
                                export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                                export IMAGE_NAME_VERSION=hbase-to-gcs-java-automated:1.0.0
                                export HBASE_SITE_PATH=../hbase-site.xml
                                export IMAGE=gcr.io/${GCP_PROJECT}/${IMAGE_NAME_VERSION}
                                export SKIP_IMAGE_BUILD=TRUE
                                
                                cd java
                                
                                bin/start.sh \
                                --container-image=$IMAGE \
                                --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'  \
                                -- --template HBASETOGCS \
                                --templateProperty hbasetogcs.output.fileformat=avro \
                                --templateProperty hbasetogcs.output.savemode=overwrite \
                                --templateProperty hbasetogcs.output.path=gs://dataproc-templates/integration-testing/output/HBASETOGCS_automated/avro  \
                                --templateProperty hbasetogcs.table.catalog=$CATALOG
                                
                            '''
                        }
                    }
                }
                stage('JDBC To JDBC'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''
                                export JARS=gs://dataproc-templates/jars/mysql-connector-java.jar
                                export SKIP_BUILD=true
                                    
                                cd java

                                bin/start.sh \
                                -- --template JDBCTOJDBC \
                                --templateProperty jdbctojdbc.input.url="$TEST_JDBC_URL" \
                                --templateProperty jdbctojdbc.input.driver="com.mysql.cj.jdbc.Driver" \
                                --templateProperty jdbctojdbc.input.table="employee" \
                                --templateProperty jdbctojdbc.output.url="$TEST_JDBC_URL" \
                                --templateProperty jdbctojdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                --templateProperty jdbctojdbc.output.table="employee_output" \
                                --templateProperty jdbctojdbc.output.mode="Overwrite" 
                            '''
                        }
                    }
                }
                stage('GCS TO Mongo') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''

                                export SKIP_BUILD=true
                                export JARS="gs://dataproc-templates/integration-testing/jars/mongo-java-driver-3.9.1.jar,gs://dataproc-templates/integration-testing/jars/mongo-spark-connector_2.12-2.4.0.jar"

                                cd java

                                bin/start.sh \
                                -- --template GCSTOMONGO \
                                --templateProperty gcs.mongodb.input.format=avro \
                                --templateProperty gcs.mongodb.input.location=gs://dataproc-templates/data/avro/empavro \
                                --templateProperty gcs.mongodb.output.uri="$ENV_TEST_MONGO_DB_URI" \
                                --templateProperty gcs.mongodb.output.database=demo \
                                --templateProperty gcs.mongodb.output.collection=test \
                                --templateProperty gcs.mongo.output.mode=overwrite

                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 10'){
            parallel{
                stage('PubSubLite To BigTable') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                cd java
                                bin/start.sh \
                                -- --template PUBSUBLITETOBIGTABLE \
                                --templateProperty pubsublite.input.project.id=$GCP_PROJECT \
                                --templateProperty pubsublite.input.subscription=projects/$GCP_PROJECT_NUMBER/locations/us-west1/subscriptions/pubsublitetobigtable-test-subscription2 \
                                --templateProperty pubsublite.checkpoint.location=dataproc-template-test1/checkpointlocation \
                                --templateProperty pubsublite.bigtable.output.project.id=$GCP_PROJECT \
                                --templateProperty pubsublite.bigtable.output.instance.id=$ENV_TEST_BIGTABLE_INSTANCE \
                                --templateProperty pubsublite.bigtable.output.table=bus-data
                            '''
                        }
                    }
                }
                stage('KAFKA TO BQ via Dstream'){
                     steps {
                         retry(count: stageRetryCount) {
                             sh '''

                                  export SKIP_BUILD=true
                                  cd java
                                  bin/start.sh \
                                  -- \
                                  --template KafkaToBQDstream \
                                  --templateProperty project.id=$GCP_PROJECT \
                                  --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                  --templateProperty kafka.topic=integration-test-kafka-bq-dstream \
                                  --templateProperty kafka.bq.dataset=kafkatobq \
                                  --templateProperty kafka.starting.offset=latest \
                                  --templateProperty kafka.bq.batch.interval=60000 \
                                  --templateProperty kafka.bq.consumer.group.id=testgroup \
                                  --templateProperty kafka.bq.stream.output.mode=append \
                                  --templateProperty kafka.bq.table=integration-test \
                                  --templateProperty kafka.bq.temp.gcs.bucket=dataproc-templates-kafkatobq \
                                  --templateProperty kafka.bq.await.termination.timeout=60000
                              '''
                         }
                     }
                }
                stage('KAFKA TO GCS via Dstream (bytes)'){
                     steps {
                         retry(count: stageRetryCount) {
                             sh '''

                                  export SKIP_BUILD=true
                                  cd java
                                  bin/start.sh \
                                  -- \
                                  --template KafkaToGCSDstream \
                                  --templateProperty project.id=$GCP_PROJECT \
                                  --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                  --templateProperty kafka.topic=integration-test-kafka-gcs-dstream \
                                  --templateProperty kafka.starting.offset=latest \
                                  --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS_DStream/bytes \
                                  --templateProperty kafka.gcs.batch.interval=60000 \
                                  --templateProperty kafka.gcs.consumer.group.id=testgroupgcs \
                                  --templateProperty kafka.gcs.write.mode=append \
                                  --templateProperty kafka.message.format=bytes \
                                  --templateProperty kafka.gcs.output.format=parquet \
                                  --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                             '''
                         }
                     }
                }
                stage('KAFKA TO GCS via Dstream (json)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''

                                 export SKIP_BUILD=true
                                 cd java
                                 bin/start.sh \
                                 -- \
                                 --template KafkaToGCSDstream \
                                 --templateProperty project.id=$GCP_PROJECT \
                                 --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                 --templateProperty kafka.topic=integration-test-json-kafka-gcs-dstream \
                                 --templateProperty kafka.starting.offset=latest \
                                 --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS_DStream/json \
                                 --templateProperty kafka.gcs.batch.interval=60000 \
                                 --templateProperty kafka.gcs.consumer.group.id=testgroupjson \
                                 --templateProperty kafka.gcs.write.mode=append \
                                 --templateProperty kafka.message.format=json \
                                 --templateProperty kafka.schema.url=gs://dataproc-templates/integration-testing/schema.json \
                                 --templateProperty kafka.gcs.output.format=parquet \
                                 --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                            '''
                        }
                    }
                }
            }
        }
    }
    post {
        always{
            script {
                if( env.GIT_BRANCH_LOCAL == 'main' ){
                    googlechatnotification url: DATAPROC_TELEPORT_WEBHOOK_URL,
    				message: 'Jenkins: ${JOB_NAME}\nBuild status is ${BUILD_STATUS}\nSee ${BUILD_URL}\n',
    				notifyFailure: 'true',
    				notifyAborted: 'true',
    				notifyUnstable: 'true',
    				notifyNotBuilt: 'true',
    				notifyBackToNormal: 'true'
                }
            }
            catchError {
                sh '''
                if [ $GIT_BRANCH_LOCAL != "main" ];then
                    gsutil rm -r $GCS_STAGING_LOCATION 2> /dev/null || true
                fi
                '''
            }
        }
    }
}