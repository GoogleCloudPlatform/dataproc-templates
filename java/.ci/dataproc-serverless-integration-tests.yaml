substitutions:
  _GCP_PROJECT: 'dataproc-templates' # Default to project running the build
  _REGION: 'us-central1' # Specify your Dataproc region
  _CLUSTER: 'your-dataproc-cluster-name' # Specify your Dataproc cluster
  _SUBNET: 'default' # Specify your subnet
  _JOB_TYPE: 'DEFAULT' # Set to 'CLUSTER' to enable cluster creation logic
  _GCS_STAGING_LOCATION_BASE: 'gs://dataproc-templates_cloudbuild/staging/dataproc-templates/${BRANCH_NAME}' # Base GCS path for staging
  _BRANCH_NAME_SUB: ${BRANCH_NAME} # Captures branch name from trigger, default is empty if not a branch trigger
  _SERVICE_ACCOUNT: 'dataproc-templates-cicd@dataproc-templates.iam.gserviceaccount.com'

  # Secrets (provide names of secrets stored in Secret Manager)
  _DATAPROC_TELEPORT_WEBHOOK_URL_SECRET: 'dataproc-teleport-webhook-url'
  _ENV_TEST_MONGO_DB_URI_SECRET: 'env-test-mongo-db-uri'
  _S3_ACCESS_KEY_ID_SECRET: 'aws-s3-access-key-id'
  _S3_SECRET_ACCESS_KEY_SECRET: 'aws-s3-secret-access-key'

  # Other environment variables from Jenkins
  _ENV_TEST_SPANNER_ID: 'spanner-integration-test'
  _ENV_TEST_HIVE_METASTORE_URIS: 'thrift://your-hive-metastore-uri:9083'
  _ENV_TEST_KAFKA_BOOTSTRAP_SERVERS: 'your-kafka-broker1:9092,your-kafka-broker2:9092'
  _ENV_TEST_CASSANDRA_HOST: 'your-cassandra-host'

availableSecrets:
  secretManager:
    - versionName: projects/$PROJECT_ID/secrets/TEST_JDBC_URL/versions/latest
      env: 'TEST_JDBC_URL'

steps:
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: build-and-upload
    env:
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'JAR_FILE=dataproc-templates-1.0-SNAPSHOT.jar'
    script: |
      #!/usr/bin/env bash
      apt install maven -y
      cd java
      mvn install -DskipTests
      echo "copying the build into staging location:" ${GCS_STAGING_LOCATION}   
      gsutil cp target/$JAR_FILE ${GCS_STAGING_LOCATION}/$JAR_FILE
        

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: gcs-to-bigquery-avro
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'SERVICE_ACCOUNT_NAME=${_SERVICE_ACCOUNT}'
    script: |
      #!/usr/bin/env bash
      echo "Running GCS TO BIGQUERY(avro) job"
      cd java
      bin/start.sh \
      -- --template GCSTOBIGQUERY \
      --templateProperty project.id=${_GCP_PROJECT} \
      --templateProperty gcs.bigquery.input.location="gs://dataproc-templates_cloudbuild/integration-testing/gcstobigquery/cities.avro" \
      --templateProperty gcs.bigquery.input.format=avro \
      --templateProperty gcs.bigquery.output.dataset="dataproc_templates" \
      --templateProperty gcs.bigquery.output.table="gcs_to_bq_avro" \
      --templateProperty gcs.bigquery.output.mode="Overwrite" \
      --templateProperty gcs.bigquery.temp.bucket.name=dataproc-templates_cloudbuild
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: gcs-to-bigquery-csv
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'SERVICE_ACCOUNT_NAME=${_SERVICE_ACCOUNT}'
    script: |
      #!/usr/bin/env bash
      echo "Running GCS TO BIGQUERY(csv) job"
      cd java
      bin/start.sh \
      -- --template GCSTOBIGQUERY \
      --templateProperty project.id=${_GCP_PROJECT} \
      --templateProperty gcs.bigquery.input.location="gs://dataproc-templates_cloudbuild/integration-testing/gcstobigquery/cities.csv" \
      --templateProperty gcs.bigquery.input.format=csv \
      --templateProperty gcs.bigquery.output.dataset="dataproc_templates" \
      --templateProperty gcs.bigquery.output.table="gcs_bq_cities" \
      --templateProperty gcs.bigquery.output.mode="Overwrite" \
      --templateProperty gcs.bigquery.temp.bucket.name=dataproc-templates_cloudbuild
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: gcs-to-spanner-googlesql
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'SERVICE_ACCOUNT_NAME=${_SERVICE_ACCOUNT}'
    script: |
      #!/usr/bin/env bash
      echo "Running GCS TO SPANNER GOOGLESQL DIALECT job"
      cd java
      bin/start.sh \
          -- --template GCSTOSPANNER \
          --templateProperty project.id=${_GCP_PROJECT} \
          --templateProperty gcs.spanner.input.format=avro \
          --templateProperty gcs.spanner.input.location=gs://dataproc-templates_cloudbuild/integration-testing/gcstospanner/emp.avro \
          --templateProperty gcs.spanner.output.instance=${_ENV_TEST_SPANNER_ID} \
          --templateProperty gcs.spanner.output.database=spark-ci-db   \
          --templateProperty gcs.spanner.output.table=badges \
          --templateProperty gcs.spanner.output.saveMode=Overwrite \
          --templateProperty gcs.spanner.output.primaryKey=empno
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: spanner-to-gcs-csv
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'SERVICE_ACCOUNT_NAME=${_SERVICE_ACCOUNT}'
    script: |
      #!/usr/bin/env bash
      echo "Running SPANNER TO GCS export in CSV"
      cd java
      
      bin/start.sh \
          -- --template SPANNERTOGCS \
          --templateProperty project.id=${_GCP_PROJECT} \
          --templateProperty spanner.gcs.input.spanner.id=${_ENV_TEST_SPANNER_ID} \
          --templateProperty spanner.gcs.input.database.id=spark-ci-db \
          --templateProperty spanner.gcs.input.table.id=movies \
          --templateProperty spanner.gcs.output.gcs.path=gs://dataproc-templates_cloudbuild/integration-testing/output/SPANNERTOGCS/csv \
          --templateProperty spanner.gcs.output.gcs.saveMode=Overwrite \
          --templateProperty spanner.gcs.output.gcs.format=csv
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: spanner-to-gcs-parquet
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'SERVICE_ACCOUNT_NAME=${_SERVICE_ACCOUNT}'
    script: |
      #!/usr/bin/env bash
      echo "Running SPANNER TO GCS export in parquet"
      cd java
      bin/start.sh \
          -- --template SPANNERTOGCS \
          --templateProperty project.id=${_GCP_PROJECT} \
          --templateProperty spanner.gcs.input.spanner.id=${_ENV_TEST_SPANNER_ID} \
          --templateProperty spanner.gcs.input.database.id=spark-ci-db \
          --templateProperty "spanner.gcs.input.table.id=(select id,name,year,rank from movies where rank < 5)" \
          --templateProperty spanner.gcs.output.gcs.path=gs://dataproc-templates_cloudbuild/integration-testing/output/SPANNERTOGCS/parquet \
          --templateProperty spanner.gcs.output.gcs.saveMode=Overwrite \
          --templateProperty spanner.gcs.output.gcs.format=parquet
    waitFor: ['build-and-upload']


  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: bigquery-to-gcs-avro
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'SERVICE_ACCOUNT_NAME=${_SERVICE_ACCOUNT}'
    script: |
      #!/usr/bin/env bash
      echo "Running BigQuery TO GCS avro"
      cd java
      bin/start.sh \
        -- --template=BIGQUERYTOGCS \
        --templateProperty project.id=${_GCP_PROJECT} \
        --templateProperty bigquery.gcs.input.table=${_GCP_PROJECT}:dataproc_templates.emp_table \
        --templateProperty bigquery.gcs.output.format=avro \
        --templateProperty bigquery.gcs.output.partition.col=emp_city \
        --templateProperty bigquery.gcs.output.mode=Overwrite \
        --templateProperty bigquery.gcs.output.location=gs://dataproc-templates_cloudbuild/integration-testing/output/BIGQUERYTOGCS/avro
    waitFor: ['build-and-upload']


  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: jdbc-to-bq
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}' # Consistent with other steps, start.sh might use it
      - 'REGION=${_REGION}' # Consistent with other steps
      - 'SKIP_BUILD=true' # Assumes this is the first template job, so a build is needed
      - 'SERVICE_ACCOUNT_NAME=${_SERVICE_ACCOUNT}'
      - 'JARS=gs://dataproc-templates_cloudbuild/integration-testing/jars/mysql-connector-java.jar'
      # TEST_JDBC_URL will be injected by secretEnv
    script: |
      #!/usr/bin/env bash
      echo "Running JDBC TO BQ job"
      cd java
      bin/start.sh \
      -- --template JDBCTOBIGQUERY \
      --templateProperty jdbctobq.bigquery.location=${_GCP_PROJECT}.dataproc_templates.jdbctobq \
      --templateProperty jdbctobq.jdbc.url="${TEST_JDBC_URL}" \
      --templateProperty jdbctobq.jdbc.driver.class.name=com.mysql.jdbc.Driver \
      --templateProperty jdbctobq.sql="select * from test.employee" \
      --templateProperty jdbctobq.write.mode=overwrite \
      --templateProperty jdbctobq.temp.gcs.bucket=dataproc-templates_cloudbuild/integration-testing/output/JDBCTOBIGQUERY
    secretEnv: ['TEST_JDBC_URL']
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: jdbc-to-spanner
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}' # Consistent with other steps, start.sh might use it
      - 'REGION=${_REGION}' # Consistent with other steps
      - 'SKIP_BUILD=true' # Assumes this is the first template job, so a build is needed
      - 'SERVICE_ACCOUNT_NAME=${_SERVICE_ACCOUNT}'
      - 'JARS=gs://dataproc-templates_cloudbuild/integration-testing/jars/mysql-connector-java.jar'
      # TEST_JDBC_URL will be injected by secretEnv
    script: |
      #!/usr/bin/env bash
      echo "Running JDBC TO Spanner job"
      cd java
      bin/start.sh \
        -- --template JDBCTOSPANNER \
        --templateProperty project.id=$GCP_PROJECT \
        --templateProperty jdbctospanner.jdbc.url="${TEST_JDBC_URL}" \
        --templateProperty jdbctospanner.jdbc.driver.class.name=com.mysql.jdbc.Driver \
        --templateProperty jdbctospanner.sql="select * from test.employee" \
        --templateProperty jdbctospanner.output.instance=${_ENV_TEST_SPANNER_ID} \
        --templateProperty jdbctospanner.output.database=spark-ci-db \
        --templateProperty jdbctospanner.output.table=employee \
        --templateProperty jdbctospanner.output.saveMode=Overwrite \
        --templateProperty jdbctospanner.output.primaryKey='empno'
    secretEnv: ['TEST_JDBC_URL']
    waitFor: ['build-and-upload']

options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'E2_HIGHCPU_8'
  automapSubstitutions: true
