# Copyright (C) 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.

# Marking these as subsitutions during development. During deployment these should go into
# the build trigger
substitutions:
  _GCP_PROJECT: 'dataproc-templates'
  _REGION: 'us-central1'
  _GCS_STAGING_LOCATION_BASE: 'gs://dataproc-templates_cloudbuild/staging/dataproc-templates-python'
  _GCS_DEPS_BUCKET: 'gs://dataproc-templates-python-deps'
  _PYTHON_EGG_FILE: 'dataproc_templates_distribution.egg'
  _ENV_TEST_HIVE_METASTORE_URIS: 'thrift://10.115.64.27:9083'

# Following not used at the present
#  _GCS_STAGING_LOCATION_BASE: 'gs://dataproc-templates_cloudbuild/staging/dataproc-templates/${BRANCH_NAME}'
#  _SERVICE_ACCOUNT: 'dataproc-templates-cicd@dataproc-templates.iam.gserviceaccount.com'
#  _ENV_TEST_CASSANDRA_HOST: '10.128.0.19'
#  _ENV_TEST_MONGO_DB_URI: "mongodb://10.128.0.8:27017"
#  _ENV_TEST_BIGTABLE_INSTANCE: 'bt-int-test'
#  _ELASTIC_INPUT_NODE: '10.128.0.12'
#  _ELASTIC_USER: 'elastic'

availableSecrets:
  secretManager:
    - versionName: projects/$PROJECT_ID/secrets/TEST_JDBC_URL/versions/latest
      env: 'TEST_JDBC_URL'
#    - versionName: projects/$PROJECT_ID/secrets/S3_ACCESS_KEY_ID/versions/latest
#      env: 'S3_ACCESS_KEY_ID'
#    - versionName: projects/$PROJECT_ID/secrets/S3_SECRET_ACCESS_KEY/versions/latest
#      env: 'S3_SECRET_ACCESS_KEY'
#    - versionName: projects/$PROJECT_ID/secrets/jdbctobqconn/versions/latest
#      env: 'JDBCTOBQCONN'
#    - versionName: projects/$PROJECT_ID/secrets/ELASTIC_PASSWORD/versions/latest
#      env: 'ELASTIC_PASSWORD'

# Defining bucket names that will be needed in dataproc-templates project - right now define one for one bucket names
# goog cannot be a prefix for bucket names, so using moff (for Moffet Park) instead.
# In the future fix it with fewer (one?) bucket name for the build.
# --hive.bigquery.temp.bucket.name="python-hive-to-bq-temp" - "moff-dataproc-templates-python-hive-temp"
# --gcs.bigquery.temp.bucket.name="python-dataproc-templates-temp-bq" - "moff-dataproc-templates-python-bq-temp"
# --mongo.bq.temp.bucket.name="dataproc-templates/integration-testing/mongotobigquery" - "moff-dataproc-templates/integration-testing/mongotobigquery"
# --jdbc.bigquery.temp.bucket.name="dataproc-templates/integration-testing/jdbctobq" - "moff-dataproc-templates/integration-testing/mongotobigquery"
# --jdbc.bigquery.temp.bucket.name="dataproc-templates/integration-testing/jdbctobq" - "moff-dataproc-templates/integration-testing/mongotobigquery"
# --s3.bq.temp.bucket.name="python-dataproc-templates-temp" - "moff-dataproc-templates-python-temp"
# python-dataproc-templates = "moff-python-dataproc-templates"

steps:
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: build-and-upload
    script: |
      #!/usr/bin/env bash
      apt-get update && apt-get install -y python3-pip python3-venv
      cd python
      python3 -m venv env
      source env/bin/activate
      pip install -r requirements.txt
      python3 setup.py bdist_egg --output=dist/${_PYTHON_EGG_FILE}
      gsutil cp dist/${_PYTHON_EGG_FILE} ${_GCS_STAGING_LOCATION_BASE}/${_PYTHON_EGG_FILE}

      # Create cities.json for GCS to Bigtable test
      cat > /tmp/cities.json << EOF
      {
          "table": { "name": "cities" },
          "rowkey": "key",
          "columns": {
              "key": {"cf": "rowkey", "col": "key", "type": "string"},
              "LatD": {"cf": "lat", "col": "LatD", "type": "string"},
              "LatM": {"cf": "lat", "col": "LatM", "type": "string"},
              "LatS": {"cf": "lat", "col": "LatS", "type": "string"},
              "NS": {"cf": "lat", "col": "NS", "type": "string"},
              "LonD": {"cf": "lon", "col": "LonD", "type": "string"},
              "LonM": {"cf": "lon", "col": "LonM", "type": "string"},
              "LonS": {"cf": "lon", "col": "LonS", "type": "string"},
              "EW": {"cf": "lon", "col": "EW", "type": "string"},
              "City": {"cf": "place", "col": "City", "type": "string"},
              "State": {"cf": "place", "col": "State", "type": "string"}
          }
      }
      EOF
      gsutil cp /tmp/cities.json gs://dataproc-templates_cloudbuild/integration-testing/bigtable/cities.json
      ls dist

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: gcs-to-bigquery
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'GCS_DEPS_BUCKET=${_GCS_DEPS_BUCKET}'
    script: |
      #!/usr/bin/env bash
      cd python
      ./bin/start.sh  -- \
      --template=GCSTOBIGQUERY \
      --gcs.bigquery.input.format="parquet" \
      --gcs.bigquery.input.location="gs://dataproc-templates_cloudbuild/integration-testing-python/GCSTOBIGQUERY/input/" \
      --gcs.bigquery.output.dataset="dataproc_templates_python" \
      --gcs.bigquery.output.table="gcs_bq_table" \
      --gcs.bigquery.output.mode="overwrite" \
      --gcs.bigquery.temp.bucket.name=${GCS_DEPS_BUCKET}
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: gcs-to-gcs
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'GCS_DEPS_BUCKET=${_GCS_DEPS_BUCKET}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
    script: |
      #!/usr/bin/env bash
      cd python
      ./bin/start.sh \
      -- --template=GCSTOGCS \
      --gcs.to.gcs.input.location="gs://dataproc-templates_cloudbuild/integration-testing-python/GCSTOGCS/input/emp.avro" \
      --gcs.to.gcs.input.format="avro" \
      --gcs.to.gcs.output.location="gs://dataproc-templates_cloudbuild/integration-testing-python/GCSTOGCS/output/" \
      --gcs.to.gcs.output.format="csv" \
      --gcs.to.gcs.output.mode="overwrite" \
      --gcs.to.gcs.temp.view.name="dataset" \
      --gcs.to.gcs.sql.query="select * from dataset where sal>1500"
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: gcs-to-jdbc
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'GCS_DEPS_BUCKET=${_GCS_DEPS_BUCKET}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'JARS=gs://dataproc-templates_cloudbuild/integration-testing/jars/mysql-connector-java.jar'
    script: |
      #!/usr/bin/env bash
      cd python
      ./bin/start.sh  -- \
      --template=GCSTOJDBC \
      --gcs.jdbc.input.format="csv" \
      --gcs.jdbc.input.location="gs://dataproc-templates_cloudbuild/integration-testing-python/GCSTOJDBC/input/sample.csv" \
      --gcs.jdbc.output.table="test.demo" \
      --gcs.jdbc.output.mode="overwrite" \
      --gcs.jdbc.output.driver="com.mysql.cj.jdbc.Driver" \
      --gcs.jdbc.batch.size=1000 \
      --gcs.jdbc.output.url="$TEST_JDBC_URL"
    secretEnv: ['TEST_JDBC_URL']
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: hive-to-bigquery
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'GCS_DEPS_BUCKET=${_GCS_DEPS_BUCKET}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'ENV_TEST_HIVE_METASTORE_URIS=${_ENV_TEST_HIVE_METASTORE_URIS}'
    script: |
      #!/usr/bin/env bash
      cd python
      ./bin/start.sh \
      --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
      -- --template=HIVETOBIGQUERY \
      --hive.bigquery.input.database="test_db" \
      --hive.bigquery.input.table="employee" \
      --hive.bigquery.output.dataset="dataproc_templates_python" \
      --hive.bigquery.output.table="hive_to_bigquery" \
      --hive.bigquery.output.mode="overwrite" \
      --hive.bigquery.temp.bucket.name=${GCS_DEPS_BUCKET}
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: hive-to-gcs
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'GCS_DEPS_BUCKET=${_GCS_DEPS_BUCKET}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'ENV_TEST_HIVE_METASTORE_URIS=${_ENV_TEST_HIVE_METASTORE_URIS}'
    script: |
      #!/usr/bin/env bash
      cd python
      ./bin/start.sh \
      --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS -- \
      --template=HIVETOGCS \
      --hive.gcs.input.database="test_db" \
      --hive.gcs.input.table="employee" \
      --hive.gcs.output.location="gs://dataproc-templates_cloudbuild/integration-testing-python/HIVETOGCS/output" \
      --hive.gcs.output.format="csv" \
      --hive.gcs.output.mode="overwrite"
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: jdbc-to-bigquery
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'GCS_DEPS_BUCKET=${_GCS_DEPS_BUCKET}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'JARS=gs://dataproc-templates_cloudbuild/integration-testing/jars/mysql-connector-java.jar'
    script: |
      #!/usr/bin/env bash
      cd python
      ./bin/start.sh \
      -- --template=JDBCTOBIGQUERY \
      --jdbc.bigquery.input.url="$TEST_JDBC_URL" \
      --jdbc.bigquery.input.driver="com.mysql.cj.jdbc.Driver" \
      --jdbc.bigquery.input.table="test.demo" \
      --jdbc.bigquery.output.mode="overwrite" \
      --jdbc.bigquery.output.dataset="dataproc_templates_python" \
      --jdbc.bigquery.output.table="jdbc_to_bigquery" \
      --jdbc.bigquery.temp.bucket.name=${GCS_DEPS_BUCKET}
    secretEnv: ['TEST_JDBC_URL']
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: jdbc-to-gcs
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'GCS_DEPS_BUCKET=${_GCS_DEPS_BUCKET}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'JARS=gs://dataproc-templates_cloudbuild/integration-testing/jars/mysql-connector-java.jar'
    script: |
      #!/usr/bin/env bash
      cd python
      ./bin/start.sh \
      -- --template=JDBCTOGCS \
      --jdbctogcs.input.url="$TEST_JDBC_URL" \
      --jdbctogcs.input.driver="com.mysql.cj.jdbc.Driver" \
      --jdbctogcs.input.table="test.demo" \
      --jdbctogcs.output.location="gs://dataproc-templates_cloudbuild/integration-testing-python/JDBCTOGCS/output" \
      --jdbctogcs.output.mode="overwrite" \
      --jdbctogcs.output.format="csv"
    secretEnv: ['TEST_JDBC_URL']
    waitFor: ['build-and-upload']

  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: jdbc-to-jdbc
    env:
      - 'GCP_PROJECT=${_GCP_PROJECT}'
      - 'GCS_STAGING_LOCATION=${_GCS_STAGING_LOCATION_BASE}'
      - 'GCS_DEPS_BUCKET=${_GCS_DEPS_BUCKET}'
      - 'REGION=${_REGION}'
      - 'SKIP_BUILD=true'
      - 'JARS=gs://dataproc-templates_cloudbuild/integration-testing/jars/mysql-connector-java.jar'
    script: |
      #!/usr/bin/env bash
      cd python
      ./bin/start.sh -- \
      --template=JDBCTOJDBC \
      --jdbctojdbc.input.url="$TEST_JDBC_URL" \
      --jdbctojdbc.input.driver="com.mysql.cj.jdbc.Driver" \
      --jdbctojdbc.input.table="test.demo" \
      --jdbctojdbc.output.url="$TEST_JDBC_URL" \
      --jdbctojdbc.output.driver="com.mysql.cj.jdbc.Driver" \
      --jdbctojdbc.output.table="demo_out" \
      --jdbctojdbc.output.mode="overwrite" \
      --jdbctojdbc.output.batch.size="1000"
    secretEnv: ['TEST_JDBC_URL']
    waitFor: ['build-and-upload']
options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'E2_HIGHCPU_8'
  automapSubstitutions: true
timeout: 3600s
