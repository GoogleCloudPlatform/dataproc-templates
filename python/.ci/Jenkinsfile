def stageRetryCount = 3

pipeline {
    
    agent any
    
    environment {
        DATAPROC_TELEPORT_WEBHOOK_URL = credentials('dataproc-teleport-webhook-url')

        TEST_JDBC_URL = credentials('env-test-jdbc-url')

        GIT_BRANCH_LOCAL = sh (
            script: "echo $branchName | sed -e 's|origin/||g' | sed -e 's|^null\$|main|'",  // Remove "origin/" and set the default branch to main
            returnStdout: true
        ).trim()
    }
    
    stages {
        stage('Checkout') {
            steps{
                git branch: "${GIT_BRANCH_LOCAL}", changelog: false, poll: false, url: 'https://github.com/GoogleCloudPlatform/dataproc-templates/'    
            }
        }
        stage('Clean Resources'){
            steps {
                catchError {
                    sh '''
                        gcloud pubsub lite-topics publish psltobt \
                            --location=us-west1 \
                            --message=\'\'\'
                            { 
                                "rowkey":"rk1",
                                "columns": [
                                    {
                                        "columnfamily":"place",
                                        "columnname":"city",
                                        "columnvalue":"Bangalore"
                                    },
                                    {
                                        "columnfamily":"place",
                                        "columnname":"state",
                                        "columnvalue":"Karnataka"
                                    },
                                    {
                                        "columnfamily":"date",
                                        "columnname":"year",
                                        "columnvalue":"2023"
                                    }
                                ] 
                            }
                            \'\'\'
                    '''
                }
            }
        }
        stage('Cluster Creation'){
            when {
                // Run this stage only if JOB_TYPE is not set to CLUSTER
                expression { env.JOB_TYPE == "CLUSTER" }
            }
            steps{
                sh '''
                if gcloud dataproc clusters list --region=$REGION --project=$GCP_PROJECT | grep -q $CLUSTER; then
                    echo "Cluster $CLUSTER already exists."
                else
                    echo "Cluster $CLUSTER does not exist. Creating now..."
                    gcloud dataproc clusters create $CLUSTER \
                    --region $REGION \
                    --subnet $SUBNET \
                    --no-address \
                    --master-machine-type n1-standard-2 \
                    --master-boot-disk-size 500 \
                    --num-workers 2 \
                    --worker-machine-type n1-standard-2 \
                    --worker-boot-disk-size 500 \
                    --image-version 2.1-debian11 \
                    --optional-components ZOOKEEPER \
                    --max-idle 1800s \
                    --project $GCP_PROJECT
                fi
                '''
            }
        }
        stage('Build'){
            steps {
                    catchError {
                        sh '''
                            python3.8 -m pip install --user virtualenv    

                            python3.8 -m venv env
                            source env/bin/activate
                            
                            export PACKAGE_EGG_FILE=dist/dataproc_templates_distribution.egg

                            cd python
                            python setup.py bdist_egg --output=$PACKAGE_EGG_FILE
                        '''
                    }
            }
        }
        stage('Parallel Execution 1'){
            parallel{ 
                stage('Hive TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                    --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                    -- --template=HIVETOGCS \
                                    --hive.gcs.input.database="default" \
                                    --hive.gcs.input.table="employee" \
                                    --hive.gcs.output.location="gs://python-dataproc-templates/hive-gcs-output" \
                                    --hive.gcs.output.format="csv" \
                                    --hive.gcs.output.mode="overwrite"
                            '''
                        }
                    }
                }  
                stage('Hive TO BigQuery') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                    --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                    -- --template=HIVETOBIGQUERY \
                                    --hive.bigquery.input.database="default" \
                                    --hive.bigquery.input.table="employee" \
                                    --hive.bigquery.output.dataset="hive_to_bq_py" \
                                    --hive.bigquery.output.table="employee" \
                                    --hive.bigquery.output.mode="overwrite" \
                                    --hive.bigquery.temp.bucket.name="python-hive-to-bq-temp"
                            '''
                        }
                    }
                } 
                stage('GCS To JDBC') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                export SKIP_BUILD=true
                                
                                cd python
                                
                                ./bin/start.sh \
                                -- --template=GCSTOJDBC \
                                --gcs.jdbc.input.format="csv" \
                                --gcs.jdbc.input.location="gs://python-dataproc-templates-temp/input/csv/sample.csv" \
                                --gcs.jdbc.output.table="demo" \
                                --gcs.jdbc.output.mode="overwrite" \
                                --gcs.jdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                --gcs.jdbc.batch.size=1000 \
                                --gcs.jdbc.output.url="$TEST_JDBC_URL"
                            '''
                        }
                    }
                }     
            }
        }
        stage('Parallel Execution 2'){
            parallel{
                stage('TEXT TO BIGQUERY') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh  -- \
                                --template=TEXTTOBIGQUERY \
                                --text.bigquery.input.compression="gzip" \
                                --text.bigquery.input.delimiter="," \
                                --text.bigquery.input.location="gs://python-dataproc-templates/text-bq-input/" \
                                --text.bigquery.output.dataset="python_dataproc_templates" \
                                --text.bigquery.output.table="python_text_bq_table" \
                                --text.bigquery.output.mode=overwrite \
                                --text.bigquery.temp.bucket.name="python-dataproc-templates-temp-bq"
                            '''
                        }
                    }
                }
                stage('JDBC To JDBC') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://datproc_template_nk/jars/mysql-connector-java-8.0.29.jar,gs://datproc_template_nk/jars/postgresql-42.2.6.jar,gs://datproc_template_nk/jars/mssql-jdbc-6.4.0.jre8.jar"
                                export SKIP_BUILD=true
                                
                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOJDBC \
                                --jdbctojdbc.input.url="$TEST_JDBC_URL" \
                                --jdbctojdbc.input.driver="com.mysql.cj.jdbc.Driver" \
                                --jdbctojdbc.input.table="demo" \
                                --jdbctojdbc.output.url="$TEST_JDBC_URL" \
                                --jdbctojdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                --jdbctojdbc.output.table="demo_out" \
                                --jdbctojdbc.output.mode="overwrite" \
                                --jdbctojdbc.output.batch.size="1000"
                            '''
                        }
                    }
                } 
                stage('JDBC To JDBC with secrets') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env
                                
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://datproc_template_nk/jars/mysql-connector-java-8.0.29.jar,gs://datproc_template_nk/jars/postgresql-42.2.6.jar,gs://datproc_template_nk/jars/mssql-jdbc-6.4.0.jre8.jar"
                                export SKIP_BUILD=true
                                
                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOJDBC \
                                --jdbctojdbc.input.url.secret="jdbctobqconn" \
                                --jdbctojdbc.input.driver="com.mysql.cj.jdbc.Driver" \
                                --jdbctojdbc.input.table="demo" \
                                --jdbctojdbc.output.url.secret="jdbctobqconn" \
                                --jdbctojdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                --jdbctojdbc.output.table="demo_out" \
                                --jdbctojdbc.output.mode="overwrite" \
                                --jdbctojdbc.output.batch.size="1000"
                            '''
                        }
                    }
                }
                stage('GCS To MONGO') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://python-dataproc-templates-temp/jars/mongo-java-driver-3.9.1.jar,gs://python-dataproc-templates-temp/jars/mongo-spark-connector_2.12-2.4.0.jar"
                                export SKIP_BUILD=true
                                
                                cd python
                                
                                ./bin/start.sh \
                                -- --template=GCSTOMONGO \
                                --gcs.mongo.input.format="avro" \
                                --gcs.mongo.input.location="gs://dataproc-templates/data/avro/empavro" \
                                --gcs.mongo.output.uri="$ENV_TEST_MONGO_DB_URI" \
                                --gcs.mongo.output.database="demo" \
                                --gcs.mongo.output.collection="analysis" \
                                --gcs.mongo.output.mode="overwrite"
                            '''
                        }
                    }
                } 
            }
        }
        stage('Parallel Execution 3'){
            parallel{
                stage('GCS TO BIGQUERY') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh  -- \
                                --template=GCSTOBIGQUERY \
                                --gcs.bigquery.input.format="parquet" \
                                --gcs.bigquery.input.location="gs://python-dataproc-templates/gcs-bq-input/" \
                                --gcs.bigquery.output.dataset="python_dataproc_templates" \
                                --gcs.bigquery.output.table="python_gcs_bq_table" \
                                --gcs.bigquery.output.mode=overwrite \
                                --gcs.bigquery.temp.bucket.name="python-dataproc-templates-temp-bq"
                            '''
                        }
                    }
                }
                stage('BIGQUERY TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=BIGQUERYTOGCS \
                                    --bigquery.gcs.input.table=python_dataproc_templates.python_gcs_bq_table \
                                    --bigquery.gcs.output.format=csv \
                                    --bigquery.gcs.output.mode=overwrite \
                                    --bigquery.gcs.output.location="gs://python-dataproc-tmplates-bq-to-gcs/csv/"
                            '''
                        }
                    }
                }

                stage('Cassandra TO BigQuery') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://spark-cassandra-connector/spark-cassandra-connector-assembly_2.12-3.2.0.jar,gs://spark-cassandra-connector/jnr-posix-3.1.8.jar,gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=CASSANDRATOBQ \
                                --cassandratobq.input.keyspace=testkeyspace \
                                --cassandratobq.input.table=emp \
                                --cassandratobq.input.host=$ENV_TEST_CASSANDRA_HOST \
                                --cassandratobq.bigquery.location=dataproc_templates.py_cassandra_bq \
                                --cassandratobq.temp.gcs.location=python-dataproc-templates-temp-bq \
                                --cassandratobq.output.mode=overwrite
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 4'){
            parallel{
                stage('MONGO TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://python-dataproc-templates-temp/jars/mongo-java-driver-3.9.1.jar,gs://python-dataproc-templates-temp/jars/mongo-spark-connector_2.12-2.4.0.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=MONGOTOGCS \
                                --mongo.gcs.output.format="avro" \
                                --mongo.gcs.output.location="gs://python-dataproc-templates-temp/integration-testing/output/MONGOTOGCS" \
                                --mongo.gcs.output.mode="overwrite" \
                                --mongo.gcs.input.uri="$ENV_TEST_MONGO_DB_URI" \
                                --mongo.gcs.input.database="demo" \
                                --mongo.gcs.input.collection="analysis"
                            '''
                        }
                    }
                }
                stage('MONGO TO BIGQUERY') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://python-dataproc-templates-temp/jars/mongo-java-driver-3.9.1.jar,gs://python-dataproc-templates-temp/jars/mongo-spark-connector_2.12-2.4.0.jar,gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=MONGOTOBIGQUERY \
                                --mongo.bq.input.uri="$ENV_TEST_MONGO_DB_URI" \
                                --mongo.bq.input.database="demo" \
                                --mongo.bq.input.collection="dummyusers" \
                                --mongo.bq.output.dataset="dataproc_templates" \
                                --mongo.bq.output.table="mongotobq" \
                                --mongo.bq.output.mode="overwrite" \
                                --mongo.bq.temp.bucket.name="dataproc-templates/integration-testing/mongotobigquery"
                            '''
                        }
                    }
                }
                stage('JDBC TO BIGQUERY') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar,gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOBIGQUERY \
                                --jdbc.bigquery.input.url="$TEST_JDBC_URL" \
                                --jdbc.bigquery.input.driver="com.mysql.jdbc.Driver" \
                                --jdbc.bigquery.input.table="test.demo" \
                                --jdbc.bigquery.output.mode="overwrite" \
                                --jdbc.bigquery.output.dataset="dataproc_templates" \
                                --jdbc.bigquery.output.table="jdbctobq" \
                                --jdbc.bigquery.temp.bucket.name="dataproc-templates/integration-testing/jdbctobq"
                            '''
                        }
                    }
                }
                stage('JDBC TO BIGQUERY with secret') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar,gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOBIGQUERY \
                                --jdbc.bigquery.input.url.secret="jdbctobqconn" \
                                --jdbc.bigquery.input.driver="com.mysql.jdbc.Driver" \
                                --jdbc.bigquery.input.table="test.demo" \
                                --jdbc.bigquery.output.mode="overwrite" \
                                --jdbc.bigquery.output.dataset="dataproc_templates" \
                                --jdbc.bigquery.output.table="jdbctobq" \
                                --jdbc.bigquery.temp.bucket.name="dataproc-templates/integration-testing/jdbctobq"
                            '''
                        }
                    }
                }
                stage('Cassandra TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh'''

                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://spark-cassandra-connector/spark-cassandra-connector-assembly_2.12-3.2.0.jar,gs://spark-cassandra-connector/jnr-posix-3.1.8.jar"
                                export SKIP_BUILD=true
                                
                                cd python

                                bin/start.sh \
                                -- --template CASSANDRATOGCS \
                                --cassandratogcs.input.keyspace=testkeyspace \
                                --cassandratogcs.input.table=emp \
                                --cassandratogcs.input.host=$ENV_TEST_CASSANDRA_HOST \
                                --cassandratogcs.output.format=csv \
                                --cassandratogcs.output.path=gs://dataproc-templates/integration-testing/cassandratogcs/output \
                                --cassandratogcs.output.savemode=overwrite
                            '''
                        }
                    }
                }        
            }
        }
        stage('Parallel Execution 5'){
            parallel{
                stage('JDBC TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOGCS \
                                --jdbctogcs.input.url="$TEST_JDBC_URL" \
                                --jdbctogcs.input.driver="com.mysql.jdbc.Driver" \
                                --jdbctogcs.input.table="test.demo" \
                                --jdbctogcs.output.location="gs://dataproc-templates/integration-testing/jdbctogcs/output-python" \
                                --jdbctogcs.output.mode="overwrite" \
                                --jdbctogcs.output.format="csv"
                            '''
                        }
                    }
                }
                stage('JDBC TO GCS with secret') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOGCS \
                                --jdbctogcs.input.url.secret="jdbctobqconn" \
                                --jdbctogcs.input.driver="com.mysql.jdbc.Driver" \
                                --jdbctogcs.input.table="test.demo" \
                                --jdbctogcs.output.location="gs://dataproc-templates/integration-testing/jdbctogcs/output-python" \
                                --jdbctogcs.output.mode="overwrite" \
                                --jdbctogcs.output.format="csv"
                            '''
                        }
                    }
                }
                stage('GCS TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=GCSTOGCS \
                                --gcs.to.gcs.input.location="gs://dataproc-templates/data/avro/empavro" \
                                --gcs.to.gcs.input.format="avro" \
                                --gcs.to.gcs.output.location="gs://python-dataproc-templates-temp/integration-testing/output/GCSTOGCS" \
                                --gcs.to.gcs.output.format="csv" \
                                --gcs.to.gcs.output.mode="overwrite" \
                                --gcs.to.gcs.temp.view.name="dataset" \
                                --gcs.to.gcs.sql.query="select * from dataset where sal>1500"
                            '''
                        }
                    }
                }
                stage('PUB/SUB LITE TO BIGTABLE') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://spark-lib/pubsublite/pubsublite-spark-sql-streaming-LATEST-with-dependencies.jar"
                                export SKIP_BUILD=true

                                cd python

                                ./bin/start.sh \
                                -- --template=PUBSUBLITETOBIGTABLE \
                                --pubsublite.bigtable.subscription.path=projects/$GCP_PROJECT/locations/$REGION/subscriptions/psltobt-sub \
                                --pubsublite.bigtable.streaming.checkpoint.location="gs://dataproc-templates/integration-testing/psltobt/checkpoint" \
                                --pubsublite.bigtable.output.project=$GCP_PROJECT \
                                --pubsublite.bigtable.output.instance=$ENV_TEST_BIGTABLE_INSTANCE \
                                --pubsublite.bigtable.output.table="output_table" \
                                --pubsublite.bigtable.streaming.timeout=20
                            '''
                        }
                    }
                }
            }
        }  
        stage('Parallel Execution 6'){
            parallel{
                stage('Hbase TO GCS (Manual)') {    
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE != "CLUSTER" }
                    }
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://deps-dataproc-template/hbase-shaded-mapreduce-2.4.12.jar,gs://deps-dataproc-template/hbase-client-2.4.12.jar,gs://deps-dataproc-template/htrace-core4-4.2.0-incubating.jar,file:///usr/lib/spark/external/hbase-spark-protocol-shaded.jar,file:///usr/lib/spark/external/hbase-spark.jar"
                                export SKIP_BUILD=true
                                
                                cd python
                                
                                ./bin/start.sh \
                                    --container-image="gcr.io/$GCP_PROJECT/hbase-to-gcs:1.0.1" \
                                    --properties="spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/" \
                                    -- --template=HBASETOGCS \
                                    --hbase.gcs.output.location="gs://python-dataproc-templates/hbase-gcs-output" \
                                    --hbase.gcs.output.format="csv" \
                                    --hbase.gcs.output.mode="overwrite" \
                                    --hbase.gcs.catalog.json=\'\'\'{
                                            "table":{"namespace":"default","name":"my_table"},
                                            "rowkey":"key",
                                            "columns":{
                                            "key":{"cf":"rowkey", "col":"key", "type":"string"},
                                            "name":{"cf":"cf", "col":"name", "type":"string"}
                                            }
                                            }\'\'\'
                            '''
                        }
                    }
                }
                    
                stage('Hbase TO GCS (Automated)') {
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE != "CLUSTER" }
                    }
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                whoami
                                sudo su
                                source env/bin/activate

                                gsutil cp gs://python-dataproc-templates/surjitsh/hbase-site.xml .
                                
                                export GCS_STAGING_LOCATION=gs://python-dataproc-templates-temp
                                export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                                export IMAGE_NAME_VERSION=hbase-to-gcs:1.0.1
                                export HBASE_SITE_PATH=../hbase-site.xml
                                export IMAGE=gcr.io/${GCP_PROJECT}/${IMAGE_NAME_VERSION}
                                export SKIP_IMAGE_BUILD=TRUE

                                cd python

                                ./bin/start.sh \
                                    --container-image=$IMAGE \
                                    --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'  \
                                    -- --template=HBASETOGCS \
                                    --hbase.gcs.output.location=gs://python-dataproc-templates/hbase-gcs-output \
                                    --hbase.gcs.output.format=csv \
                                    --hbase.gcs.output.mode=overwrite \
                                    --hbase.gcs.catalog.json=$CATALOG
                            '''
                        }
                    }
                }
                stage('GCS TO Bigtable') {
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE != "CLUSTER" }
                    }
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true
                                export JARS="gs://python-dataproc-templates-temp/jars/bigtable-hbase-2.x-shaded-2.3.0.jar,gs://deps-dataproc-template/hbase-shaded-mapreduce-2.4.12.jar,gs://deps-dataproc-template/hbase-client-2.4.12.jar,file:///usr/lib/spark/external/hbase-spark-protocol-shaded.jar,file:///usr/lib/spark/external/hbase-spark.jar"


                                cd python
                                
                                ./bin/start.sh \
                                --container-image="gcr.io/$GCP_PROJECT/bt-templates-test:1.0.0" \
                                --properties="spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/" \
                                -- --template=GCSTOBIGTABLE \
                                --gcs.bigtable.input.format="csv" \
                                --gcs.bigtable.input.location="gs://dataproc-templates/data/csv/cities.csv" \
                                --gcs.bigtable.hbase.catalog.json=\'\'\'{
                                    "table":{"namespace":"default","name":"cities"},
                                    "rowkey":"key",
                                    "columns":{
                                        "key":{"cf":"rowkey", "col":"key", "type":"string"},
                                        "LatD":{"cf":"lat", "col":"LatD", "type":"string"},
                                        "LatM":{"cf":"lat", "col":"LatM", "type":"string"},
                                        "LatS":{"cf":"lat", "col":"LatS", "type":"string"},
                                        "NS":{"cf":"lat", "col":"NS", "type":"string"},
                                        "LonD":{"cf":"lon", "col":"LonD", "type":"string"},
                                        "LonM":{"cf":"lon", "col":"LonM", "type":"string"},
                                        "LonS":{"cf":"lon", "col":"LonS", "type":"string"},
                                        "EW":{"cf":"lon", "col":"EW", "type":"string"},
                                        "City":{"cf":"place", "col":"City", "type":"string"},
                                        "State":{"cf":"place", "col":"State", "type":"string"}
                                    }
                                }\'\'\'
                            '''
                        }
                    }
                }
            }
        }
    }
    post {
        always{
            script {
                if( env.GIT_BRANCH_LOCAL == 'main' ){
                    googlechatnotification url: DATAPROC_TELEPORT_WEBHOOK_URL,
                    message: 'Jenkins: ${JOB_NAME}\nBuild status is ${BUILD_STATUS}\nSee ${BUILD_URL}\n',
                    notifyFailure: 'true',
                    notifyAborted: 'true',
                    notifyUnstable: 'true',
                    notifyNotBuilt: 'true',
                    notifyBackToNormal: 'true'
                }
            }
        }
    }
}
