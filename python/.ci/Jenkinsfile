def stageRetryCount = 3

pipeline {
    
    agent any
    
    environment {
        DATAPROC_TELEPORT_WEBHOOK_URL = credentials('dataproc-teleport-webhook-url')

        TEST_JDBC_URL = credentials('env-test-jdbc-url')

        GIT_BRANCH_LOCAL = sh (
            script: "echo $branchName | sed -e 's|origin/||g' | sed -e 's|^null\$|main|'",  // Remove "origin/" and set the default branch to main
            returnStdout: true
        ).trim()
    }
    
    stages {
        stage('Checkout') {
            steps{
                git branch: "${GIT_BRANCH_LOCAL}", changelog: false, poll: false, url: 'https://github.com/GoogleCloudPlatform/dataproc-templates/'    
            }
        }
        stage('Cluster Creation'){
            when {
                // Run this stage only if JOB_TYPE is not set to CLUSTER
                expression { env.JOB_TYPE == "CLUSTER" }
            }
            steps{
                sh '''
                if gcloud dataproc clusters list --region=$REGION --project=$GCP_PROJECT | grep -q $CLUSTER; then
                    echo "Cluster $CLUSTER already exists."
                else
                    echo "Cluster $CLUSTER does not exist. Creating now..."
                    gcloud dataproc clusters create $CLUSTER \
                    --region $REGION \
                    --properties dataproc:pip.packages='google-cloud-secret-manager==2.20.0' \
                    --subnet $SUBNET \
                    --no-address \
                    --master-machine-type n1-standard-2 \
                    --master-boot-disk-size 500 \
                    --num-workers 2 \
                    --worker-machine-type n1-standard-2 \
                    --worker-boot-disk-size 500 \
                    --image-version 2.1-debian11 \
                    --optional-components ZOOKEEPER \
                    --max-idle 1800s \
                    --project $GCP_PROJECT
                fi
                '''
            }
        }
        stage('Build'){
            steps {
                    catchError {
                        sh '''
                            python3.8 -m pip install --user virtualenv    

                            python3.8 -m venv env
                            source env/bin/activate
                            
                            export PACKAGE_EGG_FILE=dist/dataproc_templates_distribution.egg

                            cd python
                            python setup.py bdist_egg --output=$PACKAGE_EGG_FILE
                        '''
                    }
            }
        }
        stage('Parallel Execution 0'){
            parallel{
                stage('Elasticsearch to GCS'){
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate

                                gsutil cp gs://dataproc-templates/conf/elastic.jks /tmp/
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true
                                export JARS="gs://dataproc-templates/jars/elasticsearch-spark-20_2.12-8.15.3.jar"
                                export FILES="/tmp/elastic.jks"

                                cd python
                                
                                ./bin/start.sh \
                                    --properties=spark.driver.extraJavaOptions='-Djavax.net.ssl.trustStore=./elastic.jks -Djavax.net.ssl.trustStorePassword=changeit',spark.executor.extraJavaOptions='-Djavax.net.ssl.trustStore=./elastic.jks -Djavax.net.ssl.trustStorePassword=changeit'
                                    -- --template=ELASTICSEARCHTOGCS \
                                    --es.gcs.input.node=${ELASTIC_INPUT_NODE} \
                                    --es.gcs.input.index="books" \
                                    --es.gcs.input.user=${ELASTIC_USER} \
                                    --es.gcs.input.password=${ELASTIC_PASSWORD} \
                                    --es.gcs.output.format="parquet" \
                                    --es.gcs.output.location="gs://dataproc-templates/integration-testing/output/ELASTICSEARCHTOGCS" \
                                    --es.gcs.output.mode="overwrite" \
                                    --es.gcs.input.es.net.ssl="true" \
                                    --es.gcs.input.es.net.ssl.cert.allow.self.signed="true"
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 1'){
            parallel{ 
                stage('Hive TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                    --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                    -- --template=HIVETOGCS \
                                    --hive.gcs.input.database="default" \
                                    --hive.gcs.input.table="employee" \
                                    --hive.gcs.output.location="gs://python-dataproc-templates/hive-gcs-output" \
                                    --hive.gcs.output.format="csv" \
                                    --hive.gcs.output.mode="overwrite"
                            '''
                        }
                    }
                }  
                stage('Hive TO BigQuery') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                    --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                    -- --template=HIVETOBIGQUERY \
                                    --hive.bigquery.input.database="default" \
                                    --hive.bigquery.input.table="employee" \
                                    --hive.bigquery.output.dataset="hive_to_bq_py" \
                                    --hive.bigquery.output.table="employee" \
                                    --hive.bigquery.output.mode="overwrite" \
                                    --hive.bigquery.temp.bucket.name="python-hive-to-bq-temp"
                            '''
                        }
                    }
                } 
                stage('GCS To JDBC') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                export SKIP_BUILD=true
                                
                                cd python
                                
                                ./bin/start.sh \
                                -- --template=GCSTOJDBC \
                                --gcs.jdbc.input.format="csv" \
                                --gcs.jdbc.input.location="gs://python-dataproc-templates-temp/input/csv/sample.csv" \
                                --gcs.jdbc.output.table="demo" \
                                --gcs.jdbc.output.mode="overwrite" \
                                --gcs.jdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                --gcs.jdbc.batch.size=1000 \
                                --gcs.jdbc.output.url="$TEST_JDBC_URL"
                            '''
                        }
                    }
                }     
            }
        }
        stage('Parallel Execution 2'){
            parallel{
                stage('JDBC To JDBC') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://datproc_template_nk/jars/mysql-connector-java-8.0.29.jar,gs://datproc_template_nk/jars/postgresql-42.2.6.jar,gs://datproc_template_nk/jars/mssql-jdbc-6.4.0.jre8.jar"
                                export SKIP_BUILD=true
                                
                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOJDBC \
                                --jdbctojdbc.input.url="$TEST_JDBC_URL" \
                                --jdbctojdbc.input.driver="com.mysql.cj.jdbc.Driver" \
                                --jdbctojdbc.input.table="demo" \
                                --jdbctojdbc.output.url="$TEST_JDBC_URL" \
                                --jdbctojdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                --jdbctojdbc.output.table="demo_out" \
                                --jdbctojdbc.output.mode="overwrite" \
                                --jdbctojdbc.output.batch.size="1000"
                            '''
                        }
                    }
                } 
                stage('JDBC To JDBC with secrets') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://datproc_template_nk/jars/mysql-connector-java-8.0.29.jar,gs://datproc_template_nk/jars/postgresql-42.2.6.jar,gs://datproc_template_nk/jars/mssql-jdbc-6.4.0.jre8.jar"
                                export SKIP_BUILD=true
                                
                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOJDBC \
                                --jdbctojdbc.input.url.secret="jdbctobqconn" \
                                --jdbctojdbc.input.driver="com.mysql.cj.jdbc.Driver" \
                                --jdbctojdbc.input.table="demo" \
                                --jdbctojdbc.output.url.secret="jdbctobqconn" \
                                --jdbctojdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                --jdbctojdbc.output.table="demo_out" \
                                --jdbctojdbc.output.mode="overwrite" \
                                --jdbctojdbc.output.batch.size="1000"
                            '''
                        }
                    }
                }
                stage('GCS To MONGO') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://python-dataproc-templates-temp/jars/mongo-java-driver-3.9.1.jar,gs://python-dataproc-templates-temp/jars/mongo-spark-connector_2.12-2.4.0.jar"
                                export SKIP_BUILD=true
                                
                                cd python
                                
                                ./bin/start.sh \
                                -- --template=GCSTOMONGO \
                                --gcs.mongo.input.format="avro" \
                                --gcs.mongo.input.location="gs://dataproc-templates/data/avro/empavro" \
                                --gcs.mongo.output.uri="$ENV_TEST_MONGO_DB_URI" \
                                --gcs.mongo.output.database="demo" \
                                --gcs.mongo.output.collection="analysis" \
                                --gcs.mongo.output.mode="overwrite"
                            '''
                        }
                    }
                } 
            }
        }
        stage('Parallel Execution 3'){
            parallel{
                stage('GCS TO BIGQUERY') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env
                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh  -- \
                                --template=GCSTOBIGQUERY \
                                --gcs.bigquery.input.format="parquet" \
                                --gcs.bigquery.input.location="gs://python-dataproc-templates/gcs-bq-input/" \
                                --gcs.bigquery.output.dataset="python_dataproc_templates" \
                                --gcs.bigquery.output.table="python_gcs_bq_table" \
                                --gcs.bigquery.output.mode=overwrite \
                                --gcs.bigquery.temp.bucket.name="python-dataproc-templates-temp-bq"
                            '''
                        }
                    }
                }
                stage('BIGQUERY TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=BIGQUERYTOGCS \
                                    --bigquery.gcs.input.table=python_dataproc_templates.python_gcs_bq_table \
                                    --bigquery.gcs.output.format=csv \
                                    --bigquery.gcs.output.mode=overwrite \
                                    --bigquery.gcs.output.location="gs://python-dataproc-tmplates-bq-to-gcs/csv/"
                            '''
                        }
                    }
                }

                stage('Cassandra TO BigQuery') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://spark-cassandra-connector/spark-cassandra-connector-assembly_2.12-3.2.0.jar,gs://spark-cassandra-connector/jnr-posix-3.1.8.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=CASSANDRATOBQ \
                                --cassandratobq.input.keyspace=testkeyspace \
                                --cassandratobq.input.table=emp \
                                --cassandratobq.input.host=$ENV_TEST_CASSANDRA_HOST \
                                --cassandratobq.bigquery.location=dataproc_templates.py_cassandra_bq \
                                --cassandratobq.temp.gcs.location=python-dataproc-templates-temp-bq \
                                --cassandratobq.output.mode=overwrite
                            '''
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 4'){
            parallel{
                stage('MONGO TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://python-dataproc-templates-temp/jars/mongo-java-driver-3.9.1.jar,gs://python-dataproc-templates-temp/jars/mongo-spark-connector_2.12-2.4.0.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=MONGOTOGCS \
                                --mongo.gcs.output.format="avro" \
                                --mongo.gcs.output.location="gs://python-dataproc-templates-temp/integration-testing/output/MONGOTOGCS" \
                                --mongo.gcs.output.mode="overwrite" \
                                --mongo.gcs.input.uri="$ENV_TEST_MONGO_DB_URI" \
                                --mongo.gcs.input.database="demo" \
                                --mongo.gcs.input.collection="analysis"
                            '''
                        }
                    }
                }
                stage('MONGO TO BIGQUERY') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://python-dataproc-templates-temp/jars/mongo-java-driver-3.9.1.jar,gs://python-dataproc-templates-temp/jars/mongo-spark-connector_2.12-2.4.0.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=MONGOTOBIGQUERY \
                                --mongo.bq.input.uri="$ENV_TEST_MONGO_DB_URI" \
                                --mongo.bq.input.database="demo" \
                                --mongo.bq.input.collection="dummyusers" \
                                --mongo.bq.output.dataset="dataproc_templates" \
                                --mongo.bq.output.table="mongotobq" \
                                --mongo.bq.output.mode="overwrite" \
                                --mongo.bq.temp.bucket.name="dataproc-templates/integration-testing/mongotobigquery"
                            '''
                        }
                    }
                }
                stage('JDBC TO BIGQUERY') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOBIGQUERY \
                                --jdbc.bigquery.input.url="$TEST_JDBC_URL" \
                                --jdbc.bigquery.input.driver="com.mysql.jdbc.Driver" \
                                --jdbc.bigquery.input.table="test.demo" \
                                --jdbc.bigquery.output.mode="overwrite" \
                                --jdbc.bigquery.output.dataset="dataproc_templates" \
                                --jdbc.bigquery.output.table="jdbctobq" \
                                --jdbc.bigquery.temp.bucket.name="dataproc-templates/integration-testing/jdbctobq"
                            '''
                        }
                    }
                }
                stage('JDBC TO BIGQUERY with secret') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOBIGQUERY \
                                --jdbc.bigquery.input.url.secret="jdbctobqconn" \
                                --jdbc.bigquery.input.driver="com.mysql.jdbc.Driver" \
                                --jdbc.bigquery.input.table="test.demo" \
                                --jdbc.bigquery.output.mode="overwrite" \
                                --jdbc.bigquery.output.dataset="dataproc_templates" \
                                --jdbc.bigquery.output.table="jdbctobq" \
                                --jdbc.bigquery.temp.bucket.name="dataproc-templates/integration-testing/jdbctobq"
                            '''
                        }
                    }
                }
                stage('Cassandra TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh'''

                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://spark-cassandra-connector/spark-cassandra-connector-assembly_2.12-3.2.0.jar,gs://spark-cassandra-connector/jnr-posix-3.1.8.jar"
                                export SKIP_BUILD=true
                                
                                cd python

                                bin/start.sh \
                                -- --template CASSANDRATOGCS \
                                --cassandratogcs.input.keyspace=testkeyspace \
                                --cassandratogcs.input.table=emp \
                                --cassandratogcs.input.host=$ENV_TEST_CASSANDRA_HOST \
                                --cassandratogcs.output.format=csv \
                                --cassandratogcs.output.path=gs://dataproc-templates/integration-testing/cassandratogcs/output \
                                --cassandratogcs.output.savemode=overwrite
                            '''
                        }
                    }
                }        
            }
        }
        stage('Parallel Execution 5'){
            parallel{
                stage('JDBC TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOGCS \
                                --jdbctogcs.input.url="$TEST_JDBC_URL" \
                                --jdbctogcs.input.driver="com.mysql.jdbc.Driver" \
                                --jdbctogcs.input.table="test.demo" \
                                --jdbctogcs.output.location="gs://dataproc-templates/integration-testing/jdbctogcs/output-python" \
                                --jdbctogcs.output.mode="overwrite" \
                                --jdbctogcs.output.format="csv"
                            '''
                        }
                    }
                }
                stage('JDBC TO GCS with secret') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=JDBCTOGCS \
                                --jdbctogcs.input.url.secret="jdbctobqconn" \
                                --jdbctogcs.input.driver="com.mysql.jdbc.Driver" \
                                --jdbctogcs.input.table="test.demo" \
                                --jdbctogcs.output.location="gs://dataproc-templates/integration-testing/jdbctogcs/output-python" \
                                --jdbctogcs.output.mode="overwrite" \
                                --jdbctogcs.output.format="csv"
                            '''
                        }
                    }
                }
                stage('GCS TO GCS') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=GCSTOGCS \
                                --gcs.to.gcs.input.location="gs://dataproc-templates/data/avro/empavro" \
                                --gcs.to.gcs.input.format="avro" \
                                --gcs.to.gcs.output.location="gs://python-dataproc-templates-temp/integration-testing/output/GCSTOGCS" \
                                --gcs.to.gcs.output.format="csv" \
                                --gcs.to.gcs.output.mode="overwrite" \
                                --gcs.to.gcs.temp.view.name="dataset" \
                                --gcs.to.gcs.sql.query="select * from dataset where sal>1500"
                            '''
                        }
                    }
                }
            }
        }  
         stage('Parallel Execution 6'){
            parallel{
                stage('S3 TO BigQuery (avro)') {    
                    steps{
                        retry(count: stageRetryCount) {
                            withCredentials([usernamePassword(credentialsId: 'aws-s3-ro-credentials',
                        passwordVariable: 'S3_SECRET', usernameVariable: 'S3_KEY')])
                            {sh '''
                                python3.8 -m pip install --user virtualenv    

                                python3.8 -m venv env

                                source env/bin/activate
                                
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true

                                cd python

                                ./bin/start.sh \
                                    -- --template=S3TOBIGQUERY \
                                        --s3.bq.input.location="s3a://dataproc-templates-integration-tests/cities.avro" \
                                        --s3.bq.access.key="${S3_KEY}" \
                                        --s3.bq.secret.key="${S3_SECRET}" \
                                        --s3.bq.input.format="avro" \
                                        --s3.bq.output.dataset.name="dataproc_templates" \
                                        --s3.bq.output.table.name="s3_to_bq_avro_py" \
                                        --s3.bq.output.mode="overwrite" \
                                        --s3.bq.temp.bucket.name="python-dataproc-templates-temp"
                            '''
                            }
                        }
                    }
                }
            }
        }
        stage('Parallel Execution 7'){
            parallel{
                // stage('Hbase TO GCS (Manual)') {    
                //     when {
                //         // Run this stage only if JOB_TYPE is not set to CLUSTER
                //         expression { env.JOB_TYPE != "CLUSTER" }
                //     }
                //     steps{
                //         retry(count: stageRetryCount) {
                //             sh '''
                //                 python3.8 -m pip install --user virtualenv    

                //                 python3.8 -m venv env
                //                 source env/bin/activate
                                
                //                 export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                //                 export JARS="gs://deps-dataproc-template/hbase-shaded-mapreduce-2.4.12.jar,gs://deps-dataproc-template/hbase-client-2.4.12.jar,gs://deps-dataproc-template/htrace-core4-4.2.0-incubating.jar,file:///usr/lib/spark/external/hbase-spark-protocol-shaded.jar,file:///usr/lib/spark/external/hbase-spark.jar"
                //                 export SKIP_BUILD=true
                                
                //                 cd python
                                
                //                 ./bin/start.sh \
                //                     --container-image="gcr.io/$GCP_PROJECT/hbase-to-gcs:1.0.2" \
                //                     --properties="spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/" \
                //                     -- --template=HBASETOGCS \
                //                     --hbase.gcs.output.location="gs://python-dataproc-templates/hbase-gcs-output" \
                //                     --hbase.gcs.output.format="csv" \
                //                     --hbase.gcs.output.mode="overwrite" \
                //                     --hbase.gcs.catalog.json=\'\'\'{
                //                             "table":{"namespace":"default","name":"my_table"},
                //                             "rowkey":"key",
                //                             "columns":{
                //                             "key":{"cf":"rowkey", "col":"key", "type":"string"},
                //                             "name":{"cf":"cf", "col":"name", "type":"string"}
                //                             }
                //                             }\'\'\'
                //             '''
                //         }
                //     }
                // }
                    
                // stage('Hbase TO GCS (Automated)') {
                //     when {
                //         // Run this stage only if JOB_TYPE is not set to CLUSTER
                //         expression { env.JOB_TYPE != "CLUSTER" }
                //     }
                //     steps{
                //         retry(count: stageRetryCount) {
                //             sh '''
                //                 whoami
                //                 sudo su

                //                 python3.8 -m pip install --user virtualenv    

                //                 python3.8 -m venv env

                //                 source env/bin/activate

                //                 gsutil cp gs://python-dataproc-templates/surjitsh/hbase-site.xml .
                                
                //                 export GCS_STAGING_LOCATION=gs://python-dataproc-templates-temp
                //                 export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                //                 export IMAGE_NAME_VERSION=hbase-to-gcs:1.0.2
                //                 export HBASE_SITE_PATH=../hbase-site.xml
                //                 export IMAGE=gcr.io/${GCP_PROJECT}/${IMAGE_NAME_VERSION}
                //                 export SKIP_IMAGE_BUILD=TRUE

                //                 cd python

                //                 ./bin/start.sh \
                //                     --container-image=$IMAGE \
                //                     --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'  \
                //                     -- --template=HBASETOGCS \
                //                     --hbase.gcs.output.location=gs://python-dataproc-templates/hbase-gcs-output \
                //                     --hbase.gcs.output.format=csv \
                //                     --hbase.gcs.output.mode=overwrite \
                //                     --hbase.gcs.catalog.json=$CATALOG
                //             '''
                //         }
                //     }
                // }
                stage('GCS TO Bigtable') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                            
                                export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                export SKIP_BUILD=true
                                export JARS="gs://spark-lib/bigtable/spark-bigtable_2.12-0.1.0.jar"
                                export SPARK_PROPERTIES="spark.jars.packages=org.slf4j:slf4j-reload4j:1.7.36"

                                cd python
                                
                                ./bin/start.sh \
                                -- --template=GCSTOBIGTABLE \
                                --gcs.bigtable.input.format="csv" \
                                --gcs.bigtable.input.location="gs://dataproc-templates/data/csv/cities.csv" \
                                --spark.bigtable.project.id=$GCP_PROJECT \
                                --spark.bigtable.instance.id=$ENV_TEST_BIGTABLE_INSTANCE \
                                --gcs.bigtable.catalog.json=\'\'\'{
                                    "table":{"name":"cities"},
                                    "rowkey":"key",
                                    "columns":{
                                        "key":{"cf":"rowkey", "col":"key", "type":"string"},
                                        "LatD":{"cf":"lat", "col":"LatD", "type":"string"},
                                        "LatM":{"cf":"lat", "col":"LatM", "type":"string"},
                                        "LatS":{"cf":"lat", "col":"LatS", "type":"string"},
                                        "NS":{"cf":"lat", "col":"NS", "type":"string"},
                                        "LonD":{"cf":"lon", "col":"LonD", "type":"string"},
                                        "LonM":{"cf":"lon", "col":"LonM", "type":"string"},
                                        "LonS":{"cf":"lon", "col":"LonS", "type":"string"},
                                        "EW":{"cf":"lon", "col":"EW", "type":"string"},
                                        "City":{"cf":"place", "col":"City", "type":"string"},
                                        "State":{"cf":"place", "col":"State", "type":"string"}
                                    }
                                }\'\'\'
                            '''
                        }
                    }
                }
            }
        }
    }
    post {
        always{
            script {
                if( env.GIT_BRANCH_LOCAL == 'main' ){
                    googlechatnotification url: DATAPROC_TELEPORT_WEBHOOK_URL,
                    message: 'Jenkins: ${JOB_NAME}\nBuild status is ${BUILD_STATUS}\nSee ${BUILD_URL}\n',
                    notifyFailure: 'true',
                    notifyAborted: 'true',
                    notifyUnstable: 'true',
                    notifyNotBuilt: 'true',
                    notifyBackToNormal: 'true'
                }
            }
        }
    }
}
