pipeline {
    
    agent any
    
    environment {
        DATAPROC_TELEPORT_WEBHOOK_URL = credentials('dataproc-teleport-webhook-url')

        TEST_JDBC_URL = credentials('env-test-jdbc-url')

        GIT_BRANCH_LOCAL = sh (
            script: "echo $branchName | sed -e 's|origin/||g' | sed -e 's|^null\$|main|'",  // Remove "origin/" and set the default branch to main
            returnStdout: true
        ).trim()
    }
    
    stages {
                stage('Checkout') {
                    steps{
                        git branch: "${GIT_BRANCH_LOCAL}", changelog: false, poll: false, url: 'https://github.com/GoogleCloudPlatform/dataproc-templates/'    
                    }
                }
                stage('Clean Resources'){
                    steps {
                            catchError {
                                sh '''
                                    
                                '''
                            }
                    }
                }
                stage('Build'){
                    steps {
                            catchError {
                                sh '''
                                    python3.8 -m pip install --user virtualenv    
    
                                    python3.8 -m venv env
                                    source env/bin/activate
                                    
                                    export PACKAGE_EGG_FILE=dist/dataproc_templates_distribution.egg
    
                                    cd python
                                    python setup.py bdist_egg --output=$PACKAGE_EGG_FILE
                                '''
                            }
                    }
                }
    
                stage('Parallel Execution 1'){
                    parallel{ 
                            stage('Hive TO GCS') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh \
                                          --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                          -- --template=HIVETOGCS \
                                          --hive.gcs.input.database="default" \
                                          --hive.gcs.input.table="employee" \
                                          --hive.gcs.output.location="gs://python-dataproc-templates/hive-gcs-output" \
                                          --hive.gcs.output.format="csv" \
                                          --hive.gcs.output.mode="overwrite"'''
                                }
                            }  
                            stage('Hive TO BigQuery') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh \
                                          --properties=spark.hadoop.hive.metastore.uris=$ENV_TEST_HIVE_METASTORE_URIS \
                                          -- --template=HIVETOBIGQUERY \
                                          --hive.bigquery.input.database="default" \
                                          --hive.bigquery.input.table="employee" \
                                          --hive.bigquery.output.dataset="hive_to_bq_py" \
                                          --hive.bigquery.output.table="employee" \
                                          --hive.bigquery.output.mode="overwrite" \
                                          --hive.bigquery.temp.bucket.name="python-hive-to-bq-temp"'''
                                }
                            } 
                            stage('GCS To JDBC') {
                                steps{
                                    sh '''
                                    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                        export SKIP_BUILD=true
                                        
                                        cd python
                                        
                                        ./bin/start.sh \
                                        -- --template=GCSTOJDBC \
                                        --gcs.jdbc.input.format="csv" \
                                        --gcs.jdbc.input.location="gs://python-dataproc-templates-temp/input/csv/sample.csv" \
                                        --gcs.jdbc.output.table="demo" \
                                        --gcs.jdbc.output.mode="overwrite" \
                                        --gcs.jdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                        --gcs.jdbc.batch.size=1000 \
                                        --gcs.jdbc.output.url="$TEST_JDBC_URL"'''
                                }
                            }     
                        }
                    }
    
                stage('Parallel Execution 2'){
                    parallel{
                            stage('TEXT TO BIGQUERY') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh  -- \
                                        --template=TEXTTOBIGQUERY \
                                        --text.bigquery.input.compression="gzip" \
                                        --text.bigquery.input.delimiter="," \
                                        --text.bigquery.input.location="gs://python-dataproc-templates/text-bq-input/" \
                                        --text.bigquery.output.dataset="python_dataproc_templates" \
                                        --text.bigquery.output.table="python_text_bq_table" \
                                        --text.bigquery.output.mode=overwrite \
                                        --text.bigquery.temp.bucket.name="python-dataproc-templates-temp-bq"'''
                                }
                            }
                            stage('JDBC To JDBC') {
                                steps{
                                    sh '''
                                    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://datproc_template_nk/jars/mysql-connector-java-8.0.29.jar,gs://datproc_template_nk/jars/postgresql-42.2.6.jar,gs://datproc_template_nk/jars/mssql-jdbc-6.4.0.jre8.jar"
                                        export SKIP_BUILD=true
                                        
                                        cd python
                                        
                                        ./bin/start.sh \
                                        -- --template=JDBCTOJDBC \
                                        --jdbctojdbc.input.url="$TEST_JDBC_URL" \
                                        --jdbctojdbc.input.driver="com.mysql.cj.jdbc.Driver" \
                                        --jdbctojdbc.input.table="demo" \
                                        --jdbctojdbc.output.url="$TEST_JDBC_URL" \
                                        --jdbctojdbc.output.driver="com.mysql.cj.jdbc.Driver" \
                                        --jdbctojdbc.output.table="demo_out" \
                                        --jdbctojdbc.output.mode="overwrite" \
                                        --jdbctojdbc.output.batch.size="1000"'''     
                                }
                            } 
                            stage('GCS To MONGO') {
                                steps{
                                    sh '''
                                    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://python-dataproc-templates-temp/jars/mongo-java-driver-3.9.1.jar,gs://python-dataproc-templates-temp/jars/mongo-spark-connector_2.12-2.4.0.jar"
                                        export SKIP_BUILD=true
                                        
                                        cd python
                                        
                                        ./bin/start.sh \
                                        -- --template=GCSTOMONGO \
                                        --gcs.mongo.input.format="avro" \
                                        --gcs.mongo.input.location="gs://dataproc-templates/data/avro/empavro" \
                                        --gcs.mongo.output.uri="$ENV_TEST_MONGO_DB_URI" \
                                        --gcs.mongo.output.database="demo" \
                                        --gcs.mongo.output.collection="analysis" \
                                        --gcs.mongo.output.mode="overwrite"'''
                                }
                            } 
                    }
                }
    
                stage('Parallel Execution 3'){
                    parallel{
                            
                            stage('GCS TO BIGQUERY') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh  -- \
                                        --template=GCSTOBIGQUERY \
                                        --gcs.bigquery.input.format="parquet" \
                                        --gcs.bigquery.input.location="gs://python-dataproc-templates/gcs-bq-input/" \
                                        --gcs.bigquery.output.dataset="python_dataproc_templates" \
                                        --gcs.bigquery.output.table="python_gcs_bq_table" \
                                        --gcs.bigquery.output.mode=overwrite \
                                        --gcs.bigquery.temp.bucket.name="python-dataproc-templates-temp-bq"'''
                                }
                            }
                            stage('BIGQUERY TO GCS') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh \
                                        -- --template=BIGQUERYTOGCS \
                                            --bigquery.gcs.input.table=python_dataproc_templates.python_gcs_bq_table \
                                            --bigquery.gcs.output.format=csv \
                                            --bigquery.gcs.output.mode=overwrite \
                                            --bigquery.gcs.output.location="gs://python-dataproc-tmplates-bq-to-gcs/csv/"'''
                                }
                            }

                            stage('Cassandra TO BigQuery') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://spark-cassandra-connector/spark-cassandra-connector-assembly_2.12-3.2.0.jar,gs://spark-cassandra-connector/jnr-posix-3.1.8.jar,gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh \
                                        -- --template=CASSANDRATOBQ \
                                        --cassandratobq.input.keyspace=testkeyspace \
                                        --cassandratobq.input.table=emp \
                                        --cassandratobq.input.host=$ENV_TEST_CASSANDRA_HOST \
                                        --cassandratobq.bigquery.location=dataproc_templates.py_cassandra_bq \
                                        --cassandratobq.temp.gcs.location=python-dataproc-templates-temp-bq \
                                        --cassandratobq.output.mode=overwrite
                                        '''
                                }
                            }
                        }
                    }
            stage('Parallel Execution 4'){
                    parallel{

                            stage('MONGO TO GCS') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://python-dataproc-templates-temp/jars/mongo-java-driver-3.9.1.jar,gs://python-dataproc-templates-temp/jars/mongo-spark-connector_2.12-2.4.0.jar"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh \
                                        -- --template=MONGOTOGCS \
                                        --mongo.gcs.output.format="avro" \
                                        --mongo.gcs.output.location="gs://python-dataproc-templates-temp/integration-testing/output/MONGOTOGCS" \
                                        --mongo.gcs.output.mode="overwrite" \
                                        --mongo.gcs.input.uri="$ENV_TEST_MONGO_DB_URI" \
                                        --mongo.gcs.input.database="demo" \
                                        --mongo.gcs.input.collection="analysis"'''
                                }
                            }
                            stage('JDBC TO BIGQUERY') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar,gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh \
                                        -- --template=JDBCTOBIGQUERY \
                                        --jdbc.bigquery.input.url="$TEST_JDBC_URL" \
                                        --jdbc.bigquery.input.driver="com.mysql.jdbc.Driver" \
                                        --jdbc.bigquery.input.table="test.demo" \
                                        --jdbc.bigquery.output.mode="overwrite" \
                                        --jdbc.bigquery.output.dataset="dataproc_templates" \
                                        --jdbc.bigquery.output.table="jdbctobq" \
                                        --jdbc.bigquery.temp.bucket.name="dataproc-templates/integration-testing/jdbctobq"'''
                                }
                            }

                            stage('Cassandra TO GCS') {
                                steps{
                                    sh'''

                                        python3.8 -m pip install --user virtualenv    
        
                                        python3.8 -m venv env

                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://spark-cassandra-connector/spark-cassandra-connector-assembly_2.12-3.2.0.jar,gs://spark-cassandra-connector/jnr-posix-3.1.8.jar"




                                       cd python

                                       bin/start.sh \
                                       -- --template CASSANDRATOGCS \
                                       --cassandratogcs.input.keyspace=testkeyspace \
                                       --cassandratogcs.input.table=emp \
                                       --cassandratogcs.input.host=$ENV_TEST_CASSANDRA_HOST \
                                       --cassandratogcs.output.format=csv \
                                       --cassandratogcs.output.path=gs://dataproc-templates/integration-testing/cassandratogcs/output \
                                       --cassandratogcs.output.savemode=overwrite --jars "gs://spark-cassandra-connector/spark-cassandra-connector-assembly_2.12-3.2.0.jar,gs://spark-cassandra-connector/jnr-posix-3.1.8.jar"


                                    '''
                                }
                        }
                        
                        }
                    }
            stage('Parallel Execution 5'){
                    parallel{
                            stage('JDBC TO GCS') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://dataproc-templates/jars/mysql-connector-java-8.0.29.jar"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh \
                                        -- --template=JDBCTOGCS \
                                        --jdbctogcs.input.url="$TEST_JDBC_URL" \
                                        --jdbctogcs.input.driver="com.mysql.jdbc.Driver" \
                                        --jdbctogcs.input.table="test.demo" \
                                        --jdbctogcs.output.location="gs://dataproc-templates/integration-testing/jdbctogcs/output-python" \
                                        --jdbctogcs.output.mode="overwrite" \
                                        --jdbctogcs.output.format="csv"'''
                                }
                            }
                            stage('GCS TO GCS') {
                                steps{
                                    sh '''
                                    
                                        python3.8 -m pip install --user virtualenv    
    
                                        python3.8 -m venv env
    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export SKIP_BUILD=true
    
                                        cd python
                                        
                                        ./bin/start.sh \
                                        -- --template=GCSTOGCS \
                                        --gcs.to.gcs.input.location="gs://dataproc-templates/data/avro/empavro" \
                                        --gcs.to.gcs.input.format="avro" \
                                        --gcs.to.gcs.output.location="gs://python-dataproc-templates-temp/integration-testing/output/GCSTOGCS" \
                                        --gcs.to.gcs.output.format="csv" \
                                        --gcs.to.gcs.output.mode="overwrite" \
                                        --gcs.to.gcs.temp.view.name="dataset" \
                                        --gcs.to.gcs.sql.query="select * from dataset where sal>1500"'''
                                }
                            }


                        }
                    }  
            stage('Parallel Execution 6'){
                    parallel{
                            stage('Hbase TO GCS (Manual)') {
                                steps{
                                    sh '''
                                    
                                        source env/bin/activate
                                        
                                        export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                        export JARS="gs://deps-dataproc-template/hbase-shaded-mapreduce-2.4.12.jar,gs://deps-dataproc-template/hbase-client-2.4.12.jar,gs://deps-dataproc-template/htrace-core4-4.2.0-incubating.jar,file:///usr/lib/spark/external/hbase-spark-protocol-shaded.jar,file:///usr/lib/spark/external/hbase-spark.jar"
                                        export SKIP_BUILD=true
                                        
                                        cd python
                                        
                                        ./bin/start.sh \
                                            --container-image="gcr.io/$GCP_PROJECT/hbase-to-gcs:1.0.1" \
                                            --properties="spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/" \
                                            -- --template=HBASETOGCS \
                                            --hbase.gcs.output.location="gs://python-dataproc-templates/hbase-gcs-output" \
                                            --hbase.gcs.output.format="csv" \
                                            --hbase.gcs.output.mode="overwrite" \
                                            --hbase.gcs.catalog.json=\'\'\'{
                                                    "table":{"namespace":"default","name":"my_table"},
                                                    "rowkey":"key",
                                                    "columns":{
                                                    "key":{"cf":"rowkey", "col":"key", "type":"string"},
                                                    "name":{"cf":"cf", "col":"name", "type":"string"}
                                                    }
                                                    }\'\'\'
                                        '''
                                }
                            }
                            
                            stage('Hbase TO GCS (Automated)') {
                                steps{
                                    sh '''
                                        whoami
                                        sudo su
                                        source env/bin/activate
    
                                        gsutil cp gs://python-dataproc-templates/surjitsh/hbase-site.xml .
                                        
                                        export GCS_STAGING_LOCATION=gs://python-dataproc-templates-temp
                                        export CATALOG='{"table":{"namespace":"default","name":"my_table"},"rowkey":"key","columns":{"key":{"cf":"rowkey","col":"key","type":"string"},"name":{"cf":"cf","col":"name","type":"string"}}}'
                                        export IMAGE_NAME_VERSION=hbase-to-gcs:1.0.1
                                        export HBASE_SITE_PATH=../hbase-site.xml
                                        export IMAGE=gcr.io/${GCP_PROJECT}/${IMAGE_NAME_VERSION}
                                        export SKIP_IMAGE_BUILD=TRUE
    
                                        cd python
    
                                        ./bin/start.sh \
                                            --container-image=$IMAGE \
                                            --properties='spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/'  \
                                            -- --template=HBASETOGCS \
                                            --hbase.gcs.output.location=gs://python-dataproc-templates/hbase-gcs-output \
                                            --hbase.gcs.output.format=csv \
                                            --hbase.gcs.output.mode=overwrite \
                                            --hbase.gcs.catalog.json=$CATALOG
                                        '''
                                }
                            }

                            stage('GCS TO Bigtable') {
                                    steps{
                                        sh '''
                                        
                                            python3.8 -m pip install --user virtualenv    
        
                                            python3.8 -m venv env
        
                                            source env/bin/activate
                                            
                                            export GCS_STAGING_LOCATION="gs://python-dataproc-templates-temp"
                                            export SKIP_BUILD=true
                                            export JARS="gs://python-dataproc-templates-temp/jars/bigtable-hbase-2.x-shaded-2.3.0.jar,gs://deps-dataproc-template/hbase-shaded-mapreduce-2.4.12.jar,gs://deps-dataproc-template/hbase-client-2.4.12.jar,file:///usr/lib/spark/external/hbase-spark-protocol-shaded.jar,file:///usr/lib/spark/external/hbase-spark.jar"
    
        
                                            cd python
                                            
                                            ./bin/start.sh \
                                            --container-image="gcr.io/$GCP_PROJECT/bt-templates-test:1.0.0" \
                                            --properties="spark.dataproc.driverEnv.SPARK_EXTRA_CLASSPATH=/etc/hbase/conf/" \
                                            -- --template=GCSTOBIGTABLE \
                                            --gcs.bigtable.input.format="csv" \
                                            --gcs.bigtable.input.location="gs://dataproc-templates/data/csv/cities.csv" \
                                            --gcs.bigtable.hbase.catalog.json=\'\'\'{
                                                "table":{"namespace":"default","name":"cities"},
                                                "rowkey":"key",
                                                "columns":{
                                                    "key":{"cf":"rowkey", "col":"key", "type":"string"},
                                                    "LatD":{"cf":"lat", "col":"LatD", "type":"string"},
                                                    "LatM":{"cf":"lat", "col":"LatM", "type":"string"},
                                                    "LatS":{"cf":"lat", "col":"LatS", "type":"string"},
                                                    "NS":{"cf":"lat", "col":"NS", "type":"string"},
                                                    "LonD":{"cf":"lon", "col":"LonD", "type":"string"},
                                                    "LonM":{"cf":"lon", "col":"LonM", "type":"string"},
                                                    "LonS":{"cf":"lon", "col":"LonS", "type":"string"},
                                                    "EW":{"cf":"lon", "col":"EW", "type":"string"},
                                                    "City":{"cf":"place", "col":"City", "type":"string"},
                                                    "State":{"cf":"place", "col":"State", "type":"string"}
                                                }
                                            }\'\'\'
                                            '''
                                    }
                                }
  
                    }
                }
        }
        post {
            always{
                script {
                    if( env.GIT_BRANCH_LOCAL == 'main' ){
                        googlechatnotification url: DATAPROC_TELEPORT_WEBHOOK_URL,
                        message: 'Jenkins: ${JOB_NAME}\nBuild status is ${BUILD_STATUS}\nSee ${BUILD_URL}\n',
                        notifyFailure: 'true',
                        notifyAborted: 'true',
                        notifyUnstable: 'true',
                        notifyNotBuilt: 'true',
                        notifyBackToNormal: 'true'
                    }
                }
            }
        }
    }