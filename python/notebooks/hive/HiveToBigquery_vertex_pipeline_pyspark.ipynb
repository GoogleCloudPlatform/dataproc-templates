{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f55fbb-2197-4038-88c5-6c896a9f071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9eff8d-6b63-4e8f-b369-68cbb4ef04ee",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- [DataprocPySparkBatchOp reference](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html)\n",
    "- [Kubeflow SDK Overview](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)\n",
    "- [Dataproc Serverless in Vertex AI Pipelines tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)\n",
    "- [Build a Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)\n",
    "\n",
    "This notebook is built to run a Vertex AI User-Managed Notebook using the default Compute Engine Service Account.  \n",
    "Check the Dataproc Serverless in Vertex AI Pipelines tutorial linked above to learn how to setup a different Service Account.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852a764-dad3-4f3b-b0b5-a71825469fd3",
   "metadata": {},
   "source": [
    "#### Permissions\n",
    "\n",
    "Make sure that the service account used to run the notebook has the following roles:\n",
    "\n",
    "- roles/aiplatform.serviceAgent\n",
    "- roles/aiplatform.customCodeServiceAgent\n",
    "- roles/storage.objectCreator\n",
    "- roles/storage.objectViewer\n",
    "- roles/dataproc.editor\n",
    "- roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fa95e-b745-4649-8040-af99e7a4013c",
   "metadata": {},
   "source": [
    "#### Step 1:\n",
    "#### Set Google Cloud properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a04975-769b-4ce4-8b08-fbcb7b7448f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Configuration\n",
    "# User Inputs\n",
    "\n",
    "get_project_id = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = get_project_id[0]\n",
    "REGION = \"\"  # example\"us-west1\"\n",
    "GCS_STAGING_LOCATION = \"gs://<bucket_name>\" # example \"gs://bucket_name\"\n",
    "SUBNET = \"\" # example \"projects/<project-id>/regions/<region-id>/subnetworks/<subnet-name>\" \n",
    "INPUT_HIVE_DATABASE= \"\"\n",
    "INPUT_HIVE_TABLES= \"\" # example \"table1,table2,table3...\" or \"*\"\n",
    "OUTPUT_BIGQUERY_DATASET= \"\"\n",
    "TEMP_BUCKET= \"<bucket_name>\"\n",
    "HIVE_OUTPUT_MODE=\"\"\n",
    "HIVE_METASTORE= \"\" # example \"thrift://hive-cluster-m:9083\"\n",
    "MAX_PARALLELISM=10 # Overwrite the value if you want to increase number of parallel Dataproc Batch Jobs\n",
    "\n",
    "# Run Dataproc Templates from Vertex AI Pipelines\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook shows how to build a Vertex AI Pipeline to run a Dataproc Template using the DataprocPySparkBatchOp component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640fa29-1a04-4301-974d-9fcec95b7e7c",
   "metadata": {},
   "source": [
    "#### Step 2:\n",
    "#### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3742b9a-143d-49fc-b43c-e56179c7f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud notebooks requires dependencies to be installed with '--user'\n",
    "! pip3 install --upgrade google-cloud-pipeline-components kfp --user -q\n",
    "# Install latest JDK\n",
    "! sudo apt-get update\n",
    "! sudo apt-get install default-jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58552a67-9012-4ba9-82e9-d34299cd6d15",
   "metadata": {},
   "source": [
    "### Once you've installed the additional packages, you may need to restart the notebook kernel so it can find the packages.\n",
    "\n",
    "Uncomment & Run this cell if you want to restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c19b5e-e7d9-416f-ad12-edc19b6877e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#    import IPython\n",
    "#    app = IPython.Application.instance()\n",
    "#    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454bb0de-3e0a-4daa-92ed-1319e1d9604d",
   "metadata": {},
   "source": [
    "#### Step 3:\n",
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aede36-3a0a-43f7-8e4c-db2d8087e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime\n",
    "from google_cloud_pipeline_components.experimental.dataproc import DataprocPySparkBatchOp\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b33331-b529-47ac-a08d-0c166acd264e",
   "metadata": {},
   "source": [
    "#### Step 4:\n",
    "#### Change working directory to the Dataproc Templates python folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0e80a-e8b7-4e54-9f99-b7874e164978",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/python\"\n",
    "%cd /home/jupyter/dataproc-templates/python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bb252-c3e3-4d56-b1ad-f52a2db52869",
   "metadata": {},
   "source": [
    "#### Step 5:\n",
    "#### Build Dataproc Templates python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa8c93-5e69-48fc-984a-f4fa3b28519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_EGG_FILE = \"dist/dataproc_templates_distribution.egg\"\n",
    "! python ./setup.py bdist_egg --output=$PACKAGE_EGG_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528c874-b948-40c1-b57c-afff76d80b47",
   "metadata": {},
   "source": [
    "#### Step 6:\n",
    "#### Copy package to the GCS bucket\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/storage.objectCreator\n",
    " - roles/storage.objectViewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd85ce-46f6-4b35-8997-2189cf1b2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp main.py $GCS_STAGING_LOCATION/\n",
    "! gsutil cp -r $PACKAGE_EGG_FILE $GCS_STAGING_LOCATION/dist/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d8c5f-b28b-4fbb-b585-cb2f1d9c1915",
   "metadata": {},
   "source": [
    "#### Step 7:\n",
    "#### Get Hive Tables \n",
    "In case user wants to load all the Hive tables from the database, we need to get the table list using the metastore.\n",
    "\n",
    "Below cell will fetch all tables from the Hive database by running a Spark SQL query using the provided Hive Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751abe3-47b7-4b4a-81f4-498da750d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INPUT_HIVE_TABLES==\"*\":\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "    spark=SparkSession.builder \\\n",
    "          .master(\"local\")\\\n",
    "          .appName(\"Spark Job to get HIVE table list\") \\\n",
    "          .config(\"hive.metastore.uris\",\"thrift://hive-cluster-m:9083\") \\\n",
    "          .enableHiveSupport() \\\n",
    "          .getOrCreate()  \n",
    "    TABLE_LIST_DF=spark.sql(\"show tables in \"+INPUT_HIVE_DATABASE)\n",
    "    TABLE_LIST=TABLE_LIST_DF.select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "    print(\"Table Sets to Migrate: \")\n",
    "    print(TABLE_LIST)\n",
    "    spark.stop()\n",
    "else:\n",
    "    TABLE_LIST=INPUT_HIVE_TABLES.split(\",\")\n",
    "    print(\"Table Sets to Migrate: \")\n",
    "    print(TABLE_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbaf266-91a0-46e2-8a88-b594b9301c83",
   "metadata": {},
   "source": [
    "#### Step 8:\n",
    "\n",
    "Split Hive Tables list based on MAX_PARALLELISM value provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b170e58-7acf-4aae-9619-2361eef0ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "COMPLETE_LIST = copy.deepcopy(TABLE_LIST)\n",
    "PARALLEL_JOBS = len(TABLE_LIST)//MAX_PARALLELISM\n",
    "JOB_LIST = []\n",
    "while len(COMPLETE_LIST) > 0:\n",
    "    SUB_LIST = []\n",
    "    for i in range(MAX_PARALLELISM):\n",
    "        if len(COMPLETE_LIST)>0 :\n",
    "            SUB_LIST.append(COMPLETE_LIST[0])\n",
    "            COMPLETE_LIST.pop(0)\n",
    "        else:\n",
    "            break\n",
    "    JOB_LIST.append(SUB_LIST)\n",
    "print(\"List of tables for execution : \")\n",
    "print(JOB_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5a0b9-d2b2-4072-aa4f-3748609f7cdc",
   "metadata": {},
   "source": [
    "#### Step 9:\n",
    "\n",
    "Set Dataproc Template Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73197165-0ea0-4732-841d-8ba4bcd8a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\"\n",
    "MAIN_PYTHON_FILE = GCS_STAGING_LOCATION + \"/main.py\"\n",
    "JARS = [\"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\"]\n",
    "PYTHON_FILE_URIS = [GCS_STAGING_LOCATION + \"/dist/dataproc_templates_distribution.egg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332168ba-6e69-4f6e-aa03-38b4965b5304",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 10:\n",
    "#### Build pipeline and run Dataproc Template on Vertex AI Pipelines to migrate Hive tables to BigQuery\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/dataproc.editor\n",
    " - roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c13bf-c121-4465-88fc-04778ef35a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_prop={}\n",
    "runtime_prop['spark.hadoop.hive.metastore.uris']=HIVE_METASTORE\n",
    "runtime_prop['mapreduce.fileoutputcommitter.marksuccessfuljobs'] = \"false\"\n",
    "\n",
    "def migrate_hive(EXECUTION_LIST):\n",
    "    EXECUTION_LIST = EXECUTION_LIST\n",
    "    aiplatform.init(project=PROJECT_ID, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name=\"hive-to-bq-pyspark\",\n",
    "        description=\"Pipeline to migrate tables from hive to bq\",\n",
    "    )\n",
    "    def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        location: str = REGION,\n",
    "        main_python_file_uri: str = MAIN_PYTHON_FILE,\n",
    "        python_file_uris: list = PYTHON_FILE_URIS,\n",
    "        jar_file_uris: list = JARS,\n",
    "        subnetwork_uri: str = SUBNET\n",
    "    ):\n",
    "        for table in EXECUTION_LIST:\n",
    "            BATCH_ID = \"hive2bq-{}-{}\".format(table,datetime.now().strftime(\"%s\"))\n",
    "            TEMPLATE_SPARK_ARGS = [\n",
    "                                    \"--template=HIVETOBIGQUERY\",\n",
    "                                    \"--hive.bigquery.input.database={}\".format(INPUT_HIVE_DATABASE),\n",
    "                                    \"--hive.bigquery.input.table={}\".format(table),\n",
    "                                    \"--hive.bigquery.output.table={}\".format(table),\n",
    "                                    \"--hive.bigquery.output.dataset={}\".format(OUTPUT_BIGQUERY_DATASET),\n",
    "                                    \"--hive.bigquery.output.mode={}\".format(HIVE_OUTPUT_MODE),\n",
    "                                    \"--hive.bigquery.temp.bucket.name={}\".format(TEMP_BUCKET)                                    ]\n",
    "            _ = DataprocPySparkBatchOp(\n",
    "                project=project_id,\n",
    "                location=location,\n",
    "                batch_id=BATCH_ID,\n",
    "                main_python_file_uri=main_python_file_uri,\n",
    "                python_file_uris=python_file_uris,\n",
    "                jar_file_uris=jar_file_uris,\n",
    "                subnetwork_uri=subnetwork_uri,\n",
    "                runtime_config_properties=runtime_prop,\n",
    "                args=TEMPLATE_SPARK_ARGS,\n",
    "            )\n",
    "            time.sleep(1)\n",
    "\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "    pipeline = aiplatform.PipelineJob(\n",
    "            display_name=\"pipeline\",\n",
    "            template_path=\"pipeline.json\",\n",
    "            pipeline_root=PIPELINE_ROOT,\n",
    "            enable_caching=False,\n",
    "            )\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba4453-ddf9-4f35-901e-ab20433ae7fa",
   "metadata": {},
   "source": [
    "#### Step 11:\n",
    "\n",
    "Run Dataproc Batch Template based on Hive Tables list calculated in Step 8.\n",
    "\n",
    "The below cell will call function migrate_hive to migrate tables using dataproc serverless batch job and also add an entry in Audit Table for each Table Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580bcc2-9302-48dc-8a47-7ccac603fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIT_DICT={}\n",
    "AUDIT_DF = pd.DataFrame(columns=[\"Source_DB_Name\",\"Source_Table_Set\",\"Target_DB_Name\",\"Target_Table_Set\",\"Job_Start_Time\",\"Job_End_Time\",\"Job_Status\"])\n",
    " \n",
    "for execution_list in JOB_LIST:\n",
    "    print(\"\\n\\nLoading Table Set: \"+str(execution_list))\n",
    "    AUDIT_DICT[\"Source_DB_Name\"]=INPUT_HIVE_DATABASE\n",
    "    AUDIT_DICT[\"Source_Table_Set\"]='|'.join(execution_list)\n",
    "    AUDIT_DICT[\"Target_DB_Name\"]=OUTPUT_BIGQUERY_DATASET\n",
    "    AUDIT_DICT[\"Target_Table_Set\"]='|'.join(execution_list)\n",
    "    AUDIT_DICT[\"Job_Start_Time\"]=str(datetime.now())\n",
    "    try:\n",
    "        migrate_hive(execution_list)\n",
    "    except Exception:\n",
    "        AUDIT_DICT[\"Job_Status\"]=\"FAIL\"\n",
    "        print(\"\\n\\nSome Error Occured while loading Table Set: \"+str(execution_list))\n",
    "    else:\n",
    "        AUDIT_DICT[\"Job_Status\"]=\"PASS\"\n",
    "        print(\"\\n\\nLoaded Table Set: \"+str(execution_list))\n",
    "\n",
    "    AUDIT_DICT[\"Job_End_Time\"]=str(datetime.now())\n",
    "    AUDIT_DF=AUDIT_DF.append(AUDIT_DICT, ignore_index = True)\n",
    "    \n",
    "if AUDIT_DF.empty:\n",
    "    print(\"Audit Dataframe is Empty\")\n",
    "else:\n",
    "    print(AUDIT_DF)\n",
    "    AUDIT_DF.to_csv(\"gs://\"+TEMP_BUCKET+\"/audit/audit_file_{}.csv\".format(str(datetime.now())),index=False,header = False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
