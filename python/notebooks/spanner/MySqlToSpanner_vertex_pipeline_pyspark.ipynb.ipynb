{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db5371a-8f16-47b7-bcc7-5af386e9b6d8",
   "metadata": {},
   "source": [
    "# <center>MySQL to Cloud Spanner Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98acd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd944742",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- [DataprocPySparkBatchOp reference](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html)\n",
    "- [Kubeflow SDK Overview](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)\n",
    "- [Dataproc Serverless in Vertex AI Pipelines tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)\n",
    "- [Build a Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)\n",
    "\n",
    "This notebook is built to run a Vertex AI User-Managed Notebook using the default Compute Engine Service Account.  \n",
    "Check the Dataproc Serverless in Vertex AI Pipelines tutorial linked above to learn how to setup a different Service Account.  \n",
    "\n",
    "#### Permissions\n",
    "\n",
    "Make sure that the service account used to run the notebook has the following roles:\n",
    "\n",
    "- roles/aiplatform.serviceAgent\n",
    "- roles/aiplatform.customCodeServiceAgent\n",
    "- roles/storage.objectCreator\n",
    "- roles/storage.objectViewer\n",
    "- roles/dataproc.editor\n",
    "- roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89b301-5249-462a-97d8-986488b303fd",
   "metadata": {},
   "source": [
    "## Step 1: Install Libraries\n",
    "#### Run Step 1 one time for each new notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef65ec2-ad6b-407f-a993-7cdf871bba11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install pymysql SQLAlchemy\n",
    "pip3 install --upgrade google-cloud-pipeline-components kfp --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90943f-b965-4f7f-b631-ce62227d5e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get update -y\n",
    "sudo apt-get install default-jdk -y\n",
    "wget https://mirrors.estointernet.in/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz\n",
    "tar -xf apache-maven-3.6.3-bin.tar.gz\n",
    "sudo rm -rf /usr/bin/apache-maven-3.6.3\n",
    "sudo mv apache-maven-3.6.3 /usr/bin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef1307-902e-4713-8948-b86084e19312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waititng some time for kernel to restart\n",
    "import os\n",
    "import IPython\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d01e33-9099-4d2e-b57e-575c3a998d84",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703b502-1b41-44f1-bf21-41069255bc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pymysql\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "from google_cloud_pipeline_components.experimental.dataproc import DataprocSparkBatchOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4a209-db59-42f6-bba7-30cd46b16bad",
   "metadata": {},
   "source": [
    "## Step 3: Assign Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d3fbd8-013f-45e6-b7e9-8f31a4580e91",
   "metadata": {},
   "source": [
    "### Step 3.1 Common Parameters\n",
    " \n",
    "- PROJECT : GCP project-id\n",
    "- REGION : GCP region\n",
    "- GCS_STAGING_LOCATION : GCS staging location to be used for this notebook to store artifacts\n",
    "- SUBNET : VPC subnet\n",
    "- JARS : list of jars. For this notebook mysql connector and avro jar is required in addition with the dataproc template jars\n",
    "- MAX_PARALLELISM : Parameter for number of jobs to run in parallel default value is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f6dd9-2e13-447c-b28d-10fa2321b759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = \"<project-id>\"\n",
    "REGION = \"<region>\"\n",
    "GCS_STAGING_LOCATION = \"<gs://bucket/[folder]>\"\n",
    "SUBNET = \"<projects/{project}/regions/{region}/subnetworks/{subnet}>\"\n",
    "MAX_PARALLELISM = 2 # default value is set to 2\n",
    "\n",
    "# Do not change this parameter unless you want to refer below JARS from new location\n",
    "JARS = [GCS_STAGING_LOCATION + \"/jars/mysql-connector-java-8.0.29.jar\",\"file:///usr/lib/spark/external/spark-avro.jar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051df2af-bd8b-47c7-8cb2-05404ca0d859",
   "metadata": {},
   "source": [
    "### Step 3.2 MYSQL to GCS Parameters\n",
    "- MYSQL_HOST : MYSQL instance ip address\n",
    "- MYSQL_PORT : MySQL instance port\n",
    "- MYSQL_USERNAME : MYSQL username\n",
    "- MYSQL_PASSWORD : MYSQL password\n",
    "- MYSQL_DATABASE : name of database that you want to migrate\n",
    "- MYSQLTABLE_LIST : list of tables you want to migrate eg: ['table1','table2'] else provide an empty list for migration whole database eg : [] \n",
    "- MYSQL_OUTPUT_GCS_LOCATION : gcs location where mysql output will be writtes eg :\"gs://bucket/[folder]\"\n",
    "- MYSQL_OUTPUT_GCS_MODE : output mode for MYSQL data one of (overwrite|append)\n",
    "- MYSQL_OUTPUT_GCS_FORMAT : output file formate for MYSQL data one of (avro|parquet|orc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd2824-e9a0-4ceb-a3c9-32f79973432a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MYSQL_HOST = \"<host>\"\n",
    "MYSQL_PORT = \"<port>\"\n",
    "MYSQL_USERNAME = \"<username>\"\n",
    "MYSQL_PASSWORD = \"<password>\"\n",
    "MYSQL_DATABASE = \"<database>\"\n",
    "MYSQLTABLE_LIST = [] # leave list empty for migrating complete database else provide tables as ['table1','table2']\n",
    "MYSQL_OUTPUT_GCS_LOCATION = \"<gs://bucket/[folder]>\"\n",
    "MYSQL_OUTPUT_GCS_MODE = \"<mode>\" # one of overwrite|append\n",
    "MYSQL_OUTPUT_GCS_FORMAT = \"<format>\" # one of avro|parquet|orc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918048e3-4e98-4a91-8f5c-7c2cf1da559c",
   "metadata": {},
   "source": [
    "### Step 3.3 GCS to Cloud Spanner Parameters\n",
    "- SPANNER_INSTANCE : cloud spanner instance name\n",
    "- SPANNER_DATABASE : cloud spanner database name\n",
    "- SPANNER_TABLE_PRIMARY_KEYS : provide dictionary of format {\"table_name\":\"primary_key\"} for tables which do not have primary key in MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590468f-8782-4ccc-b2eb-62a9ea2599d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SPANNER_INSTANCE = \"<instance>\"\n",
    "SPANNER_DATABASE = \"<database>\"\n",
    "SPANNER_TABLE_PRIMARY_KEYS = {} # provide table which do not have PK in MYSQL {\"table_name\":\"primary_key\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b1536-d58e-423b-b3c2-cc0c171d275e",
   "metadata": {},
   "source": [
    "### Step 3.4 Notebook Configuration Parameters\n",
    "Below variables shoulld not be changed unless required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0f037-e888-4479-a143-f06a39bd5cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PYMYSQL_DRIVER = \"mysql+pymysql\"\n",
    "JDBC_DRIVER = \"com.mysql.cj.jdbc.Driver\"\n",
    "JDBC_URL = \"jdbc:mysql://{}:{}/{}?user={}&password={}\".format(MYSQL_HOST,MYSQL_PORT,MYSQL_DATABASE,MYSQL_USERNAME,MYSQL_PASSWORD)\n",
    "MAIN_CLASS = \"com.google.cloud.dataproc.templates.main.DataProcTemplate\"\n",
    "WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/java/\"\n",
    "JAR_FILE = \"dataproc-templates-1.0-SNAPSHOT.jar\"\n",
    "GRPC_JAR_PATH = \"./grpc_lb/io/grpc/grpc-grpclb/1.40.1\"\n",
    "GRPC_JAR = \"grpc-grpclb-1.40.1.jar\"\n",
    "LOG4J_PROPERTIES_PATH = \"./src/test/resources\"\n",
    "LOG4J_PROPERTIES = \"log4j-spark-driver-template.properties\"\n",
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\"\n",
    "\n",
    "# adding dataproc template JAR and grpc jar\n",
    "JARS.append(GCS_STAGING_LOCATION + \"/\" + GRPC_JAR)\n",
    "JARS.append(GCS_STAGING_LOCATION + \"/\" + JAR_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c062b-5a91-4372-b440-5c37a12fbf87",
   "metadata": {},
   "source": [
    "## Step 4: Generate MySQL Table List\n",
    "This step creates list of tables for migration. If MYSQLTABLE_LIST is kept empty all the tables in the MYSQL_DATABASE are listed for migration otherwise the provided list is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e362ac-30cd-4857-9e2a-0e9eb926e627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(MYSQLTABLE_LIST) == 0:\n",
    "    DB = sqlalchemy.create_engine(\n",
    "            sqlalchemy.engine.url.URL.create(\n",
    "                drivername=PYMYSQL_DRIVER,\n",
    "                username=MYSQL_USERNAME,\n",
    "                password=MYSQL_PASSWORD,\n",
    "                database=MYSQL_DATABASE,\n",
    "                host=MYSQL_HOST,\n",
    "                port=MYSQL_PORT\n",
    "              )\n",
    "            )\n",
    "    with DB.connect() as conn:\n",
    "        print(\"connected to database\")\n",
    "        results = DB.execute('show tables;').fetchall()\n",
    "        print(\"Total Tables = \", len(results))\n",
    "        for row in results:\n",
    "            TABLE_LIST.append(row[0])\n",
    "\n",
    "print(\"list of tables for migration :\")\n",
    "print(MYSQLTABLE_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a62e8-7499-41c6-b32b-73b539b0c7c4",
   "metadata": {},
   "source": [
    "## Step 5: Get Primary Keys for tables not present in SPANNER_TABLE_PRIMARY_KEYS\n",
    "For tables which do not have primary key provided in dictionary SPANNER_TABLE_PRIMARY_KEYS this step fetches primary key from MYSQL_DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda8fac-582c-4d4a-b871-311bb2863335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DB = sqlalchemy.create_engine(\n",
    "            sqlalchemy.engine.url.URL.create(\n",
    "                drivername=PYMYSQL_DRIVER,\n",
    "                username=MYSQL_USERNAME,\n",
    "                password=MYSQL_PASSWORD,\n",
    "                database=MYSQL_DATABASE,\n",
    "                host=MYSQL_HOST,\n",
    "                port=MYSQL_PORT\n",
    "              )\n",
    "            )\n",
    "with DB.connect() as conn:\n",
    "    for table in MYSQLTABLE_LIST:\n",
    "        primary_keys = []\n",
    "        if table not in SPANNER_TABLE_PRIMARY_KEYS:\n",
    "            results = DB.execute(\"SHOW KEYS FROM {} WHERE Key_name = 'PRIMARY'\".format(table)).fetchall()\n",
    "            for row in results:\n",
    "                primary_keys.append(row[4])\n",
    "            if primary_keys:\n",
    "                SPANNER_TABLE_PRIMARY_KEYS[table] = \",\".join(primary_keys)\n",
    "            else:\n",
    "                SPANNER_TABLE_PRIMARY_KEYS[table] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a210f-48da-474f-bf46-89e755d01c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pkDF = pd.DataFrame({\"table\" : MYSQLTABLE_LIST, \"primary_keys\": list(SPANNER_TABLE_PRIMARY_KEYS.values())})\n",
    "print(\"Below are identified primary keys for migrating mysql table to spanner:\")\n",
    "pkDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5f841-a687-4723-a8e6-6e7e752ba36e",
   "metadata": {},
   "source": [
    "## Step 6: Create JAR files and Upload to GCS\n",
    "#### Run Step 6 one time for each new notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22220ae3-9fb4-471c-b5aa-f606deeca15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd $WORKING_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee7afc-699b-4c1a-aeec-df0f99764ae0",
   "metadata": {},
   "source": [
    "#### Setting PATH variables for JDK and Maven and executing MAVEN build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40f634-1983-4267-a4c1-b072bf6d81ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-8.0.29.tar.gz\n",
    "tar -xf mysql-connector-java-8.0.29.tar.gz\n",
    "export JAVA_HOME=\"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "export PATH=\"$JAVA_HOME/bin:$PATH\"\n",
    "export MAVEN_HOME=\"/usr/bin/apache-maven-3.6.3\"\n",
    "export PATH=\"$MAVEN_HOME/bin:$PATH\"\n",
    "export PATH\n",
    "mvn clean spotless:apply install -DskipTests \n",
    "mvn dependency:get -Dartifact=io.grpc:grpc-grpclb:1.40.1 -Dmaven.repo.local=./grpc_lb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a779f-2c39-42ec-98be-0f5e9d715447",
   "metadata": {},
   "source": [
    "#### copying JARS files to GCS_STAGING_LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cdcd5-0f3e-4f51-aa78-93d1976cb0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp target/$JAR_FILE $GCS_STAGING_LOCATION/$JAR_FILE\n",
    "!gsutil cp $GRPC_JAR_PATH/$GRPC_JAR $GCS_STAGING_LOCATION/$GRPC_JAR\n",
    "!gsutil cp $LOG4J_PROPERTIES_PATH/$LOG4J_PROPERTIES $GCS_STAGING_LOCATION/$LOG4J_PROPERTIES\n",
    "!gsutil cp mysql-connector-java-8.0.29/mysql-connector-java-8.0.29.jar $GCS_STAGING_LOCATION/jars/mysql-connector-java-8.0.29.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bb170-09c4-40d1-baaf-9e907f215889",
   "metadata": {},
   "source": [
    "## Step 7: Calculate Parallel Jobs for MySQL to GCS\n",
    "This step uses MAX_PARALLELISM parameter to calculate number of parallel jobs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c501db0-c1fb-4a05-88b8-a7e546e2b1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate parallel jobs:\n",
    "COMPLETE_LIST = copy.deepcopy(MYSQLTABLE_LIST)\n",
    "PARALLEL_JOBS = len(MYSQLTABLE_LIST)//MAX_PARALLELISM\n",
    "JOB_LIST = []\n",
    "while len(COMPLETE_LIST) > 0:\n",
    "    SUB_LIST = []\n",
    "    for i in range(MAX_PARALLELISM):\n",
    "        if len(COMPLETE_LIST)>0 :\n",
    "            SUB_LIST.append(COMPLETE_LIST[0].lower())\n",
    "            COMPLETE_LIST.pop(0)\n",
    "        else:\n",
    "            break\n",
    "    JOB_LIST.append(SUB_LIST)\n",
    "print(\"list of tables for execution : \")\n",
    "print(JOB_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6f83b-891a-4515-a1d6-f3406a25dc2a",
   "metadata": {},
   "source": [
    "## Step 8: Execute Pipeline to Migrate tables from MySQL to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98914b-bd74-4d0e-9562-7019d504a25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mysql_to_gcs_jobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fa2d8-4ef7-4722-87c8-eec6c06f892b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def migrate_mysql_to_gcs(EXECUTION_LIST):\n",
    "    EXECUTION_LIST = EXECUTION_LIST\n",
    "    aiplatform.init(project=PROJECT,staging_bucket=GCS_STAGING_LOCATION)\n",
    "    \n",
    "    @dsl.pipeline(\n",
    "        name=\"java-mysql-to-gcs-pyspark\",\n",
    "        description=\"Pipeline to get data from mysql to gcs\",\n",
    "    )\n",
    "    def pipeline(\n",
    "        PROJECT_ID: str = PROJECT,\n",
    "        LOCATION: str = REGION,\n",
    "        MAIN_CLASS: str = MAIN_CLASS,\n",
    "        JAR_FILE_URIS: list = JARS,\n",
    "        SUBNETWORK_URI: str = SUBNET,\n",
    "        FILE_URIS: list = [GCS_STAGING_LOCATION + \"/\" + LOG4J_PROPERTIES]\n",
    "    ):\n",
    "        for table in EXECUTION_LIST:\n",
    "            BATCH_ID = \"mysql2gcs-{}\".format(datetime.now().strftime(\"%s\"))\n",
    "            mysql_to_gcs_jobs.append(BATCH_ID)\n",
    "            TEMPLATE_SPARK_ARGS = [\n",
    "            \"--template=JDBCTOGCS\",\n",
    "            \"--templateProperty\", \"project.id={}\".format(PROJECT),\n",
    "            \"--templateProperty\", \"jdbctogcs.jdbc.url={}\".format(JDBC_URL),\n",
    "            \"--templateProperty\", \"jdbctogcs.jdbc.driver.class.name={}\".format(JDBC_DRIVER),\n",
    "            \"--templateProperty\",\"jdbctogcs.output.location={}/{}\".format(MYSQL_OUTPUT_GCS_LOCATION,table),\n",
    "            \"--templateProperty\", \"jdbctogcs.output.format={}\".format(MYSQL_OUTPUT_GCS_FORMAT),\n",
    "            \"--templateProperty\", \"jdbctogcs.write.mode={}\".format(MYSQL_OUTPUT_GCS_MODE),\n",
    "            \"--templateProperty\", \"jdbctogcs.sql=select * from {}\".format(table),\n",
    "            ]\n",
    "\n",
    "            _ = DataprocSparkBatchOp(\n",
    "                project=PROJECT_ID,\n",
    "                location=LOCATION,\n",
    "                batch_id=BATCH_ID,\n",
    "                main_class=MAIN_CLASS,\n",
    "                jar_file_uris=JAR_FILE_URIS,\n",
    "                file_uris=FILE_URIS,\n",
    "                subnetwork_uri=SUBNETWORK_URI,\n",
    "                args=TEMPLATE_SPARK_ARGS\n",
    "            )\n",
    "            time.sleep(3)\n",
    "\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "    pipeline = aiplatform.PipelineJob(\n",
    "            display_name=\"pipeline\",\n",
    "        template_path=\"pipeline.json\",\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        enable_caching=False,\n",
    "        )\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44205b54-1ac7-42f3-85ad-5b20f531056b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for execution_list in JOB_LIST:\n",
    "    print(execution_list)\n",
    "    migrate_mysql_to_gcs(execution_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce7f828-dacc-404b-8927-dc3813e7216a",
   "metadata": {},
   "source": [
    "## Step 9: Get status for tables migrated from MySql to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611510f-271c-447a-899d-42fbb983268d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bearer_token():\n",
    "    \n",
    "    try:\n",
    "        #Defining Scope\n",
    "        CREDENTIAL_SCOPES = [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "\n",
    "        #Assining credentials and project value\n",
    "        credentials, project_id = google.auth.default(scopes=CREDENTIAL_SCOPES)\n",
    "\n",
    "        #Refreshing credentials data\n",
    "        credentials.refresh(requests.Request())\n",
    "\n",
    "        #Get refreshed token\n",
    "        token = credentials.token\n",
    "        if token:\n",
    "            return (token,200)\n",
    "        else:\n",
    "            return \"Bearer token not generated\"\n",
    "    except Exception as error:\n",
    "        return (\"Bearer token not generated. Error : {}\".format(error),500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcbc63-19db-42a8-a2ed-d9855da00c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.auth.transport import requests\n",
    "import google\n",
    "token = get_bearer_token()\n",
    "if token[1] == 200:\n",
    "    print(\"Bearer token generated\")\n",
    "else:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3cf87-6d28-4b23-8466-87d3399f7a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "mysql_to_gcs_status = []\n",
    "job_status_url = \"https://dataproc.googleapis.com/v1/projects/{}/locations/{}/batches/{}\"\n",
    "for job in mysql_to_gcs_jobs:\n",
    "    auth = \"Bearer \" + token[0]\n",
    "    url = job_status_url.format(PROJECT,REGION,job)\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json; charset=UTF-8',\n",
    "      'Authorization': auth \n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    mysql_to_gcs_status.append(response.json()['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097575d-07c2-4659-a75f-d7e898e3f077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statusDF = pd.DataFrame({\"table\" : MYSQLTABLE_LIST,\"mysql_to_gcs_job\" : mysql_to_gcs_jobs, \"mysql_to_gcs_status\" : mysql_to_gcs_status})\n",
    "statusDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70a6d4-3372-4f5c-bdf6-e0c856ec4318",
   "metadata": {},
   "source": [
    "## Step 10: Execute Pipeline to Migrate tables from GCS to Cloud Spanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42a236-4970-4859-9094-39b4fd3a41bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcs_to_spanner_jobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fa0ac-c99d-4d35-b7d8-5fd5ca23d12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def migrate_gcs_to_spanner(EXECUTION_LIST):\n",
    "    EXECUTION_LIST = EXECUTION_LIST\n",
    "    aiplatform.init(project=PROJECT, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name=\"java-gcs-to-spanner-pyspark\",\n",
    "        description=\"Pipeline to get data from gcs to spanner\",\n",
    "    )\n",
    "    def pipeline(\n",
    "        PROJECT_ID: str = PROJECT,\n",
    "        LOCATION: str = REGION,\n",
    "        MAIN_CLASS: str = MAIN_CLASS,\n",
    "        JAR_FILE_URIS: list = JARS,\n",
    "        SUBNETWORK_URIS: str = SUBNET,\n",
    "        FILE_URIS: list = [GCS_STAGING_LOCATION + \"/\" + LOG4J_PROPERTIES]\n",
    "    ):\n",
    "        for table in EXECUTION_LIST:\n",
    "            BATCH_ID = \"gcs2spanner-{}\".format(datetime.now().strftime(\"%s\"))\n",
    "            gcs_to_spanner_jobs.append(BATCH_ID)\n",
    "            TEMPLATE_SPARK_ARGS = [\n",
    "            \"--template=GCSTOSPANNER\",\n",
    "            \"--templateProperty\", \"project.id={}\".format(PROJECT),\n",
    "            \"--templateProperty\",  \"gcs.spanner.input.format={}\".format(MYSQL_OUTPUT_GCS_FORMAT),\n",
    "            \"--templateProperty\", \"gcs.spanner.input.location={}/{}/\".format(MYSQL_OUTPUT_GCS_LOCATION,table),\n",
    "            \"--templateProperty\", \"gcs.spanner.output.instance={}\".format(SPANNER_INSTANCE),\n",
    "            \"--templateProperty\", \"gcs.spanner.output.database={}\".format(SPANNER_DATABASE),\n",
    "            \"--templateProperty\", \"gcs.spanner.output.table={}\".format(table),\n",
    "            \"--templateProperty\", \"gcs.spanner.output.saveMode={}\".format(MYSQL_OUTPUT_GCS_MODE.capitalize()),\n",
    "            \"--templateProperty\", \"gcs.spanner.output.primaryKey={}\".format(SPANNER_TABLE_PRIMARY_KEYS[table])\n",
    "            ]\n",
    "            _ = DataprocSparkBatchOp(\n",
    "                project=PROJECT_ID,\n",
    "                location=LOCATION,\n",
    "                batch_id=BATCH_ID,\n",
    "                main_class=MAIN_CLASS,\n",
    "                jar_file_uris=JAR_FILE_URIS,\n",
    "                file_uris=FILE_URIS,\n",
    "                subnetwork_uri=SUBNETWORK_URIS,\n",
    "                args=TEMPLATE_SPARK_ARGS\n",
    "            )\n",
    "            time.sleep(3)\n",
    "                                                    \n",
    "\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "    pipeline = aiplatform.PipelineJob(\n",
    "            display_name=\"pipeline\",\n",
    "            template_path=\"pipeline.json\",\n",
    "            pipeline_root=PIPELINE_ROOT,\n",
    "            enable_caching=False,\n",
    "            )\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c93a4-9621-418f-a004-673537c63bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for execution_list in JOB_LIST:\n",
    "    print(execution_list)\n",
    "    migrate_gcs_to_spanner(execution_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85368f00-4d8a-4ea2-a1a6-53dc7e4e35f7",
   "metadata": {},
   "source": [
    "## Step 11: Get status for tables migrated from GCS to Cloud Spanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd1517-ed35-40fb-87de-d85fb84e7483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.auth.transport import requests\n",
    "import google\n",
    "\n",
    "token = get_bearer_token()\n",
    "if token[1] == 200:\n",
    "    print(\"Bearer token generated\")\n",
    "else:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57b94e-3747-4d92-b67d-d342cddb193e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "gcs_to_spanner_status = []\n",
    "job_status_url = \"https://dataproc.googleapis.com/v1/projects/{}/locations/{}/batches/{}\"\n",
    "for job in gcs_to_spanner_jobs:\n",
    "    auth = \"Bearer \" + token[0]\n",
    "    url = job_status_url.format(PROJECT,REGION,job)\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json; charset=UTF-8',\n",
    "      'Authorization': auth \n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    gcs_to_spanner_status.append(response.json()['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c346cea-5dfb-4239-8caf-c56efa819f76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statusDF['gcs_to_spanner_job'] = gcs_to_spanner_jobs\n",
    "statusDF['gcs_to_spanner_status'] = gcs_to_spanner_status\n",
    "statusDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0961f164-c7e4-4bb5-80f0-25fd1051147b",
   "metadata": {},
   "source": [
    "## Step 12: Validate row counts of migrated tables from MySQL to Cloud Spanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a28fb-3a39-4a10-b92d-0685b351a1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mysql_row_count = []\n",
    "spanner_row_count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25299344-c167-4764-a5d1-56c1b384d104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get mysql table counts\n",
    "DB = sqlalchemy.create_engine(\n",
    "            sqlalchemy.engine.url.URL.create(\n",
    "                drivername=PYMYSQL_DRIVER,\n",
    "                username=MYSQL_USERNAME,\n",
    "                password=MYSQL_PASSWORD,\n",
    "                database=MYSQL_DATABASE,\n",
    "                host=MYSQL_HOST,\n",
    "                port=MYSQL_PORT\n",
    "              )\n",
    "            )\n",
    "with DB.connect() as conn:\n",
    "    for table in MYSQLTABLE_LIST:\n",
    "        results = DB.execute(\"select count(*) from {}\".format(table)).fetchall()\n",
    "        for row in results:\n",
    "            mysql_row_count.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e539d-5180-4f5b-915e-35f7ea45e0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get spanner table counts\n",
    "from google.cloud import spanner\n",
    "\n",
    "spanner_client = spanner.Client()\n",
    "instance = spanner_client.instance(SPANNER_INSTANCE)\n",
    "database = instance.database(SPANNER_DATABASE)\n",
    "\n",
    "for table in MYSQLTABLE_LIST:\n",
    "    with database.snapshot() as snapshot:\n",
    "        results = snapshot.execute_sql(\"select count(*) from {}\".format(table))\n",
    "        for row in results:\n",
    "            spanner_row_count.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1afe12-3eb9-4133-8377-66dc63ac649c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statusDF['mysql_row_count'] = mysql_row_count \n",
    "statusDF['spanner_row_count'] = spanner_row_count \n",
    "statusDF"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
