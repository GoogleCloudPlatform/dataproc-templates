# Common Properties.
## GCP project id.
project.id=<project-id>
## GCS staging bucket
gcs.staging.bucket.path=<gcs-staging-path>

# HiveToGCS Template properties.
## Source Hive warehouse dir.
spark.sql.warehouse.dir=<warehouse-path>
## GCS output path.
hive.gcs.output.path=<gcs-output-path>
## Name of hive input table.
hive.input.table=<hive-input-table>
## Hive input db name.
hive.input.db=<hive-output-db>
## GCS output format. Optional, defaults to avro.
hive.gcs.output.format=<gcs-output-format>
## Optional, column to partition hive data.
hive.partition.col=<hive-partition-col>



# HiveToBQ Template properties.
##Bigquery table name
hivetobq.bigquery.location=<bigquery-location>
hivetobq.input.table=<hive-input-table>
hivetobq.input.db=<hive-input-db>
#Write mode to use while writing output to BQ. Supported values are - Append/Overwrite/ErrorIfExists/Ignore
hivetobq.append.mode=ErrorIfExists
# Optional, column to partition by
hivetobq.partition.col=
# Spark warehouse directory location
hivetobq.spark.sql.warehouse.dir=<spark-warehouse-directory>


# WordCount properties.
## Format of input file to be processed.
word.count.input.format=
## input path for word count.
word.count.input.path=
## input path for word count.
word.count.output.path=


# GCSToBigTable properties.
## Bigtable instance ud.
bigtable.instance.id=<bigtable instance id>

## BigTable output table name.
bigtable.output.table.name=<bigtable output table>
## Column from input source to be used as key column in Bigtable.
bigtable.key.col=<input col to be used as row key for BT>
## BigTable column family name.
bigtable.col.family.name=<bigtable col family>

# PubSubToBQ properties
## Project that contains the input PubSub subscription to be read
pubsub.input.project.id=<pubsub project id>
## PubSub subscription name
pubsub.input.subscription=<pubsub subscription>
## Stream timeout, for how long the subscription will be read
pubsub.timeout.ms=60000
## Streaming duration, how often wil writes to BQ be triggered
pubsub.streaming.duration.seconds=15
## Project that contains the output table
pubsub.bq.output.project.id=<pubsub to bq output project id>
## Big Query output dataset
pubsub.bq.output.dataset=<bq output dataset>
## Big Query output table
pubsub.bq.output.table=<bq output table>


#GCStoBigQuery properties
gcs.bigquery.input.format=<avro,parquet,csv>
gcs.bigquery.input.location=<input-file-location>
gcs.bigquery.output.dataset =<bq output dataset>
gcs.bigquery.output.table =<bq output table>
gcs.bigquery.temp.bucket.name =<temp bucket>

# Spanner To GCS properties
spanner.id=<spanner-id>
database.id=<database-id>
table.id=<table-id>
gcs.export.path=<gcs-path>

# S3 to BQ
s3.bq.access.key=<s3-accesss-key>
s3.bq.secret.key=<s3-secret-key>
s3.bq.input.format=<avro,parquet,csv,json>
s3.bq.input.location=<s3-input-location>
s3.bq.output.dataset.name=<bq-dataset-name>
s3.bq.output.table.name=<bq-output-table>
s3.bq.ld.temp.bucket.name=<temp-bucket>

# Bigquery to GCS
bigquery.gcs.input.table=<project:dataset.table, eg: bigquery-public-data:samples.shakespeare>
bigquery.gcs.output.format=<avro,parquet,csv,json>
bigquery.gcs.output.location=<gcs-path>
