# Common Properties.
## GCP project id.
project.id=<project-id>
## GCS staging bucket
gcs.staging.bucket.path=<gcs-staging-path>

# HiveToGCS Template properties.
## Source Hive warehouse dir.
spark.sql.warehouse.dir=
## GCS output path.
hive.gcs.output.path=
## Name of hive input table.
hive.input.table=
## Hive input db name.
hive.input.db=default
## GCS output format. Optional, defaults to avro.
hive.gcs.output.format=avro
## Optional, column to partition hive data.
hive.partition.col=


# HiveToBQ Template properties.
##Bigquery table name
hivetobq.bigquery.location=
hivetobq.input.table=
hivetobq.input.db=default
#Write mode to use while writing output to BQ. Supported values are - Append/Overwrite/ErrorIfExists/Ignore
hivetobq.append.mode=Overwrite
# Optional, column to partition by
hivetobq.partition.col=
# Spark warehouse directory location
hivetobq.spark.sql.warehouse.dir=


# JDBCToBQ Template properties.
##Bigquery table name
jdbctobq.bigquery.location=
jdbctobq.jdbc.url=jdbc:mysql://host:port/
jdbctobq.jdbc.driver.class.name=com.mysql.jdbc.Driver
#Customize any properties supported by jdbc. Value to be in JSON format
jdbctobq.jdbc.properties={"user":"usernamehere","password":"passwordhere"}
##jdbc:bigquery://https://www.googleapis.com/bigquery/v2:443;ProjectId=nsadineni;OAuthType=1;
jdbctobq.input.table=
jdbctobq.input.db=
#Write mode to use while writing output to BQ. Supported values are - Append/Overwrite/ErrorIfExists/Ignore
jdbctobq.append.mode=ErrorIfExists
# Optional, column to partition by
jdbctobq.partition.col=
# Spark warehouse directory location
jdbctobq.spark.sql.warehouse.dir=

# WordCount properties.
## Format of input file to be processed.
word.count.input.format=
## input path for word count.
word.count.input.path=
## input path for word count.
word.count.output.path=


# GCSToBigTable properties.
## Bigtable instance ud.
bigtable.instance.id=<bigtable instance id>

## BigTable output table name.
bigtable.output.table.name=<bigtable output table>
## Column from input source to be used as key column in Bigtable.
bigtable.key.col=<input col to be used as row key for BT>
## BigTable column family name.
bigtable.col.family.name=<bigtable col family>

# PubSubToBQ properties
## Project that contains the input PubSub subscription to be read
pubsub.input.project.id=<pubsub project id>
## PubSub subscription name
pubsub.input.subscription=<pubsub subscription>
## Stream timeout, for how long the subscription will be read
pubsub.timeout.ms=60000
## Streaming duration, how often wil writes to BQ be triggered
pubsub.streaming.duration.seconds=15
## Project that contains the output table
pubsub.bq.output.project.id=<pubsub to bq output project id>
## Big Query output dataset
pubsub.bq.output.dataset=<bq output dataset>
## Big Query output table
pubsub.bq.output.table=<bq output table>


#GCStoBigQuery properties
gcs.bigquery.input.format=<avro,parquet,csv>
gcs.bigquery.input.location=<input-file-location>
gcs.bigquery.output.dataset =<bq output dataset>
gcs.bigquery.output.table =<bq output table>
gcs.bigquery.temp.bucket.name =<temp bucket>

# Spanner To GCS properties
spanner.id=<spanner-id>
database.id=<database-id>
table.id=<table-id>
gcs.export.path=<gcs-path>

# S3 to BQ
s3.bq.access.key=<s3-accesss-key>
s3.bq.secret.key=<s3-secret-key>
s3.bq.input.format=<avro,parquet,csv,json>
s3.bq.input.location=<s3-input-location>
s3.bq.output.dataset.name=<bq-dataset-name>
s3.bq.output.table.name=<bq-output-table>
s3.bq.ld.temp.bucket.name=<temp-bucket>