[COMPOSER]
DAG_NAME=submit_pyspark_dataproc_template
SCHEDULE_INTERVAL=
EXEC_TIMEOUT=600
RUN_TIMEOUT=600
RETRY_DELAY=60
RETRIES=1

[ENV_VARS]
GCP_PROJECT=<your_project
REGION=<your_region>
GCS_STAGING_LOCATION=<gs://your_location>
SUBNET=<your_subnet>
JARS=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar

[TEMPLATE_ARGS]
RUNNING_TEMPLATE=<CHOSEN_TEMPLATE>

[GCSTOBIGQUERY]
template=GCSTOBIGQUERY
gcs.bigquery.input.location=<gs://your_location>
gcs.bigquery.output.dataset=<your_dataset>
gcs.bigquery.output.table=<your_table>
gcs.bigquery.input.format=avro|parquet|csv|json
gcs.bigquery.temp.bucket.name=<your_temp_bucket_name>
gcs.bigquery.output.mode=overwrite|append|ignore|errorifexists

[BIGQUERYTOGCS]
template=BIGQUERYTOGCS
bigquery.gcs.input.table=<your_table>
bigquery.gcs.output.format=avro|parquet|csv|json
bigquery.gcs.output.mode=overwrite|append|ignore|errorifexists
bigquery.gcs.output.location=<gs://your_location>